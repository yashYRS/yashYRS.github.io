<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-11-11T19:14:24+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">&amp;gt;_  cd /home/</title><subtitle> </subtitle><author><name>Yash Sarrof</name></author><entry><title type="html">A brief history of Me ;)</title><link href="http://localhost:4000/about/about.html" rel="alternate" type="text/html" title="A brief history of Me ;)" /><published>2021-10-03T00:00:00+05:30</published><updated>2021-10-03T00:00:00+05:30</updated><id>http://localhost:4000/about/about</id><content type="html" xml:base="http://localhost:4000/about/about.html">&lt;p&gt;My name is Yash Raj Sarrof. I graduated from from Manipal Institute of Technology majoring in Computer Science. I am currently working as a Machine Learning Engineer at &lt;a href=&quot;https://glib.ai/&quot;&gt;Glib&lt;/a&gt;. I am part of the R&amp;amp;D team, and our primary focus is to bring out a semblance of structure to the myriad unstructured formats in which financial documents are generated.&lt;/p&gt;

&lt;p&gt;Prior to this, I was an intern at Samsung R&amp;amp;D Bangalore, where I worked in the On-Device AI division on projects that aimed at improving user’s interaction with screenshots. My role dabbled from training models for script identification, to data augmentation for text localization in screenshots.&lt;/p&gt;

&lt;p&gt;Although I am fascinated with deep learning models and try to keep myself updated with the latest developments on it, my primary interest lies in research that approaches NLP problems from a linguistic standpoint. I wish to pursue my masters and hopefully a PhD as well in future, and contribute to the research community in a meaningful way.&lt;/p&gt;

&lt;p&gt;I strive to keep the posts non-redundant, although in the process end up writing sparingly. The structure of the blog is as follows -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Projects:&lt;/strong&gt; posts about some of the projects that I have worked on&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thoughts:&lt;/strong&gt; personal stories and random thoughts that pop up in my head&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CS articles:&lt;/strong&gt; things that I have learnt through my work, and want to share&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking forward to suggestions, criticisms and possible collaborations.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="About" /><summary type="html">My name is Yash Raj Sarrof. I graduated from from Manipal Institute of Technology majoring in Computer Science. I am currently working as a Machine Learning Engineer at Glib. I am part of the R&amp;amp;D team, and our primary focus is to bring out a semblance of structure to the myriad unstructured formats in which financial documents are generated.</summary></entry><entry><title type="html">Instead of demanding a seat, build your table or better yet extract some tables</title><link href="http://localhost:4000/technical/TableContentExtraction.html" rel="alternate" type="text/html" title="Instead of demanding a seat, build your table or better yet extract some tables" /><published>2021-09-01T00:00:00+05:30</published><updated>2021-09-01T00:00:00+05:30</updated><id>http://localhost:4000/technical/TableContentExtraction</id><content type="html" xml:base="http://localhost:4000/technical/TableContentExtraction.html">&lt;p&gt;Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/hard1.png&quot; alt=&quot;Tables not following a pattern in Multi column layouts&quot; /&gt;
  &lt;figcaption&gt;Tables not following a pattern in Multi column layouts&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/hard2.png&quot; alt=&quot;Variations in appearance of tables&quot; /&gt;
  &lt;figcaption&gt;Variations in appearance of tables&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;popular-datasets&quot;&gt;Popular Datasets&lt;/h3&gt;

&lt;p&gt;Some popular datasets that have been curated over the last few years in order to facilitate testing and training of Document AI tasks and will be used in this post are briefly described here. The disparity in the number of samples across datasets depends on whether the entire dataset was manually curated or generated automatically in a semi-supervised fashion.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset Name&lt;/th&gt;
      &lt;th&gt;Number of Samples&lt;/th&gt;
      &lt;th&gt;Document categories covered&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://ir.nist.gov/cdip/&quot;&gt;IIT-CDIP&lt;/a&gt; (Illinois Institute of Technology Complex Document Information Processing Test Collection)&lt;/td&gt;
      &lt;td&gt;Made from over 6 Million Scanned documents&lt;/td&gt;
      &lt;td&gt;Spans all kinds of documents&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~aharley/rvl-cdip/&quot;&gt;RVL-CDIP&lt;/a&gt; (Ryerson Vision Lab Complex Document Information Processing)&lt;/td&gt;
      &lt;td&gt;400K grayscale images (320K training, 40K validation, and 40K test images)&lt;/td&gt;
      &lt;td&gt;Subset of IIT-CDIP&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.10213.pdf&quot;&gt;SROIE&lt;/a&gt; (Scanned Receipt OCR and Information Extraction)&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;Scanned Receipts&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://guillaumejaume.github.io/FUNSD/&quot;&gt;FUNSD&lt;/a&gt; (Form Understanding in Noisy Scanned Documents)&lt;/td&gt;
      &lt;td&gt;199 fully annotated forms, 31485 words, 9707 semantic entities, 5304 relations Samples&lt;/td&gt;
      &lt;td&gt;Exclusively contains forms in scanned documents&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge&quot;&gt;CORD&lt;/a&gt; (COVID-19 Open Research Dataset)&lt;/td&gt;
      &lt;td&gt;Curated from 500K scholarly articles&lt;/td&gt;
      &lt;td&gt;Articles about COVID-19, SARS-CoV-2, and related coronaviruses&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/applicaai/kleister-nda&quot;&gt;Kleister NDA&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;540 NDAs, 3299 unique pages&lt;/td&gt;
      &lt;td&gt;Scanned and born-digital long formal Non Disclosure Agreements&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://paperswithcode.com/dataset/docvqa&quot;&gt;Doc VQA&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;12767 Document Images&lt;/td&gt;
      &lt;td&gt;Industrial documents including typewritten, printed, handwritten and born-digital text&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ibm-aur-nlp/PubLayNet&quot;&gt;PubLayNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;made from over 1 million PDF articles&lt;/td&gt;
      &lt;td&gt;Scientific articles and reports in the medical domain&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.primaresearch.org/dataset/&quot;&gt;PRImA&lt;/a&gt; (Pattern Recognition and Image Analysis) Layout Anaylsis Dataset&lt;/td&gt;
      &lt;td&gt;305 ground-truthed images&lt;/td&gt;
      &lt;td&gt;Magazines and techinical articles spanning multiple domains&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/doc-analysis/TableBank&quot;&gt;Table Bank&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;417,234 high quality labeled tables&lt;/td&gt;
      &lt;td&gt;Tables extracted from LaTeX and Word documents&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://dell-research-harvard.github.io/HJDataset/&quot;&gt;HJ&lt;/a&gt; Historical Japanese Dataset&lt;/td&gt;
      &lt;td&gt;Over 250,000 layout element annotations of seven types&lt;/td&gt;
      &lt;td&gt;Complex layouts from all kinds of historical documents&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;initial-approaches&quot;&gt;Initial Approaches&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The first efforts to compartmentalize a report into tabular and non tabular areas were made in 2016, where existing object detection architectures (CNNs, R - CNNs and their variants) were trained to detect tables in pdfs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In 2018, an end to end framwework was recommended in order to extract semantic structures of tables and paragraphs from documents. Pretrained word embeddings were in a fully conventional network to get decent results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In 2019, a Graph Convolutional Network was proposed, which combined visual and textual cues from a document for achieving the same result of identifying semantic structures.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;layout-lm&quot;&gt;Layout LM&lt;/h3&gt;

&lt;p&gt;Almost every iteration brought with itself promising results, and had scope of application in real world datasets, 
However 2 aspects which the inital approaches hadn’t tried, and was first explored by LayoutLM &lt;a class=&quot;citation&quot; href=&quot;#xu2020layoutlm&quot;&gt;(Xu et al., 2020)&lt;/a&gt;, were&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Avoiding reliance on labelled data, since the number of such publicly known datasets were limited&lt;/li&gt;
  &lt;li&gt;Trying joint pretrained models taking both textual and layout information into account. Up until now, the pretrained models were either CV or NLP models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Proposed by a team at Microsoft Research Asia, LayoutLM constituted a novel approach to simply pretraining for document AI tasks, and recommended fine tuning for subsequent tasks as per need. LayoutLM extends the core idea used in BERT to gain better performance in document AI tasks.&lt;/p&gt;

&lt;h4 id=&quot;modifications-made-to-the-bert-pretraining-stage&quot;&gt;Modifications made to the BERT pretraining stage:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Along with the word embeddings, embeddings to leverage the visual layout information present in the documents were added.
    &lt;ul&gt;
      &lt;li&gt;2d Position Embedding: The top left corner of the document page is considered to be the origin, and the bounding box for the word in the resulting coordinate system is stored as (x0, y0, x1, y1), with x0, y0 representing the upper left, and (x1, y1) representing the bottom right corner respectively. This helps model sparsity of the current word in the page, vicinity to other words, size of the current word relative to others and other spatial features, that would otherwise not be stored in case of traditional word embeddings.&lt;/li&gt;
      &lt;li&gt;Image Embedding:
        &lt;ul&gt;
          &lt;li&gt;Word Image Embedding: These image embeddings are generated by a faster R-CNN, which is fed the document image and the Region of Interest is specified by an OCR system that gives the bounding box results of the word in question.&lt;/li&gt;
          &lt;li&gt;[CLS] token Image Embedding: Using the same faster R-CNN, embedding of the entire document image is generated the Region of Interest set as the entire page. This is generated to assist in the downstream fine tuning tasks.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The pretraining task of Masked Language Modelling(MLM) changed to Masked Visual Language Model(MVLM). Just as 15% of the text tokens were masked in the original task. Here, word embedding is masked but the 2d Positional embeddings are retained, thereby utilizing language contexts in conjunction with spatial information to predict the masked token.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The pretraining task of Next Sentence Prediction (NSP) changed to Multi Label Document Classification (MDC). This change was made to encapsulate the knowledge from different document domains and thus generate better document level representations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;layoutlm-fine-tuning&quot;&gt;LayoutLM Fine tuning:&lt;/h4&gt;

&lt;p&gt;The fine tuning stage in LayoutLM is virtually identical that of BERT, differing only in the  3 downstream tasks were carried out viz. Form understanding using the FUNSD dataset, receipt information extraction using the SROIE dataset, and finally document classification on the RVL-CDIP dataset. The exact hyperparameters for each of the tasks differ and can be found in the paper. The overall idea of fine tuning is still the same, however slight improvements to all of these tasks as well as on other document AI tasks are constantly being made.&lt;/p&gt;

&lt;h4 id=&quot;sample-pipeline&quot;&gt;Sample Pipeline:&lt;/h4&gt;

&lt;p&gt;The document image is fed into the OCR system, which in turn generates the bounding boxes of each individual word in the page. Each coordinate x0, x1, y0, y1 along with the actual text are converted into a embedding vector, which in turn is concatenated. These embeddings are used to get pretrained LayoutLM embeddings. Parallely, the bounding boxes of each individual word along with the original document image is also fed into a faster RCNN, which generates the image embeddings. The image embedding and the pretrained LayoutLM embedding are combined and used for all downstream tasks.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/LayoutLMArchitecture.png&quot; alt=&quot;Sample Input for Layout LM&quot; /&gt;
  &lt;figcaption&gt;Sample Input for Layout LM&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;layout-lm-v2&quot;&gt;Layout LM v2&lt;/h3&gt;

&lt;p&gt;Research continued in 2 distinct directions in the case of document AI. The first continued with the initial approaches and tried to combine various NLP and CV individually pretrained models combining the outputs in a shallow manner. Although these methods invariably have state of the art performances on several datasets, there are a few key problems with continuing in this fashion. Such methods end up not performing well in case the document type is changed (from receipt understanding to form understanding) or in case the underlying domain is changed (medical documents to financial documents). This results in constant rework to adapt to every domain. The other approach of going about document AI tasks was the one introduced in LayoutLM, where the visual and text components are combined together to generate a unified pretrained model, and as per the task at hand, fine tuning is carried out with minimal effort. LayoutLM v2 &lt;a class=&quot;citation&quot; href=&quot;#xu2020layoutlmv2&quot;&gt;(Xu et al., 2020)&lt;/a&gt; quite evidently chose the second route, fruther improving upon the ideas from the previous iteration. Although the number of fine tuning tasks on the which the results of the improved pretrained model were shown were also increased, the more significant changes were made in the pretraining stages.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pretraining Embeddings&lt;/strong&gt;: Unline LayoutLM, where image and text embeddings were added in the finetuning stages, in the new iteration, the image information is encoded in the pretraining itself. To account for the changes, the embeddings are divided into the following categories
    &lt;ul&gt;
      &lt;li&gt;Text Embedding: To encapsulate the textual meaning, the text is initially separated into segments of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; tokens each. In case, some tokens are smaller in lenghts, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[PAD]&lt;/code&gt; tokens are used to fill out the gaps. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CLS]&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[END]&lt;/code&gt; tokens are present to denote the start and the end of the text sequence in each segment. The final embedding per token however has a few extra information encoded.
        &lt;ul&gt;
          &lt;li&gt;Token Position embedding (PosEmb1D): Generated based on the position of the token in the segment&lt;/li&gt;
          &lt;li&gt;Segment embedding (SegEmb): Generated based on the position of the segment amongst all the ones in the page&lt;/li&gt;
          &lt;li&gt;Token embedding (TokEmb): The text embedding as done in LayoutLM&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

\[t_i = TokEmb(w_i) + PosEmb1D(i) + SegEmb(s_i)\]

\[0 ≤ i &amp;lt; L\]

    &lt;ul&gt;
      &lt;li&gt;Visual Embedding: Visual information needs to be embedded to encapsulate information about font styles, text alignments, skew etc.
        &lt;ul&gt;
          &lt;li&gt;Resize document image to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;224 X 224&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Feed resized image into the encoder architecture. The ResNeXt-FPN architecture is used as a backbone&lt;/li&gt;
          &lt;li&gt;Average pool, the feature map output from the encoder to to get a fixed &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W X H&lt;/code&gt; size&lt;/li&gt;
          &lt;li&gt;Flatten the output to get a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W H&lt;/code&gt; size&lt;/li&gt;
          &lt;li&gt;Each element in the resulting vector is projected linearly, to get the visual token embedding&lt;/li&gt;
          &lt;li&gt;To maintain conformity with the textual embeddings, all the visual tokens (each element of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W H&lt;/code&gt; vector) are assigned the same segment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[C]&lt;/code&gt;.&lt;/li&gt;
          &lt;li&gt;Similar to textual embeddings, each element in the vector, also has a corresponding 1 D positional embedding.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

\[v_i = Proj(VisTokEmb(I_i)) + PosEmb1D(i) + SegEmb([C])\]

\[0 ≤ i &amp;lt; WH\]

    &lt;ul&gt;
      &lt;li&gt;Layout Embedding: Layout information becomes critical to embed spatial information and is similar to the 2D positional embeddings done for LayoutLM. The encapsulated knowledge here is critical, since in most cases of complex tables, the grammar won’t make sense, unless the text’s position in a table is known apriori. The layout embedding is generated for both visual and text tokens by discretizing and normalzing the bounding boxes of these tokens.&lt;/li&gt;
    &lt;/ul&gt;

\[l_i = Concat(PosEmb2D_x(x_0, x_1, w), PosEmb2D_y (y_0, y_1, h))\]

\[0 ≤ i &amp;lt; WH + L\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/Layoutv2Part1.png&quot; alt=&quot;Generating embeddings from document image in LayoutLMv2 &quot; /&gt;
  &lt;figcaption&gt;Generating embeddings from document image in LayoutLMv2 &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Spatial Aware Attention Mechanism&lt;/strong&gt;: The textual embeddings (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;/code&gt;), and the visual embeddings (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;) are concatenated into a single vector. The resulting vector is added to the layout embedding vector (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt;). The resulting sequence (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;) is fed into the transformer style architecture of BERT and LayoutLM. However, owing to the mulit modal input embeddings, absolute positional embeddings aren’t modelled. The whole purpose of trying to model the local invariance in the document would fail in case standard attention mechanism is used in conjunction with  relative positional embeddings. Therefore, the standard attention mechanism is equipped with 3 different bias terms to denote the learnable 1D and 2D biases along X and Y directions.&lt;/li&gt;
&lt;/ul&gt;

\[\alpha_{ij} = \alpha_{ij} + b_{j-i}^{(1D)} + b_{x_j - x_i}^{(2D_x)} + b_{y_j - y_i}^{(2D_y)}\]

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/Layoutv2Part2.png&quot; alt=&quot;Combining the Embeddings&quot; /&gt;
  &lt;figcaption&gt;Combining the Embeddings&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pretraining Tasks&lt;/strong&gt; Although, the Masked Visual Language Model pretraining task was retained, the document classification task was scrapped and in place 2 new training strategies were introduced.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Text Image Alignment&lt;/em&gt;: Randomly the portion in the document image, where a couple of text tokens lie are masked. The objective of the classifier being trained on the encoder outputs is to ascertain, whether the text token is visually present in the entire document image or not.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Text Image Matching&lt;/em&gt;: To gain a more coarse multi modal understanding the alignment task is slightly modified, and this time, the entire document image being fed could be a different page altogether, and the classifier needs to determine, whether the image and text belong to the same page or not.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It must be noted that all these tasks are carried out parallely with a combined loss function to prepare the pretrained model.&lt;/p&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/Layoutv2Part3.png&quot; alt=&quot;Pretraining tasks&quot; /&gt;
  &lt;figcaption&gt;Pretraining tasks&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;global-table-extractor&quot;&gt;Global Table Extractor&lt;/h3&gt;

&lt;p&gt;Both frameworks introduced till now focussed on improving their pretraining methods so as to be able to perform better on all document AI tasks, not just table detection and extraction. Global Text Extractor (GTE) &lt;a class=&quot;citation&quot; href=&quot;#zheng2021global&quot;&gt;(Zheng et al., 2021)&lt;/a&gt; takes a different approach by focussing on just extracting tables. The core idea behind GTE is to try and find individual cells from tables and relate the structure of the detected cells to one another and eventually identifying the entire table.&lt;/p&gt;

&lt;h4 id=&quot;contributions-in-terms-of-datasets&quot;&gt;Contributions in terms of datasets&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the process of the development of such a framework, the authors managed to &lt;strong&gt;enhance PubTabNet&lt;/strong&gt; by adding the cell structure annotations. The HTML version and the PDF version of the same document is matched. The HTML structure gives the logical structure of table cells, while the PDF gives the exact bounding boxes of each word. The combination of the 2 streams of data, therefore are useful in exactly annotating the boundary of each table cell.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tables differ a lot based on domains, and thus to not improve the performance of the GTE framework, a new dataset called &lt;strong&gt;FinTabNet&lt;/strong&gt; was curated. This dataset contains annual reports from S&amp;amp;P 500 companies. Processes similar to the enhanced PubTabNet were used to provide the table and the individual cell bounding boxes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;architecture&quot;&gt;Architecture&lt;/h4&gt;

&lt;p&gt;The entire GTE framework is construed by a combination of a few object detectors used in a sequence. The object detection system used here is repplacable, and can be chosen based on convenience. The 2 major components in the framework are GTE Table and GTE Cell that are used for table detection and cell structure recognition respectively.&lt;/p&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/GTE.png&quot; alt=&quot;Overall Architecture&quot; /&gt;
  &lt;figcaption&gt;Overall Architecture&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;GTE Table&lt;/strong&gt;: A object detector trained to find only cells, and another to find tables are run in parallel. It should be noted that the cell detector does so, without the knowledge of the overall bounding boxes of the tables. In addition to the standard loss functions of the individual object detectors, a cell constraint penalty loss function is utilized. This loss function penalizes the outputs of the table detector by comparing the tables detected with the cells produced. A few simple rules dictated by the structure of tables are used to calculate the penalty.
    &lt;ul&gt;
      &lt;li&gt;The percentage of the area covered by the cells inside a table is lower than a threshold.&lt;/li&gt;
      &lt;li&gt;Area just inside the table (around the boundary of the table), has too few cells. Since it is rare for the 1st row and columns to be empty in tables&lt;/li&gt;
      &lt;li&gt;Area outside the table contains cells.&lt;/li&gt;
      &lt;li&gt;The bottom of the table does not have many cells. Absence of enough cells in the final few rows indicate that the table boundary could have been drawn earlier. 
The hyperparameters used to specify each of the thresholds in these constraint conditions are provided in detail in the Supplementary section of the paper. The final constraint loss function is thereby used to score the table bounding boxes and again, and final predictions are made based on these new rankings.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/GTETable.png&quot; alt=&quot;Structure of GTE Table&quot; /&gt;
  &lt;figcaption&gt;Structure of GTE Table&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;GTE Cell&lt;/strong&gt;: The table locations obtained from the output of the GTE Table component is masked in the original full page image that was fed to GTE Table. The masked image is then fed into a network to determine the kind of subsequent cell detection network it should be fed to. In some tables, cells are demarcated based on drawn lines (both horizontal and vertical), while in some cases, the boundaries of the cells are understood, and not explicitly drawn. Therefore, a preliminary check to determine, whether the drawn lines are useful demarcators or not is done by the first network. The output for the cell detectors is then post processed, so that any text box inside the table that did not overlap with any of the cell bounding boxes does not go unassigned.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/table_extraction/GTECell.png&quot; alt=&quot;Structure of GTE Cell&quot; /&gt;
  &lt;figcaption&gt;Structure of GTE Cell&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Note: The performance on individual datasets for all the frameworks mentioned here were state of the art when they were published, and the optimal hyperparameter configurations for each of the tasks are mentioned in detail in the respective papers.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;xu2020layoutlm&quot;&gt;Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., &amp;amp; Zhou, M. (2020). Layoutlm: Pre-training of text and layout for document image understanding. &lt;i&gt;Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining&lt;/i&gt;, 1192–1200.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;xu2020layoutlmv2&quot;&gt;Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C., Che, W., &amp;amp; others. (2020). LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. &lt;i&gt;ArXiv Preprint ArXiv:2012.14740&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zheng2021global&quot;&gt;Zheng, X., Burdick, D., Popa, L., Zhong, X., &amp;amp; Wang, N. X. R. (2021). Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. &lt;i&gt;Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision&lt;/i&gt;, 697–706.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Yash Sarrof</name></author><category term="Technical" /><summary type="html">Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field.</summary></entry><entry><title type="html">SAdam: An Adam variant that converges faster for convex loss functions</title><link href="http://localhost:4000/projects/SadamOptimizer.html" rel="alternate" type="text/html" title="SAdam: An Adam variant that converges faster for convex loss functions" /><published>2021-03-29T00:00:00+05:30</published><updated>2021-03-29T00:00:00+05:30</updated><id>http://localhost:4000/projects/SadamOptimizer</id><content type="html" xml:base="http://localhost:4000/projects/SadamOptimizer.html">&lt;p&gt;The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent  regret bound where  is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent  regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Projects" /><summary type="html">The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent regret bound where is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.</summary></entry><entry><title type="html">Agony, expectation, relief &amp;amp; the journey in between</title><link href="http://localhost:4000/thoughts/Hopeless.html" rel="alternate" type="text/html" title="Agony, expectation, relief &amp;amp; the journey in between" /><published>2020-12-18T00:00:00+05:30</published><updated>2020-12-18T00:00:00+05:30</updated><id>http://localhost:4000/thoughts/Hopeless</id><content type="html" xml:base="http://localhost:4000/thoughts/Hopeless.html">&lt;p&gt;Somewhere in the middle of February 2020, I was moving into my first rented house. The place was perfect, it was isolated enough from the main roads to be quite and sombre at night, whilst also being a 10 minute walk from the Samsung R&amp;amp;D Bangalore Campus where I was interning. The internship stipend was healthy, and so life in the city was pretty exciting. Weekend get togethers, making my own food, collaborating on expensive side projects that previously wouldn’t even have crossed my mind, life had almost never been better. I had already applied for my Masters to the college of my choice KU Leuven in Belgium. However, I was banking on my internship being converted to a placement offer by Samsung, since I really liked the work and the environment around me. Working for a couple of years at this company to build up my capital and then applying again would have been the ideal scenario. By then, the first news of COVID appearing in India had already started coming in. As always, first warnings are rarely paid heed to by people at large, and I was no different. Eventually reality did kick in, and by the time complete lockdown was announced, there was panic amongst all the people I knew in Bangalore. Most of them started booking their flights to go back to their home towns in order to not be stuck. Me and a couple of other friends that I had made at Samsung, decided to stay back in Bangalore during the lockdown, hoping for things to improve quickly and avoiding the risk of travelling. I had not anticipated for the lockdown to keep increasing from the initial proposal of 21 days. There was a partial relaxation after around 45 days though, and our office campus was opened to 10% of the employees. I requested to be allowed in the campus, since availing the services of the office pantry would have significantly helped me avoid preparing all 3 meals at home everyday without a regular supply of essentials. My division head agreed, since most of the full time employees were reluctant to come to the office, as they lived with their families and did not want to take any risks.&lt;/p&gt;

&lt;p&gt;Around this time I found out that I had gotten a admit in KU Leuven and was relieved to have secured a safe backup, in case my internship did not convert. The process of getting the placement offer hinged on a positive appraisal by team managers (which I had already received) and me passing the “Advanced Test” of Samsung. The caveat was that, Samsung conducts these tests on their premises or on college campuses only when there are sufficient enough people to give the test. The test was originally supposed to happen in the last week of March, but owing to lockdown and the flurry of people that had gone back to their home towns, the test kept getting postponed. My team manager in the last week of my internship in June 2020, told me that he had tried his best to find a loophole of sorts to get me to stay a little longer to be able to take the test as soon as it was feasible, but his efforts hadn’t borne any fruit. The last communication from the HR ended on a positive note, when they said that they were looking at the possibility of holding the test online. They had also given indications albeit unofficially, that the advanced exam would probably be held in the month of September. I was sent links to 2 consecutive practice tests that they were using to gauge the feasibility of actually relying on the online exam completely. My backup of going for Masters right away was also falling apart as there were no signs of the Belgium Consulate opening for visa appointments anytime soon in any part of the country. Staying in Bengaluru whilst job hunting and incurring the expenses for rent without a stable income made little sense. Therefore in the last week of June 2020, I came back to my hometown of Kolkata.&lt;/p&gt;

&lt;h3 id=&quot;a-bad-decision&quot;&gt;A bad decision&lt;/h3&gt;

&lt;p&gt;I could have started my job hunt right away and taken the best offer at hand. Whenever Samsung would reply back with the link to the Advanced test, I would give the test and on passing, again evaluate the better job. In hindsight I should have chosen to do this. Today, I dont relate to the idealistic world view I held at that time. I was probably smitten by the work environment that I had seen at Samsung, and desperately wanted to work there. In either case, I chose to take up a 3 month internship in the R&amp;amp;D department at a local firm called Fortuna Impex Private Limited within my first week of arrival at Kolkata. The rationale behind this was to just keep myself occupied till the Advanced test, clearing which, in my head was child’s play. The work environment at this firm however turned out to be awful. My colleagues would procrastinate and only work when the CEO would come knocking around asking questions. There was no defined hierarchy amongst the leadership ergo conflicting instructions about the next steps in the project were a routine occurrence. I took advice from my relatives who were entrepreneurs based in Kolkata, who told me that these things were commonplace and I simply needed time to get used to the work culture in Bengal. I decided to continue after listening to their views, and hoped that things would improve.&lt;/p&gt;

&lt;h3 id=&quot;when-it-rains-it-pours&quot;&gt;When it rains it pours&lt;/h3&gt;

&lt;p&gt;15th August, Independence day in India is a public holiday, and a lot of essential services are also generally closed. This going to sound comical, but I was skimming through the day’s newspapers, glad to have gotten an off day from the office, when I took a yawn and my mouth got stuck. I literally wasn’t being able to do something as simple as close my mouth. I tried for a while, pressuring it with my hands, but it was stuck. Helpless, I went to my younger brother, who didn’t understand what was happening. With my mouth stuck, I was just making noises and wasn’t being to speak. I somehow gestured to him to pass me a pen and some paper. I wrote that my mouth was stuck, and he started laughing, thinking I was making a joke. After a while, he realized this was a serious problem, and we woke our parents from their afternoon nap. They suggested a heat compress to calm the nerves, but nothing seemed to work. We tried to call up the clinics we knew of nearby, and all of them did not have a dentist scheduled to come that day. The emergency services were also not willing to respond right away, since they were swamped with COVID cases at that time, and wanted a RT-PCR report before taking me in. We eventually discovered a doctor who lived a few minutes away and was willing to help us out. We went there and the doctor closed my mouth. My mouth had remained stuck in an open position for around 90 minutes, and I was in a lot of pain. He said that he had forcefully closed it at the moment, and that I in all possibility had severe TMJ (temporomandibular joint) disorder. He suggested we cover up skull with a bandage tightly so that my mouth does not open much and that I visit a maxillofacial surgeon at the earliest, since this could recurr at any point. I consulted with Mr. Utsa Butta, who I had found was one of the best in this field. After multiple visits, blood tests, RT-PCR tests and sleep studies, I was diagnosed with level 2 of TMJ and was told that, that the entire treatment could span over 3 years, but it was curable. We got started with the treatment plan right away. I had never contemplated quitting an internship before, but the pain, being on a liquid diet and hence low on energy coupled with the toxic work environment were too much for me, and I tendered my resignation in the the 1st week of September, and after completing the knowledge transfer in the next few days, left the internship on the 12th September.&lt;/p&gt;

&lt;p&gt;The steadily rising COVID cases in Belgium and the constant unabated pain in my Jaw meant that the plan of going for Masters right away had to be dropped. I decided to wait for the treatment to complete and only then apply for Masters again. Around this time, I contacted my ex team manager at Samsung, who hinted that HR had other priorities and were not even considering the Advanced test for the interns. Owing to a mixture of naive decision making, an unusual health problem and an unforeseen global crisis, a promising situation in March had gradually turned into a grim situation in September. I finally started looking for jobs that allowed remote work so as to not hinder my treatment.&lt;/p&gt;

&lt;h3 id=&quot;trying-to-navigate-out-of-the-muddle&quot;&gt;Trying to navigate out of the muddle&lt;/h3&gt;

&lt;p&gt;Over the course of October, I submitted a plethora of applications to a lot of jobs. I got my first break in the 2nd week of October, when I cleared multiple rounds for the post of a Junior Research Fellow at IIT Kharagpur. The posting was remote till at least a couple of months, and even after that, Kharagpur was 3-4 hour train journey from my place. I made it to the last round, eventually losing out to the only other applicant who had made it there. I was not making a lot of headways in other companies of my choice, and all throughout the feeling of being worthless for not being able to advance my career in any way whatsoever, was difficult to shake off.&lt;/p&gt;

&lt;p&gt;Finally, I got my first job offer at Bharat Electronics Limited (BEL). During the final interview, I had been informed by the panelists that the joining would be within 2 weeks. I had to submit some bit of paperwork, and was told that within a week of completing the same, I would get an email with the joining date. Having learnt the lesson from last time, I intended not to stop the application process till I got the joining date, and indeed weeks upon weeks passed and there was no communication from them.&lt;/p&gt;

&lt;p&gt;In the first week of December, I got an email from Crossover, stating that I had made it to the final round. It was one of the first companies that I had applied to, and had cleared all their preliminary rounds in September itself. I was surprised by the late response but scheduled an interview right away. The interview went well, and soon after a job offer came along. They wanted me to start within 4 days on the coming Monday. However, the job offer was contingent on clearing an aptitude test that would be taken on the 1st day of work. The company had changed the format for their aptitude test deeming the test that I had given initially, invalid. I accepted the offer without any qualms, as this job fulfilled all the criterias in my checklist. I got done with an impending minor tongue surgery (lingual frenectomy) which was a part of the treatment for TMJ. I had been putting it off, since it can take around 7 days until after the surgery to start speaking coherently again, and I wanted to get it done, once I knew there were not going to be any clashes with any interviews.&lt;/p&gt;

&lt;h3 id=&quot;darkest-before-the-dawn-they-say&quot;&gt;Darkest before the dawn, they say&lt;/h3&gt;

&lt;p&gt;I remember the day clearly, it was the 14th of December, my intended 1st day at the job at Crossover. I had worked for about 3 hours, when I got the mail with the link for the aptitude exam. There was a personal proctor that was assigned to me for the duration of the test, which was supposed to have 50 questions, that I had to answer in 30 minutes. Around 15 minutes into the test, something that never happens in my house happened, the electricity in my house went away. It takes about 2-3 minutes for the generators to get the power back on. The proctor deemed the test invalid for the interruption that had happened. I could not fight back properly on call, since I had impaired speech at that time, but I explained the hapless nature of it all, soon after in an email thread. Subsequently, I was given another chance to give the test after around 6 hours. In the meantime, I continued working as it was still supposedly the 1st day at the job. I also  arranged a Tier 3 Internet connection in a matter of hours, and told my parents to buy a small inverter from the local shop for the room that I was working in. I gave the test without any interruptions. Within minutes of completion, I got an email from the Crossover team with the results. I had scored 44/50 in that test. I broke down in disappointment, as the cut off was 45. After 3 months of hustling, I was back to square one.&lt;/p&gt;

&lt;p&gt;On 15th December I went back to the Doctor, for him to inspect my recovery post the frenectomy. I was told, that I should be able to start speaking within a day or two. I had stopped applying to new places, ever since the surgery, and I completed my first application since then on the 16th Night. On the morning of 17th December, I finally felt my voice again and was being able to speak clearly albeit in a low voice. I went through the usual drills, looking for callbacks, or emails regarding the applications that I had made. It had been 7 weeks since the BEL job offer, and there was still no word from them. Having found nothing, I had started looking for more opportunities, when I saw a call from an unknown number. I picked it up, and was surprised to know that one of the CEOs of the startup that I had applied to last night had called me. He said, that he had looked at my profile and wanted to proceed for an informal interview right away. I didn’t mind, and after about 30 minutes of an impromptu interview, he said that he wanted to take a final technical interview, and if possible wanted to get it done right away. He sent me a link to a video conference, where there were 3 other panelists. The interview spanned more than 2 hours, and at the end, Mohit who had taken my first interview said that he would get back to me, by the end of the day. Having experienced multiple hollow claims in the past couple of months I had started to disregard such assertions unless given to me in writing. But true to his word, Mohit did call me within 3 hours, and made me a job offer. After negotiating on the terms for a while, I accepted. I was to start the job on literally the next day, which incidentally happened to be my birthday as well.&lt;/p&gt;

&lt;p&gt;I have long been a repudiator of the theories involving happenchance. However, the complete turnaround from having no hope of securing a job in the near future and an inability to speak intelligibly, that just somehow happened to coincide with my birthday, to this day makes me reconsider my views on fate.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Thoughts" /><summary type="html">Somewhere in the middle of February 2020, I was moving into my first rented house. The place was perfect, it was isolated enough from the main roads to be quite and sombre at night, whilst also being a 10 minute walk from the Samsung R&amp;amp;D Bangalore Campus where I was interning. The internship stipend was healthy, and so life in the city was pretty exciting. Weekend get togethers, making my own food, collaborating on expensive side projects that previously wouldn’t even have crossed my mind, life had almost never been better. I had already applied for my Masters to the college of my choice KU Leuven in Belgium. However, I was banking on my internship being converted to a placement offer by Samsung, since I really liked the work and the environment around me. Working for a couple of years at this company to build up my capital and then applying again would have been the ideal scenario. By then, the first news of COVID appearing in India had already started coming in. As always, first warnings are rarely paid heed to by people at large, and I was no different. Eventually reality did kick in, and by the time complete lockdown was announced, there was panic amongst all the people I knew in Bangalore. Most of them started booking their flights to go back to their home towns in order to not be stuck. Me and a couple of other friends that I had made at Samsung, decided to stay back in Bangalore during the lockdown, hoping for things to improve quickly and avoiding the risk of travelling. I had not anticipated for the lockdown to keep increasing from the initial proposal of 21 days. There was a partial relaxation after around 45 days though, and our office campus was opened to 10% of the employees. I requested to be allowed in the campus, since availing the services of the office pantry would have significantly helped me avoid preparing all 3 meals at home everyday without a regular supply of essentials. My division head agreed, since most of the full time employees were reluctant to come to the office, as they lived with their families and did not want to take any risks.</summary></entry><entry><title type="html">It’s not what they are underneath but what they do, that increases producitivity</title><link href="http://localhost:4000/technical/LinuxTools.html" rel="alternate" type="text/html" title="It’s not what they are underneath but what they do, that increases producitivity" /><published>2020-10-10T00:00:00+05:30</published><updated>2020-10-10T00:00:00+05:30</updated><id>http://localhost:4000/technical/LinuxTools</id><content type="html" xml:base="http://localhost:4000/technical/LinuxTools.html">&lt;p&gt;As a Computer Science engineer every miniscule jump in productivity in and around the workspace tends to have a butterfly effect, and ends up being a huge time savior. Over the years, I have disovered a few open source programs that have had a similar effect for me, and despite a oblique learning curve for most of them, they have become indispensible aspects of my working environment. It should be noted that most of these tools are specific to Linux, and anyone using other some other Operating System might not find much value from this post. Another common thread amongst everything listed here is that they are all open source, and have an active community. The configuration files for my system can be found &lt;a href=&quot;!https://github.com/yashYRS/dotfiles&quot;&gt;here&lt;/a&gt;, although I would urge everyone to rice their own systems, since the configurability of each and every tool is what makes Linux so beautiful to work with.&lt;/p&gt;

&lt;h3 id=&quot;newsboat&quot;&gt;Newsboat&lt;/h3&gt;

&lt;p&gt;The amount of information that needs to be consumed on a daily basis keeps compounding over time, if not kept in check. There are many a technical blogs that one can follow to keep in touch with the latest trends in the industry. However, again filtering out the posts that are relevant to one’s domain can end up being time consuming and tiresome. I used to navigate to each such blog every time a mail notification would come up. I came across the concept of RSS feeds and stumbled upon newsboat, when I looked around for suggestions to reduce the time taken for reading each of these blogs. On Homebrew, newsboat is included by default, however I had to download it in my Ubuntu system. It is essentially a HTML renderer, and enables one to read the RSS feeds on a terminal.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/productivity/newsboat_post.png&quot; alt=&quot;How a post would look like on a RSS-feed reader&quot; /&gt;
  &lt;figcaption&gt;How a post would look like on a RSS-feed reader&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;There aren’t many dependencies, and can be installed easily from one’s relevant package managers.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt install newsboat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Once install is complete, add the links to the blogs one wants to follow by adding it to the &lt;em&gt;.newsboat/urls&lt;/em&gt; file in the .newsboat folder. At the end of the link, &lt;em&gt;tags&lt;/em&gt; can be added to each of the links, to filter posts.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/productivity/newsboat_bloglist.png&quot; alt=&quot;Newsboat greets you with the list of blogs added to the config&quot; /&gt;
  &lt;figcaption&gt;Newsboat greets you with the list of blogs added to the config&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;The feeds can be refreshed manually, however I have added this command to &lt;em&gt;crontab&lt;/em&gt;, and the feeds therefore refresh every 30 minutes for me.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  # To Manually refresh
  newsboat -r
  # To add to crontab
  crontab -l | { cat; echo &quot;*/30 0 0 0 0 newsboat -r&quot;; } | crontab -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Newsboat has vim bindings for navigating through the posts and blogs, although the key bindings can be configured as per one’s preferences. To get a comprehensive deep dive into some of the features, would recommend going through the well maintained official &lt;a href=&quot;!https://newsboat.org/releases/2.10.1/docs/newsboat.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;i3wm&quot;&gt;I3wm&lt;/h3&gt;

&lt;p&gt;I remember being awestruck when I saw my friend Rohit working on his system. The speed at which he was switching between applications, the core idea of not having to use one’s mouse at all while coding unless absolutely necessary was on display. I asked him what OS he was using, and he smirked telling me that it wasn’t a different OS, just a different window manager called i3. I had decided right then and there to try it out, and have not looked back ever since. Traditional window managers, on most desktop environments are floating managers, where each window needs to be resized appropriately. The workspaces also need to be manually set everytime one wants to separate aspects of development. It is not much of a problem, except when using a tiling manager, all of this is handled automatically. There are many a tiling window managers, and i3 is just one of the many alternatives out there. It is my preference, owing to the configurability and the active community online, which comes around to my rescue, everytime I get stuck with something. It should be noted that using i3 out of the box, is not that great a idea. The true power can be only realized, once one tunes it to adjust it for own’s needs.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/productivity/i3_tiling.png&quot; alt=&quot;An example of the way i3 tiles applications&quot; /&gt;
  &lt;figcaption&gt;An example of the way i3 tiles applications&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;For example, I have separated my workspaces into 6 categories (web, code, terminal, documents, calls, media) and by default the applications will always open in their respective workspaces, irrespective of where I give the command for opening them. Of course, one can easily move the applications around, according to immediate need. However this default action helps me in switching between my working environments with incredible speed. I have key bindings for virtually anything and everything that I do on a daily basis, and thus overall, the experience of using my system is very pleasant. In addition to the extensive flexibility that i3 provides, it is minimalisitc, and hardly uses up resources. If I haven’t sold it enough already, here are some pictures of i3 setups from &lt;a href=&quot;!https://www.reddit.com/r/unixporn&quot;&gt;unixporn&lt;/a&gt;, to inspire more people to use it.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/productivity/i3_sample.jpg&quot; alt=&quot;Just start using it already&quot; /&gt;
  &lt;figcaption&gt;Just start using it already&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;ranger&quot;&gt;Ranger&lt;/h3&gt;

&lt;p&gt;I must have made it obvious by now, that I favour the terminal heavily. So when I came across a tree file manager completely based on the terminal, I had to switch to it. At the very onset, it might seem futile to push for something of this sort, since GUI file managers like nautilus have no problems whatsover. However the advantage of using ranger is that, it can do everything that a GUI manager can do, but in addition to that, has previews, which enables one to look at a snapshot of the contents of a file, before opening it. The vim bindings, configurability, speed brought about by ranger is incomparable. It is one of those tools that one just has to try before appreciating the beauty of it.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;center&gt;
  &lt;img src=&quot;/images/productivity/ranger_2.jpg&quot; alt=&quot;A sample of a preview of a video on ranger&quot; /&gt;
  &lt;figcaption&gt;A sample of a preview of a video on ranger&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;an-overextended-epilogue&quot;&gt;An overextended epilogue&lt;/h3&gt;

&lt;p&gt;Almost everything that I have recommended above, becomes valuable only if one can touch type and ideally also is acquainted with vim bindings. I recommend &lt;a href=&quot;https://www.keybr.com&quot;&gt;keybr&lt;/a&gt; for learning to touch type, &lt;a href=&quot;https://www.monkeytype.com&quot;&gt;monkeytype&lt;/a&gt; for testing your typing speeds reliably, and &lt;a href=&quot;!https://www.vimgenius.com&quot;&gt;vimgenius&lt;/a&gt; for pratising your vim bindings. And again, this is more of a preference, but browsers don’t really come better than &lt;a href=&quot;https://brave.com/&quot;&gt;Brave&lt;/a&gt; , which uses a Chromium engine called Blink at its core. The main advantage of using it is the complete control it provides over ads, cross-site cookies and trackers across websites. Besides the privacy benefits, it also has a great rewards program given out in form of their natively developed cyptocurrency called BAT (Basic Attention Token priced at $1.18 at the time of writing the post) and provides a local wallet to store them. And yeah, it also consumes way less resources compared to Chrome.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Technical" /><summary type="html">As a Computer Science engineer every miniscule jump in productivity in and around the workspace tends to have a butterfly effect, and ends up being a huge time savior. Over the years, I have disovered a few open source programs that have had a similar effect for me, and despite a oblique learning curve for most of them, they have become indispensible aspects of my working environment. It should be noted that most of these tools are specific to Linux, and anyone using other some other Operating System might not find much value from this post. Another common thread amongst everything listed here is that they are all open source, and have an active community. The configuration files for my system can be found here, although I would urge everyone to rice their own systems, since the configurability of each and every tool is what makes Linux so beautiful to work with.</summary></entry><entry><title type="html">Using Gardner’s Multiple Intelligence Theory to gauge aptitude of children</title><link href="http://localhost:4000/projects/GardnerTheory.html" rel="alternate" type="text/html" title="Using Gardner’s Multiple Intelligence Theory to gauge aptitude of children" /><published>2020-03-10T00:00:00+05:30</published><updated>2020-03-10T00:00:00+05:30</updated><id>http://localhost:4000/projects/GardnerTheory</id><content type="html" xml:base="http://localhost:4000/projects/GardnerTheory.html">&lt;p&gt;The plan is to make a storytelling game, which tests the calibre of the child and his leaning towards a certain aspect of intelligence. The overall aim in the game would be to find where the treasure is, and in a bid to do that the player has to go through a series of adventures, with each adventure checking/testing one of the intelligence aspects as discussed by Gardner in his theory of Multiple Intelligence. The main challenge, in designing such a game is to hold the attention of the child, since a question answer based game becomes too boring for kids, no matter how much the parents advocates playing them, ergo the main motivation behind making the game in such a format is to make sure that the children never realize that they are being tested in any way whatsover. A score for all the 8 intelligence aspects isn’t produced for the children to see, since upon seeing a low score in any particular format, kids could get heavily discouraged and actually start believing that they aren’t good enough in that particular aspect. Instead, the proposal is to only show in the application the intelligence subtype in which the kid’s performance was the best, with encouraging taglines. There will be options to get the detailed analysis of the report as well for the parent (through email, not to be viewed in the game console). The detailed report would not factor just 1 session of the game, instead each session is judged and the report keeps getting updated. For multiple kids playing through the same device, there will be options to add player names, thus avoiding conflicts in the report.&lt;/p&gt;

&lt;p&gt;Example of some of the adventures - Note : If the performance in a particular aspect is good, more adventures covering that aspect pop up, also the difficulty levels keep increasing.&lt;/p&gt;

&lt;p&gt;Word Smart : There will be sample conversations between 2 people that are displayed, with certain clues to reach the final destination being hidden in the conversation. Thus if the child actually figures that out and uses the hint to move forward, points are awarded.&lt;/p&gt;

&lt;p&gt;Math Smart : A Time limit within which a lock has to be opened by solving some logic based puzzles (if solved in less time more points). Other activities include fitting different shapes(objects) in a square(shown as a suitcase), more the number of items, more the points awarded.&lt;/p&gt;

&lt;p&gt;Body Smart : Gravity based movement, along the lines of Subway Surfer, avoid obstacles along the journey, hand eye coordination of the kid gets tested in this manner. Scoring done with respect to the number of obstacles avoided, the time for which there was no collision.&lt;/p&gt;

&lt;p&gt;Picture Smart : Pictorially rich Maps to get to the destination given at the start of the game, if the shortest path is being taken according to the map, points awarded, further getting out of Mazes in certain parts of the game, solving jigsaw puzzles to open locks.&lt;/p&gt;

&lt;p&gt;Music Smart : There are pits placed along the path, on falling in them, the player dies, upon which the player can restart from the point where he/she had left upon completing some activity, eg: a GIF shown, the player has to choose amongst given options an appropriate background score. Other activities would include choosing amongst junctures where different sounds indicate what could be expected along a path, eg : sound of a tiger/ birds/ elephants coming from 3 different paths. If the safest path chosen, points are awarded.&lt;/p&gt;

&lt;p&gt;People Smart : There will be a lot of people that a player encounters along the game, a conversation can be initiated with each of them, and each can contribute in some way or the other. If the player interacts and makes them allies, points are awarded in this domain.&lt;/p&gt;

&lt;p&gt;Nature Smart : There will be chocolate bars available along the journey. On eating them, there will be options to throw the wrapper. There will always be a dustbin around if a bar/any other food item is provided. If the player makes a effort to go to the dustbin to throw the wrapper, points are awarded. Again, there will be activities where there is a time limit (a gateway closing), but some natural resources are being wasted (eg: tap running), the player has to sacrifice time, to stop the wastage (close the tap water).&lt;/p&gt;

&lt;p&gt;Self Smart : The player would come across some people who would come up to player and have a story to share. Upon hearing the story, there will be options to hear more/ignore the story and move on. If the player pays heed to the story, shows some empathy, provides some suggestions to the people who came up to him ,points are awarded.&lt;/p&gt;

&lt;p&gt;An important thing to note here is the fact that the adventures here are completely seperate from each other, so the order in which they appear apart from a few cases, would not matter. Thus our AI agent, would figure out where the child’s leaning is, and based on that not show some adventures at all, or show more of a particular kind of adventure.&lt;/p&gt;

&lt;p&gt;To develop the game, we would be using various modules available in Python (eg: pyglet, cocos2d ) and MongoDB (for back end) to make a PC based application with limited graphic support due to lack of domain expertise in game development and design.&lt;/p&gt;

&lt;p&gt;Future Extensions possible :&lt;/p&gt;

&lt;p&gt;The game could be made available on a lot of other platforms, so an android version of the app/ web based version could be developed.
The number of adventures in each of the subdomains could be added, with adventures being tagged more appropriately for age groups.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Projects" /><summary type="html">The plan is to make a storytelling game, which tests the calibre of the child and his leaning towards a certain aspect of intelligence. The overall aim in the game would be to find where the treasure is, and in a bid to do that the player has to go through a series of adventures, with each adventure checking/testing one of the intelligence aspects as discussed by Gardner in his theory of Multiple Intelligence. The main challenge, in designing such a game is to hold the attention of the child, since a question answer based game becomes too boring for kids, no matter how much the parents advocates playing them, ergo the main motivation behind making the game in such a format is to make sure that the children never realize that they are being tested in any way whatsover. A score for all the 8 intelligence aspects isn’t produced for the children to see, since upon seeing a low score in any particular format, kids could get heavily discouraged and actually start believing that they aren’t good enough in that particular aspect. Instead, the proposal is to only show in the application the intelligence subtype in which the kid’s performance was the best, with encouraging taglines. There will be options to get the detailed analysis of the report as well for the parent (through email, not to be viewed in the game console). The detailed report would not factor just 1 session of the game, instead each session is judged and the report keeps getting updated. For multiple kids playing through the same device, there will be options to add player names, thus avoiding conflicts in the report.</summary></entry><entry><title type="html">Some of my favourite mini-side projects</title><link href="http://localhost:4000/projects/AllMiniSideProjects.html" rel="alternate" type="text/html" title="Some of my favourite mini-side projects" /><published>2019-06-18T00:00:00+05:30</published><updated>2019-06-18T00:00:00+05:30</updated><id>http://localhost:4000/projects/AllMiniSideProjects</id><content type="html" xml:base="http://localhost:4000/projects/AllMiniSideProjects.html">&lt;p&gt;Like any other beginner to the field, I have experimented with different technologies, often times collaborating with others to learn more about a particular topic that spurred interest. That over time, has led to me making weird, unsurprisingly unsustainable projects in short bursts. However each of these projects acted as a gateway for me into a new domain, and therefore are memorable for me.&lt;/p&gt;

&lt;h3 id=&quot;earthquake-detection-system&quot;&gt;&lt;a href=&quot;!https://github.com/yashYRS/DetectEarthquake&quot;&gt;Earthquake Detection System&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The theme for Microsoft’s Code Fun Do++ for the year 2018, was Earthquakes. Each team was to make something that would help in the overall management of natural disasters.  &lt;a href=&quot;!https://github.com/tanmay2298&quot;&gt;Tanmay&lt;/a&gt; and  &lt;a href=&quot;!https://github.com/chhayankjain&quot;&gt;Chhayank&lt;/a&gt; were my teammates for the event. We had noticed that most other teams were working on either the prediction or management of earthquakes (making prototypes of platforms to crowdfund resources to distribute for alleviating the distress in affected areas). While brainstorming, we got to know that it takes considerable amount of time (as much as 20 minutes) for the disaster management authorities to be notified about any natural disaster that takes place. We instantly started working on something to reduce this time gap. Our final solution was extremely minimalistic and simple. We wanted to use live feeds provided by the government owned surveilance tools (cameras/satelite data) and periodically scan the frames (say every 30 seconds). For every camera, we would identify a static background object like a lamp post, or a zebra crossing or a street signal. These static background objects can be occluded owing to some foreground objects (humans, cars, bikes …) appearing. However, the idea was that we would have multiple such live feeds that were being scanned, and the chance of the occlusions appearing in sync in all of these cameras was virtually nill. In case of an earthquake, the shaking of the ground, the camera would result in all of the cameras reporting a change in the position of these static objects, and in case of this, we would immediately ping the concerned authorities.&lt;/p&gt;

&lt;h4 id=&quot;pseudo-code-for-detection-of-discrepancy-using-lanes&quot;&gt;Pseudo code for detection of discrepancy using lanes&lt;/h4&gt;

&lt;p&gt;The nmber of common lines between 2 consecutive frames is high but as the earthquake starts, due to the shifting of the lines, the average keeps dropping due to the constant shaking of everything, and a alert is issued
The assumptions made were that, the color for demarkating lanes are generally white. Although, it wasn’t a hard assumption, since there was a provision to override the color for cities with different coloring schemes.&lt;/p&gt;

&lt;p&gt;Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;frame&lt;/code&gt; refers to the current frame being examined, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upper_white&lt;/code&gt; &amp;amp; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lower_white&lt;/code&gt; controls the color range to identify car lanes. The loop given here is run periodically (every 30 seconds). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ave_list&lt;/code&gt; refers to the list that contains the historical ground truth against which the current extraction is compared.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Flag to show frame status
frame_status = POSITIVE

# detecting white lanes of a road ....
hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv, lower_white, upper_white)

# Finding the edges on the road
edges = cv2.Canny(mask, 75, 150)
edges = cv2.GaussianBlur(edges, (5, 5), 0)

try:
    # Detect white lines in the frame 
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=20)
    ave_list.append(lines)

    # If lines are not detected for the current frame, return the same 
    if len(lines) &amp;gt; 0:
        # Function to match the current extraction with the historical extraction 
        same = check_lines(ave_list)

		# If the number of similar lines is lower than a threshold
		# however the number of lines in the frame is high
		# the status of the frame is set to show negative
        if same &amp;lt; same_thresh and len(lines) &amp;gt; line_thresh:
        	frame_status = NEGATIVE
        else:
        	# If the status continues to be positive, the historical data is scrapped
        	# to keep the current extraction as the basis for future comparisons
        	ave_list = [lines]
    else:
    	frame_status = POSITIVE
except Exception as e:
    frame_status = NEGATIVE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another experimental feature of the prototype we had created, was to predict earthquakes based on a long standing theory, that animals can often times sense an impending disaster, and start getting fidgety hours before the actual calamity. As this hadn’t been proved yet, we wanted an accompanying analyzer along with our primary earthquake detection, which would capture animal movements in a zoo/ safari. We tracked their usual entry and exit from frames and stored this information into a database. Our goal was to collate this data with the times at which the earthquake was detected, and in case a correlation was found, animal movement could be permanently be used as a measure to predict occurrences of an earthquake.&lt;/p&gt;

&lt;h4 id=&quot;pseudo-code-for-recording-entry-and-exit-of-animals&quot;&gt;Pseudo code for recording entry and exit of animals&lt;/h4&gt;

&lt;p&gt;An assumption made was that, the live feeds that were to be provided for the zoos, to record animal movements would track a single animal per camera. Thus, identifying the animal in every iteration is not required, and the detection of any large moving object can be considered equivalent to the presence of an animal in the frame. Each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;frame&lt;/code&gt; is processed as follows&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# By default, every frame is considered be to empty
occupied = False

# resize the frame, convert it to grayscale, and blur it 
frame = imutils.resize(frame, width=500)
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

# if the first frame is None, initialize it
if first_frame is None:
    first_frame = gray
    continue

# compute the absolute difference between the current and first frame
frameDelta = cv2.absdiff(first_frame, gray)
thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]

# dilate the thresholded image to fill in holes
thresh = cv2.dilate(thresh, None, iterations=2)

# find contours on thresholded image
cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]

# loop over the contours
for c in cnts:
    if cv2.contourArea(c) &amp;lt; min_area:
        continue

    # If contour is large, something is occupying the frame
	# Therefore, record entry if previous frame was empty 
    if prev_status is False:
        occupied = True
    	save_entry_time(datetime.datetime.now(tz=timezone.utc))
        break

# if none of the contours were big enough, current frame is empty
if occupied is False and prev_status is True:            
	save_exit_time(datetime.datetime.now(tz=timezone.utc))

# previous status of frames updated
prev_status = occupied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sudoku-solver&quot;&gt;&lt;a href=&quot;!https://github.com/yashYRS/Sudoku_Solver&quot;&gt;Sudoku Solver&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;I had a long weekend somewhere around my 4th Semester, and was feeling bored. I had just completed the CS - 231n course, and wanted to start training models to solve for various tasks. I had also learnt the basics of image processing and wanted to work on something that required the same. An automated sudoku solver happened to be the first suggestion that I came across online while searching for potential ideas that I could work on that required both image processing and neural networks. In either case, it turned out to be a fun weekend, as I implemented it over the course of 3 days. The pipeline of the project was as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Identifying each individual square in a sudoku puzzle: The hard assumptions made for the input is that, the sudoku image has to be non-skewed, and the grid lines in the Sudoku have to be clear. If the input satisfies these conditions, puzzles are extracted from the entire sudoku image. In case, the input can’t be processed, there is a provision to input the puzzle manually. The pseudo code for extraction -&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Convert to gray scale
gray = cv2.cvtColor(sudoku_image, cv2.COLOR_BGR2GRAY)
# Blur to be able to detect all the edges in the image clearly
blur_gray = cv2.GaussianBlur(gray, (5, 5), 0)
# Use the Canny edge detector to get the edges on the screen
edges = cv2.Canny(blur_gray, 30, 90, apertureSize=3)


# Find lines greater than 160 pixels in length
lines = cv2.HoughLines(edges, 1, np.pi/180, 160, 0, 0)

# sort the lines based on the value of Perpendicular
lines = sorted(lines, key=lambda line: line[0][0])

# Filter the lines to only contain horizontal and vertical lines
h_list, v_list = filter_lines_based_on_angle(lines, puzzle)

# Find the intersection between horizontal and vertical lines
points = get_intersection(h_list, v_list)

# If number of intersection points detected is not 100
# the user is asked to enter the board manually
if len(points) == 100:
	# After identifying points, square images are formed from the points 
	puzzle = extract_individual_digits(image, points)
else:
	puzzle = enter_board_manually()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Extracting square images from the points in the image:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def_indidual_digits(image, points)
    board = []
    for row in range(9):
        row_board = []
        # goes till 9, since last diagonal not needed

        for column in range(9):
            x1, y1 = [int(i)+3 for i in points[row*10 + column]]

            # coordinates of the diagonals of the rectangle
            x2, y2 = [int(i)-3 for i in points[row*10 + column + 11]]

            # area of 1 box, each box has 1 digit
            temp_img = img[y1:y2, x1:x2]

            if(temp.size != 0):

                # to maintain uniformity with the model's requirements
                cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)

            	# Predict the digit found using a trained digit recognizer model
                row_board.append(predict_image(model, temp_img))

        board.append(row_board)
    return board
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Identifying the digits in each square image: I had initially trained a shallow neural net (Conv2d layer, Max Pool layer followed by a couple of fully connected layers) on the MNIST dataset for identifying the digits from the individual squares. I had added a few random images with empty squares and added them with the tag -1 to the MNIST dataset for being able to predict on empty squares as well. However later, I changed the pipeline to a Logistic Regression model trained on the original dataset and a simple check before prediction to check if the input image contained a digit. The pseudo code -&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# model_path is the file, from where the trained model is loaded
model = LogisticRegression()
model.load_state_dict(torch.load(model_path))

# Preprocess and flatten the image for testing
image_tensor = torch.flatten(test_transforms(image))

# If the image is non empty, then the digit is identified by the model
if torch.sum(image_tensor) &amp;gt; non_empty_thresh:
	output_arr = model(image_tensor).detach().numpy()
    digit_identified = np.argmax(output_arr)
else:
	# The square does not contain any number, return -1 to denote the same
	digit_identified = -1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Solve the sudoku, based on the information gathered, and display the solution: I simply used backtracking to solve the sudoku. There are innumerable posts explaining backtracking (&lt;a href=&quot;https://www.geeksforgeeks.org/sudoku-backtracking-7/&quot;&gt;refer&lt;/a&gt;), hence am not providing the pseudo code here.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;alpha-beta-pruning-to-make-a-game-engine&quot;&gt;&lt;a href=&quot;!https://github.com/yashYRS/Tic-Tac-Toe&quot;&gt;Alpha Beta Pruning to make a game engine&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The perception that I had about AI when I was in high school was almost entirely built up by the hype around how no chess grandmaster was capable of beating a chess engine (that and JARVIS in Ironman xD ). It was inevitable that the first thing I started reading about, after learning the fundamentals of programming was how to make a chess engine. Today, I appreciate the intricacies of AI and my understanding of the field has evolved since, but when I read about alpha-beta pruning, the elegance of it all to make engines suitable for games with perfect information, removed some of the mystique behind something I had been utterly fascinated by. The game that I chose to make was Tic-Tac-Toe, since I did not want to spend time, on coding up the graphics of chess, but was just interested in making alpha-beta pruning work. It took me a weekend’s time to make this game, but till date, whenever I am bored, I just fire up the game and start playing. There are a plethora of posts explaining alpha-beta pruning (&lt;a href=&quot;https://iq.opengenus.org/alpha-beta-pruning-minimax-algorithm&quot;&gt;refer&lt;/a&gt;), if anyone starting out wants to explore it.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Projects" /><summary type="html">Like any other beginner to the field, I have experimented with different technologies, often times collaborating with others to learn more about a particular topic that spurred interest. That over time, has led to me making weird, unsurprisingly unsustainable projects in short bursts. However each of these projects acted as a gateway for me into a new domain, and therefore are memorable for me.</summary></entry><entry><title type="html">The Smart India Hackathon Experience</title><link href="http://localhost:4000/thoughts/SIHExperience.html" rel="alternate" type="text/html" title="The Smart India Hackathon Experience" /><published>2019-03-11T00:00:00+05:30</published><updated>2019-03-11T00:00:00+05:30</updated><id>http://localhost:4000/thoughts/SIHExperience</id><content type="html" xml:base="http://localhost:4000/thoughts/SIHExperience.html">&lt;p&gt;On some night in January 2019, me and my roommates Aadil &amp;amp; Sushant were discussing how the hectic schedule in our college, and all our other commitments for improving our profiles, virtually meant that we weren’t spending time with each other at all. We all agreed that the only way to actually hang out more was to work on the same project. We started looking out for options, and after carefully weighing in our alternatives, we decided on participating in the Smart India Hackathon. The decision was guided by the kind of projects on offer, and a time frame that would have allowed to us to maintain our other commitments, manage classes and assignments and still give about 20 hours per week to this project. A blatant flaw with this plan, that we realized much later was that this competition was one which a lot of people from all across the country looked forward to, with some even spending more than a year preparing for it. For us, the first round of the competition was about 20 days away, with the final round in about 2 months time. There was another catch, each team had to have a minimum of 6 members, and after a mix of deliberation &amp;amp; persuation over the next week, we managed to form a team, adding Daniel, Arshia and Damodar to our team, all of whom were our batchmates.&lt;/p&gt;

&lt;h3 id=&quot;the-1st-round&quot;&gt;The 1st round&lt;/h3&gt;

&lt;p&gt;The competition portal had released the problem statements somewhere around July 2018, and the format of the competition was that each such problem statement had a solitary winners prize. Each category’s winner would be selected out of the 4 teams, that would be selected after the 1st round. However, in case a problem statement did not have impressive enough submissions, that problem statement would be scrapped. Each team was allowed to submit proposals to 3 unique problem statements. The challenge was to select a problem that had a high probability of being chosen by others as well, besides thinking about a viable solution to the same. The initial discussions between our team did not not bear any fruit owing to a sense of disharmony between our team members. Some of the members were meeting with each other for the first time, and were not happy with the inequity in the work that was being done. At that time, we had to make a decision we had been putting off till now - selecting a team captain. Again, since the team captain would have to take care of a lot of the administrative side of things without getting any extra credit in the end result, nobody volunteered to take the part. Eventually I agreed to take up the role after a blind vote, where we chose the people we wanted to see become the captain. I distinctly remember the first discussion that we had post that vote. We all were standing in a circle outside our food court, faces looking grim in the team. We all pledged to dedicate a substantial amount of our time to this project, agreeing that taking it lightly and just continuing to act on whims would lead to nothing but a waste of time. Over the next week we all warmed up to each other, working till late at night on somedays to brainstorm over ideas. We eventually narrowed down 2 problems, and submitted proposals for them. The 1st problem statement was to build an application to find a nearby parking spot and the 2nd one was to build a game based on Gardner’s theory of Intelligence to gauge the aptitude of a child, and recommend possible career options he/she might be suited for. After submitting our proposals on the final day, we felt that our solution for parking spot finder was the one that had a higher chance of being picked up if at all.&lt;/p&gt;

&lt;h3 id=&quot;a-surprise-and-some-second-thoughts&quot;&gt;A surprise and some second thoughts&lt;/h3&gt;

&lt;p&gt;After having made the submission, the next few days were quiet as our sessional exams had started. Once our exams got over, we went about inquiring about in all the departments of our college about the other teams that had participated from our college. We found out, that 4 more teams had participated just like us, and we also found out that our college campus was one of the places where the final round was going to be held. After another week, we eventually did get the results of the 1st round. We were the only team that actually did make it through from our college in the 2nd round, but to our surprise, we actually were selected for the problem statement that we were less confident in. But to our dismay the center for participation for the 2nd round for our problem statement specifically, was in Jaipur about 2000 Km away from our college campus in Manipal. Since the 2nd round was only a fortnight away, we seriously thought of quitting the event, since booking air tickets at such a late juncture would have cost us more than our combined prize money. The other option was to travel via train, which would involve 5 days of travel time to and from, with a 2 day stay in the Jaipur center for the competition. The issue here was that our college did not ususally provide any kind of support for students travelling to other venues for participation, which implied that the labs and assignmemnts we would miss during that period wouldn’t be rescheduled for us and our grades would get heavily affected. And in all of this, we were not even thinking about the extra time and effort that would be required to get a prototype for our proposal ready by the day of the final competition. We again had a long meeting amongst ourselves, and eventually agreed that such an experience of being in the final four of a national level competition should not be missed upon. We charted out the plans, set up timelines, completed a lot of the administrative work which included creating tshirts for every member in the team, getting permissions from multiple departments in the college, getting printed banners and standees all of which was mandatory. The only bit of support we got in the entire process was from the guy we went to for making the standees, who promised not charging us, if we won the competition. We also had to select a team name. Having seen Mr. Robot recently and in a bid to get over the formalities, we named ourselves PySociety. By this time, the team had really bonded and spirits were generally high, when we were working together. So, the actual goal of getting to hang out more with friends was somewhere being accomplished.&lt;/p&gt;

&lt;h3 id=&quot;the-journey&quot;&gt;The journey&lt;/h3&gt;

&lt;p&gt;About 60 % of the work had been completed, when we were about to board the train from Manipal. A long journey lay in front of us, the first train would take us to Mumbai, where we would have to wait for about 9 hours, before boarding the train for Jaipur. We had kept local copies of documentations, had taken enough power backup to be able to work without internet during the train journey. We arrived at Mumbai, the Chattrapati Sivaji terminal station somewhere around 3 in the night. Aadil in our team hailed from Navi Mumbai, so after the mandatory walk along Marine Drive where the sea front just looks majestic, we headed towards his home. Aadil’s Mom had made biriyani for us and to this day, I haven’t had biriyani that has tasted as good. We left for Jaipur in a jovial mood that persisted throughout the remaining journey. At the time of our arrival at the Jaipur center around 4 PM on the 1st March, 75% of the work was completed. We were satisfied with the progress, but we weren’t able to get any more work done on that day owing to a overdrawn opening ceremony in the evening. As the competition was due to start at 6 in the morning the next day we called in an early night. 5 of us guys were given one apartment by the organizers, and all the girls from the all the teams were staying in another block. The block in which we were staying was under maintenance and was isolated from the rest of the campus, giving the entire place an eerie look. In our apartment though, there were only 2 beds which could take no more than 3 people. We were given 2 extra mattresses which we kept in the living room. Me and Sushant were sleeping there peacefully and around 5 in the morning we heard vehement knocking on the door. Altough none of us had faith in the existence of spectral beings, the sudden outburst in such an isolated environment terrified us, and we literally started screaming in fear. Everybody in the apartment woke up, we opened the door and we saw no one. We stood outside, confused about the entire ordeal, and just then someone in the corridor shouted that it was just a local guard who liked to play such pranks.&lt;/p&gt;

&lt;p&gt;After that uncanny start, we got ready and made our way to the hall. There were to be 3 rounds of presentations over the course of the next 36 hours. There was little chance to fraternize with other teams since a considerable amount of work was still left. We got down to it and started working. About 14 hours in, we had our 1st round of presentation in front of the judging panel who gave us their feedback. We estimated that the with the progress that we had made, incorporating the feedback along with completing the rest of the project would take us around 10 more hours, and we would have time to spare before the final presentation. The competition rules mandated the presence of at least 2 members at the table assigned to us. We decided to give Sushant and Arshia who were in charge of integrating all the modules the entire night’s rest, since they would primarily be in action the next day. Aadil and Daniel decided to take a nap in the competition hall itself, since they thought getting up on cue would be easier. Me and Damodar tried to do our best, but by 7 in the morning, we were tired and were virtually sleeping with our eyes open when Sushant and Arshia came back and took charge. However we followed Aadil’s lead, and decided to take rest in the competition hall itself. Around 10, we were all semi-fresh when it was time for our 2nd round of presentation.&lt;/p&gt;

&lt;h3 id=&quot;panic-and-hysteria&quot;&gt;Panic and Hysteria&lt;/h3&gt;

&lt;p&gt;The judges in the 2nd round did not give us any suggestions, nor did they make any comments about anything they were seeing. We continued with our work, after their inspection. Around 1 o clock, we were pretty satisfied with our work, and all the modules were individually properly unit tested. The integrated application was working well, albeit enough tests had not been done on them. Seeing the progress, Aadil and Daniel decided to eventually go back to the rooms to get a power nap to catch up on sleep. Meanwhile, me and Damodar started working on improving the graphics and overall look of the application. Around 2, I sensed a bit of panic in Sushant’s eyes, he had been testing the entire application end to end for some time now. On being asked, he said that the application was randomly crashing in 1 out of 5 test runs and the logs weren’t clear enough to debug, what was causing the crash. We all left what we were doing to get to the bottom of this. Only Sushant and Arshia were experts in Flask, the web framework of our choice for the application. Although the others could support by continuously testing end to end, and trying to find the pattern of the crashes, it was up to the 2 to fix the bug. From having hours to spare with a complete application, to not having a stable application an hour before submission was due, anxiety was creeping in.&lt;/p&gt;

&lt;p&gt;The Education minister of our country at that time, Mr. Prakash Javedkar had come to our center and was meeting with all the participants. There were 32 teams at the venue, 4 each for the 8 problem statements that had been assigned this center. The orientation of the the tables was such that, there were 28 tables around in a circle, and there were 4 tables in the middle file. It would have been a huge honour to meet with a man of his calibre. We were seated in the middle file, but owing to the orientation we were not in his line of vision, and the team escorting Mr Javedkar could not have insisted him to turn around to the teams he had skipped, and thus he ended up meeting the 28 teams seated in the circle. We got back to attending to our crisis. At this point, we had identified that the application was only crashing while transitioning between games. There were 2 kinds of transitions, from the end of the game to the story scene, and from the story scene to launch a new game. We were working on narrowing down the issue, with about half an hour to go, when the team captains were asked to step out and come near the podium. It was frustrating to be interrupted at this juncture, but I had no option but to follow the rules. I was sitting there along with 31 other people, when suddenly the video in front of us started, and the prime minister of our country Mr. Narendra Modi was with us on video call. An address from him was least expected. For a moment, I forgot about the issue at hand and intently listened to what he had to say, and his vision for backing this event.&lt;/p&gt;

&lt;p&gt;I came back after listening to his rousing speech, with about 10 minutes to go before our final submission. Inspired from Mr Modi, I addressed my team appreciating each one of them for having worked so hard all these days, and to look at the positive side of the mess we were in, hoping that it would lift their spirits. All of them were listening keenly, when suddenly Damodar could not help himself and started laughing hysterically. Apparently the rest of the team had identified that the crash only occurred when the scene transitioned from the story mode to launch the 3rd level of the same game type. The scores from the 1st level were being passed on, instead of the 2nd one, and owing to a mismatch in the scoring patterns in the 3 levels, the crash was occuring. They had already made the change and tested the run a few times successfully. To this day, all 5 of them laugh at me for that “motivating speech”.&lt;/p&gt;

&lt;h3 id=&quot;the-winning-moment&quot;&gt;The Winning moment&lt;/h3&gt;

&lt;p&gt;Once again in great spirits we all headed for the final presentation, this time in a separate hall with extra panelists who had hitherto not been present. It all went smoothly, and we came back to the hall to interact with the other participants. One of the teams said that they had been working on the project for over a year, having had multiple interactions from the Ministry of Culture who were the guiding force behind the problem statement we were a part of. Another team turned out to be former champions of the event, having won the inaugral edition of the hackathon back in 2017. We discussed our proposals with each of them, fraternized with the other teams who had were submitting solutions for other problem statements. It was a lively atmosphere, and we almost did not realize that it was time for the winners to be announced. Every team assembled back at their tables. We got into one final huddle and agreed that if we win, we will get up and go and get our awards with absolute non-chalance à la MS Dhoni (for context he is always calm, no matter the result). Our problem statement was supposed to be the last one to have the winners proclaimed. The guy who was announcing the winners said ‘P-Y’, and we all knew what the next word was gonna be, and before he could finish saying “society” we were ready to get on the stage to collect our award. Everyone was so full of emotion, but we tried our level best to not show a shred of it. We all got out of the hall, took some pictures to mark the occasion. One would expect people to celebrate after such a win, but we all looked at each other and almost telephatically agreed that we all needed sleep before the long train journey back home. Two and a half days later, we arrived back at Manipal around 8:15 in the morning. I rushed directly to the my lab from the station, as it was to start at 8:30 in the morning. I eventually reached about 5 minutes late, but the teacher was kind enough to pardon my shabby appearance (long train journeys tends to do that to you) and late entry. Tanmay, one of my best buddies in class was gesturing, intending to know what had happened at the hackathon. I simply smiled back at him and showed him the victory sign.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Thoughts" /><summary type="html">On some night in January 2019, me and my roommates Aadil &amp;amp; Sushant were discussing how the hectic schedule in our college, and all our other commitments for improving our profiles, virtually meant that we weren’t spending time with each other at all. We all agreed that the only way to actually hang out more was to work on the same project. We started looking out for options, and after carefully weighing in our alternatives, we decided on participating in the Smart India Hackathon. The decision was guided by the kind of projects on offer, and a time frame that would have allowed to us to maintain our other commitments, manage classes and assignments and still give about 20 hours per week to this project. A blatant flaw with this plan, that we realized much later was that this competition was one which a lot of people from all across the country looked forward to, with some even spending more than a year preparing for it. For us, the first round of the competition was about 20 days away, with the final round in about 2 months time. There was another catch, each team had to have a minimum of 6 members, and after a mix of deliberation &amp;amp; persuation over the next week, we managed to form a team, adding Daniel, Arshia and Damodar to our team, all of whom were our batchmates.</summary></entry><entry><title type="html">React Native, thou aren’t as complex as thou seem</title><link href="http://localhost:4000/technical/ReactNative.html" rel="alternate" type="text/html" title="React Native, thou aren’t as complex as thou seem" /><published>2018-12-14T00:00:00+05:30</published><updated>2018-12-14T00:00:00+05:30</updated><id>http://localhost:4000/technical/ReactNative</id><content type="html" xml:base="http://localhost:4000/technical/ReactNative.html">&lt;p&gt;During my intership at Optimize IT Systems, my job entailed creating a basic curriculum for React Native, since the company wanted to migrate towards developing mobile applications with it. This post contains snippets from that curriculum along with a mention of the potential pitfalls that one might fall into as a beginner.&lt;/p&gt;

&lt;h3 id=&quot;navigation-between-screens-in-react-native&quot;&gt;Navigation between Screens in React Native&lt;/h3&gt;
&lt;p&gt;Navigation in React Native is generally enabled through third party modules. Although native node modules to do the same can obviously developed, but the already existing ones work pretty well with practically no downsides. I recommend ‘react-navigation’, &lt;a href=&quot;https://medium.com/the-react-native-log/thousand-ways-to-navigate-in-react-native-f7a1e311a0e8&quot;&gt;This&lt;/a&gt; contains a comparative study with regards to the options available.&lt;/p&gt;

&lt;p&gt;Installing React Navigation can be done in two ways -&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yarn add react-navigation 
npm install react-navigation
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running any one of the two, will set up the project to use navigation properly. Changing a package manager to install different third party modules leads to complications in in terms of the consistency in the versions of the different modules installed. The .lock files also get hampered with and often the only way out is to rebuild the entire project, which can be done by running &lt;strong&gt;yarn install&lt;/strong&gt; again, after removing the node-modules subdirectory entirely ( or &lt;strong&gt;npm install&lt;/strong&gt; if that is being used ). Since the node-modules sub-directory can be so easily rebuild, it is generally not pushed while using version control for a project. 
Yarn on one hand, makes a lock file right at the very onset, and uses that to always install a certain version of the dependencies, while npm will always go for the latest dependencies, and that is where the conflict can occur, since some methods get deprecated, and things that work in one environmental setup, might not work at all in a slightly different environment&lt;/p&gt;

&lt;p&gt;Two most popular Navigatiors available with react-navigation are,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Stack Navigator , 2. Tab Navigator&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Stack Navigator works as expected like a stack, with pages on being called, being pushed on the stack, and going back results in popping, thus we end up with the previously opened page.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Sample Stack Navigator : 
	const AStack = StackNavigator(
	  {
	    A: {
	      screen: PageA
	    },
	    AA: {
	      screen: PageAA
	    }
	  },
	  {
	    initialRouteName: 'A',
	  }
	);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will enable an environment where in navigation would be possible, however all of these need to be done manually by adding functions or other methods to actually navigate to other page. A possible analogy is the fact that making the navigator object is like knowing the route to a certain place, however we just knowing it, won’t take us there, we explicitly 
need to go there as well.&lt;/p&gt;

&lt;p&gt;Unlike the stack navigator which goes deeper into the application, TabNavigator gives us quick access to pages that need to toggled more frequently. Like in the facebook app, we can toggle easily between the notifications, messages and friend requests pages, whilst clicking on some page/post enables for us to go deeper.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Sample Tab Navigator, most of the code given is self explanatory... 
export default TabNavigator(
	  {
	    AS: {
	      screen: pageA
	    },
	    BS: {
	      screen: pageB
	    },
	    CS: {
	      screen: pageC 
	    }
	  },
	  {
	    navigationOptions: ({ navigation }) =&amp;gt; ({
	      tabBarIcon: ({ focused, tintColor }) =&amp;gt; {
	        const { routeName } = navigation.state; 
	        let iconName = icons[routeName];
	        let color = (focused) ? '#fff' : '#929292';
	        return &amp;lt;MaterialIcons name={iconName} size={35} color={color} /&amp;gt;;
	      },
	    }),
	    tabBarPosition: 'bottom',
	    animationEnabled: true,
	    tabBarOptions: {
	      showIcon: true,
	      showLabel: false,
	      style: { backgroundColor: '#333' }
	    }
	  },
	  {
	    initialRouteName: 'BS'
	  }
);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For passing of data/objects between 2 screens, we pass the objects using a key-value pair using the navigate function :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Button 
	onPress={() =&amp;gt; { 
		navigation.navigate(
			'pageC',				// the page to go to 
			{ 
				key1 : Data1,
				key2 : Data2
			}						// data to pass to the page 
		);
  	}} 
/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now these passed values need to be retrieved as well, which can be done by adding the following in the NavigationOptions provided, at the start of the main class of a page :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static navigationOptions = ({ navigation }) =&amp;gt; {
	const { params } = navigation.state; // extract params, now using it requires params.data_objname   
  	return {
        	headerTitle: 'Exercises',
        	headerStyle: {	backgroundColor: '#333' }, 
  	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Appropriately modules ( third party modules and classes referred ) have to be imported for Navigation to work properly. Also by playing around with styling and by modifying parameters, highly complex navigation techniques replete with animations can be made. A combination of the stack and tab navigators could be used to provide for the framework in case of a very complex workflow.&lt;/p&gt;

&lt;h3 id=&quot;working-with-databases&quot;&gt;Working with Databases&lt;/h3&gt;
&lt;p&gt;A local database is almost a bare-minimum for the making of even the simplest of apps, and like any other technologies, 
there’s a lot to choose from in terms of what one might want to use in their application, to accomplish local storage. 
&lt;a href=&quot;https://aboutreact.com/local-database-in-react-native-app/&quot;&gt;This&lt;/a&gt; offers a great overall outlook for the options available out there to implement local database in React-Native apps.&lt;/p&gt;

&lt;h4 id=&quot;sqlite-&quot;&gt;Sqlite :&lt;/h4&gt;
&lt;p&gt;It uses a third party module called : ‘react-native-sqlite-storage’, thus same needs to be added by yarn&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yarn add react-native-sqlite-storage 
yarn add react-navigation 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, this isn’t enough, since we need to provide some extra information in the android-specific folder, for these databases to work properly.&lt;/p&gt;

&lt;p&gt;Step 1 : Add to the dependencies section in android/app/build.gradle&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;implementation project(':react-native-sqlite-storage')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step 2 : Add to the android/settings.gradle&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include ':react-native-sqlite-storage'
project(':react-native-sqlite-storage').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-sqlite-storage/src/android'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step 3 : Add to MainApplication.java&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import org.pgsqlite.SQLitePluginPackage // import statement 
new SQLitePluginPackage() // append to getPackages
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After following the above steps, the application is now ready to use sqlite-database. The basic syntax of an sqlite execute function is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;db.transaction(function(txn) {
  		txn.executeSql(
	    query,                 //Query same as in Sqlite
		argsToBePassed[],      //Argument to the query 
		function(tx, res) {}   //ToDo with the result
  		);
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However before it is used, importing &lt;strong&gt;openDatabase&lt;/strong&gt; is necessary, and opening the database is necessary as well. 
Here though the database being accessed does not necessarily have to be created before hand, if nothing is present,
a new albeit empty database is created. We can even manipulate existing databases and is precisely the reason why we 
would even consider using SQLite plugins, since although it is used across domains in myriad applications, the plugin provided
in React Native is slower compared to other alternative, for eg : Realm.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import {openDatabase} from 'react-native-sqlite-storage'
var db = openDatabase({name : 'name-of-database.db'})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thus for illustration a sample operation of all the operations are being shown below :&lt;/p&gt;

&lt;h5 id=&quot;query-a-table&quot;&gt;Query a table&lt;/h5&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;db.transaction(tx =&amp;gt; {
      		tx.executeSql(
        		'SELECT * FROM table_user where user_id = ?', [this.search_user_id],
        		(tx, results) =&amp;gt; {
        			// DO SOMETHING WITH THE RESULTS, DISPLAY OR SET VALUES 
        		}
        	)
		}
	);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;insert-into-a-table&quot;&gt;Insert into a table&lt;/h5&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;db.transaction(tx =&amp;gt; {
      		tx.executeSql(
        		'INSERT INTO table_user (user_name, user_id) VALUES (?,?)', [this.state.name,this.state.id],
        		(tx, results) =&amp;gt; {
        			// After execution of the insert statement, this function gets executed 
        		}
        	)
		}
	);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;create-a-table&quot;&gt;Create a table&lt;/h5&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;db.transaction(tx =&amp;gt; {
      		tx.executeSql( 
      			'CREATE TABLE IF NOT EXISTS table_user(user_id INTEGER PRIMARY KEY AUTOINCREMENT, user_name VARCHAR(20)' 
      		)
		}
	);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;update-values-in-a-table&quot;&gt;Update values in a table&lt;/h5&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;db.transaction(tx =&amp;gt; {
      		tx.executeSql(
        		'UPDATE table_user set user_id = ?. user_name = ?', [this.state.id, this.state.name],
        		(tx, results) =&amp;gt; {
        			// After execution of the update statement, this function gets executed 
        		}
        	)
		}
	);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;delete&quot;&gt;Delete&lt;/h5&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;db.transaction(tx =&amp;gt; {
      		tx.executeSql(
        		'DELETE FROM table_user where user_id = ?', [this.search_user_id],
        		(tx, results) =&amp;gt; {
        			// After delete, this function gets executed 
        		}
        	)
		}
	);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A very common error is to not handle the data types appropriately while trying to do any CRUD operation on the database, 
especially if the state variables are being used to change/update/insert values in the database. For eg: use parseInt(str,10) 
if the required type is Integer for a particular column in database. A good reference for an IOS-app demo for SQLite is available in this blog &lt;a href=&quot;https://brucelefebvre.com/blog/2018/11/06/react-native-offline-first-db-with-sqlite/&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;realm-&quot;&gt;Realm :&lt;/h4&gt;

&lt;p&gt;Realm is not a database in a very traditional sense, since it cannot be used universally. It has been designed specifically 
for mobile devices, although it makes up for this disadvantage with substantially faster speed, and it’s simplicity of design.
At the very heart of it, it uses JS objects which are dynamically mapped to a full, custom database engine. Extremely complex 
data models can be modelled using Realm, and is thus is a very popular local database choice for mobile applications.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yarn add realm 
react-native link realm 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similar to sqlite, a few additional steps are required to finish the setup&lt;/p&gt;

&lt;p&gt;1) In the &lt;strong&gt;settings.gradle&lt;/strong&gt; file :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include ':realm'
project(':realm').projectDir = new File(rootProject.projectDir, '../node_modules/realm/android')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) 	In the &lt;strong&gt;android/app/build.gradle&lt;/strong&gt;, if gradle version &amp;lt; 3.0, then replace implementation with compile :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;implementation project(':realm')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3) In &lt;strong&gt;MainApplication.java&lt;/strong&gt; :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import io.realm.react.RealmReactPackage ; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And inside the getPackage() function after MainReactPackage(),&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;new RealmReactPackage() ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;making-a-schema&quot;&gt;Making a Schema&lt;/h5&gt;
&lt;p&gt;Making a schema is equivalent to designing the table structure in SQL. However, the functionality provided by Realm, enables making even extremely complex schemas, very easy to design, since overall design is very similar to the object-oriented design.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cont Xschema = {
	name : 'X',
	primaryKey : 'id',
	properties : {
		name : 'string',
		id : 'integer',
		birthday : 'date',
		achievements : 'achievement[]',
		death : 'date?',
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, &lt;em&gt;name : ‘X’&lt;/em&gt; isn’t the property of the network, it is instead serving the purpose of a alias. Thus, every Xschema object will have the same name-alias, for referring. However, the &lt;em&gt;name&lt;/em&gt; inside the &lt;em&gt;properties&lt;/em&gt; block is a property of each object. There are options galore for datatypes &lt;a href=&quot;https://realm.io/docs/javascript/latest/&quot;&gt;docs&lt;/a&gt;. #e can use our own custom datatypes, i.e. we could use embedded schemas in our design, to model extremely difficult situations extremely easily.&lt;/p&gt;

&lt;p&gt;Appending a &lt;strong&gt;[]&lt;/strong&gt; at the end of some datatype implies that the entity is a list containing those datatypes variables, similarly  a &lt;strong&gt;?&lt;/strong&gt; implies that using the parameter is optional. Again, there is a lot more options to make better schemas, and a look into the &lt;a href=&quot;https://realm.io/docs/javascript/latest/&quot;&gt;docs&lt;/a&gt; might prove worthwhile.&lt;/p&gt;

&lt;h5 id=&quot;insert-a-schema&quot;&gt;Insert a Schema&lt;/h5&gt;
&lt;p&gt;The following code block can be used for inserting an entry in some table, in realm referred to as making a object of a schema&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;realm.write( ()=&amp;gt;  {
	realm.create('X',{
		id : parseInt(this.state.id, 10) ,
		...
		..
		.
	}) ; 
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thus, after opening the ‘X’ type schema, we create an object of that instance, the reference for which may or may not be saved, as per needs. It must be noted that the realm object needs to be passed, across the screens for it to be accessible everywhere, or the path to the same needs to be passed as props.&lt;/p&gt;

&lt;h5 id=&quot;query-a-schema&quot;&gt;Query a Schema&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Simplest query , return all the objects of a particular schema.&lt;/p&gt;

    &lt;p&gt;a = realm.objects(‘X’)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Equivalent to a where clause, using filter.&lt;/p&gt;

    &lt;p&gt;a.filtered(‘condition1 AND condition2’) ;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;where a is the collection of all objects of that particular schema, as said before&lt;/p&gt;

&lt;h5 id=&quot;delete-a-schema&quot;&gt;Delete a Schema&lt;/h5&gt;
&lt;p&gt;For deleting entities, we will need object references of everything we need to delete, thus we use the query function first to get those references, then we call the delete operation. An example -&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;realm.write(() =&amp;gt; {
    a = realm.objects('employee').filtered('id =' + parseInt(this.state.id,10))
	if ( a.length &amp;gt; 0 ) {
		realm.delete(a) ; 
	}
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;update-a-schema&quot;&gt;Update a Schema&lt;/h5&gt;
&lt;p&gt;Update is very similar to delete, we simply get the applications, and then we simply update using the object references, just as assignment to any other object is done. An example :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;realm.write(() =&amp;gt; {
    obj = realm.objects('employee').filtered('id =' + parseInt(this.state.id,10)) ; 
    for (i = 0 ; i &amp;lt; obj.length ; i++ ) {
        obj[i].param1 = some_variable ; 
        ...
        ..
        .
    }	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;shared-preferences&quot;&gt;Shared Preferences&lt;/h3&gt;
&lt;p&gt;Shared Preferences in Mobile Application Development refers to the data that is stored in the app itself, for customizing the overall experience of using the application for a user. These are data that persists, even if the the app is closed or the device is stopped. Also these data values can be accessed across the application, as in every page in the app will have access to this data.&lt;/p&gt;

&lt;p&gt;One of the simplest ways to implement this feature is to use the AsyncStorage functionality provided by react-native as a basic component. Thus since it is not a third party module, no extra setup is required for using this. A disadvantage of Async Storage is that, although the data stored is persistent, it stores everything in an unencrypted form. However, in most use cases, the data stored via Shared preferences isn’t all that sensitive and so is widely used.&lt;/p&gt;

&lt;p&gt;A few important things to note about Async Storage is the fact that it stores data in key-value pairs [ like a dictionary in Python ], and it can only store value in string format. Thus for any other datatype, the responsibility of conversion handling is passed on to the app itself.&lt;/p&gt;

&lt;p&gt;All methods in Async Storage return a Promise object ( an object that either has a value or may produce a value in future ), thus, it follows that almost all the times any method of async storage is used, we end up using it within Async-Await blocks. In loose terms, it is a way of handling promise objects in a better fashion, async if prefixed before a function implies that the function will always return a promise, whilst await makes the application wait for the promise result to get resolved. 
For detailed information on Promise objects &lt;a href=&quot;https://medium.com/javascript-scene/master-the-javascript-interview-what-is-a-promise-27fc71e77261&quot;&gt;link&lt;/a&gt; and for Async-Await concepts &lt;a href=&quot;https://hackernoon.com/6-reasons-why-javascripts-async-await-blows-promises-away-tutorial-c7ec10518dd9&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After understanding Promise objects, and Await-Async calls, it becomes pretty straight-forward to use Async-Storage, however to avoid the application from crashing, exception handling is critical, and needs to cover cases in which promise returned is NULL.&lt;/p&gt;

&lt;p&gt;Some of the operations are shown here below&lt;/p&gt;

&lt;p&gt;Get Data  -&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;try {
  	AsyncStorage.getItem(
    	'Key').then(data =&amp;gt; {
      	this.setState({another : data}) // code after **then** dictates what is to be done with the data retrieved
    	}) ;
 	} catch (error) {
   		console.log('retrieve not happening') 
 	}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Insert/Update Data -&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;try {
  AsyncStorage.setItem('Key', JSON.stringify(this.state.textValue)); // set state variable to be equal to key's value
} catch (error) {
  console.log('Couldn\'t save') ;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Delete Data -&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;try {
  AsyncStorage.removeItem('Key', (err) =&amp;gt; {
       console.log('Deleted...');		// code after comma, dictates action after deleting the value associated with key
  });		
}
catch(exception) {
  console.log('Delete not happening') ; 
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;card-view&quot;&gt;Card View&lt;/h3&gt;

&lt;p&gt;CardView, a very popular Material Design resembles a frame, and has a very elegant styling already coded up for itself. It is a very potent tool to use, when information of the same kind needs to be displayed somewhere in the application. 
is the &lt;a href=&quot;https://developer.android.com/reference/android/support/v7/widget/CardView&quot;&gt;docs&lt;/a&gt; detail the the philosophy behind this design.&lt;/p&gt;

&lt;p&gt;Add the third party modules using your default package manager,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yarn add react-native-elements 
yarn add react-native-vector-icons
react-native link react-native-vector-icons 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note : Not adding react-native-vector-icons will result in the code not working, since Card Component requires that module for the initial auto-generated code added during installation to work.&lt;/p&gt;

&lt;p&gt;Thereafter using a CardView is exactly similar to how a normal View would be used,&lt;/p&gt;

&lt;p&gt;Import it,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import {Card} from 'react-native-elements'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Wrap some content in it,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Card&amp;gt;
	&amp;lt;Text&amp;gt; some text &amp;lt;/Text&amp;gt;
	...
	..
	.
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And whatever is wrapped gets displayed, we don’t need to worry about the size of the card, it adjusts according to the content it has, although we need to careful of the fact that the screen size might not accomodate all the cards. Thus we wrap the overall Content within ScrollView, so that we can view everything there,&lt;/p&gt;

&lt;h5 id=&quot;common-errors&quot;&gt;Common errors&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SrollView Child layout must be applied through …. error type : This happens because in ScrollView, we can’t have styling 
done through the normal &lt;em&gt;style&lt;/em&gt; option, instead we have to use :&lt;/p&gt;

    &lt;p&gt;contentContainerStyle = {styles.someStyle}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ScrollView, does not crash, but it does not scroll either. In this case, generally adding one option to ScrollView styles 
component ( here someStyle in the styles Stylesheet ) makes the code work,&lt;/p&gt;

    &lt;p&gt;flexGrow : 1&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;custom-camera&quot;&gt;Custom Camera&lt;/h3&gt;

&lt;p&gt;Since, the &lt;strong&gt;Camera&lt;/strong&gt; component used here is an in-built component, provided by the expo class, using it doesn’t require any kind of external library installation. However, for using the Camera in Android , the app should have the necessary mandation to use the device’s Camera. For the same, we use the &lt;strong&gt;Permission&lt;/strong&gt; component provided by expo. The Camera component simply gives us the view of the camera itself, it does not involve using intents to use the default Camera App in a given android device. Thus every functionality of the camera, from mechanisms to click pictures, to deleting pictures, to flipping the camera needs to be done explicity through code. Again, the photos captured do not get saved themselves, they need to be save explicitly by connecting the &lt;strong&gt;FileSystem&lt;/strong&gt; component. using any database mechanism. Here we use ‘react-native-simple-store’ which is a 3rd party module (thus needs to be added manually using package managers) , a wrapper for Async-Storage,&lt;/p&gt;

&lt;p&gt;Thus import statement includes&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import {Permissions, Camera , FileSystem} from 'expo' ; 
import 'react-native-simple-store'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For looking at the camera contents, we simply use &amp;lt;Camera&amp;gt; &amp;lt;/Camera&amp;gt; as we would use &amp;lt;Text&amp;gt; or &amp;lt;Button&amp;gt;, we style it appropriately, however the only difference is we use Modal View to embed the Camera view, and the advantage of using a modal view is the fact that we can actually set as to whether the Modal view is visible at some point, or whether it isn’t.
It becomes really useful with Cameras, since we simulate opening and closing of the camera by manipulating the visibility of the container modal view.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Modal
	animationType=&quot;slide&quot;
	transparent={false}		
	visible={this.state.is_camera_visible}		// control visibility by a state variable...
	onRequestClose={() =&amp;gt; { this.setState({ is_camera_visible: false }); 
}}&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now as a sub element, to this Modal, we could define views, which will have the Camera component, along with a few buttons, meant to serve different functionalities of the camera. The styling of the buttons, camera screen component can be modified to one’s suiting and serve the customization. A sample is given below&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Camera style={/* style */} type={this.state.type} ref={ref =&amp;gt; { this.camera = ref; }}&amp;gt;
	&amp;lt;View style={/* style for Body */}&amp;gt;
		&amp;lt;View style={/* style for Button */}&amp;gt;
			&amp;lt;Button {/* close camera */} /&amp;gt;
			&amp;lt;Button {/* flip- camera */}/&amp;gt;
			&amp;lt;Button {/* capture-photo */}/&amp;gt;
		&amp;lt;/View&amp;gt;
	&amp;lt;/View&amp;gt;
&amp;lt;/Camera&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;here, one important point that needs to be emphasized upon is the setting of the &lt;strong&gt;this.camera&lt;/strong&gt; variable, we connect the Camera View component inside Modal, to the actual Camera using this, very reference. Thus playing around with these, we can even simultaneously use both the front and back cameras, also the &lt;strong&gt;type&lt;/strong&gt; here signifies, whether the front or the rear camera is being used, therefore a state variable can be used to keep track of the current type, and changing to a different one, would involve, simply changing the state variable’s content.&lt;/p&gt;

&lt;p&gt;However, before using any of these functionalities, as said before permissions need to be sorted for Android Devices. A way of securing permissions is shown below :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 	componentWillMount() {
    	Permissions.askAsync(Permissions.CAMERA).then((response) =&amp;gt; {
      		this.setState({ has_camera_permission: response.status === 'granted' }); 
    	});
  	}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;componentWillMount code, executes before any page has been rendered, thus including this, would invoke an alert, asking for permissions to use the Camera. Here a state variable, has_camera_permission is used to store whether or not permission has been availed, so the button to launch the camera can use this status as a condition, to open up camera, only if 
status == granted.&lt;/p&gt;

&lt;p&gt;The code to take a picture is given below :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if(this.camera){ // check whether there's a camera reference
	this.camera.takePictureAsync().then((data) =&amp;gt; {
		// code for processing the picture captured, stored in data... 
	});
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, we have to store the pictures that have been captured, using FileSystem another component for which expo provides support. Importing the component is necessary as indicated before, we use a variable to store the location of the directory in which  all the photos captured will get stored. Here we use &lt;em&gt;react-native-simple-store&lt;/em&gt; in conjunction with &lt;em&gt;FileSystem&lt;/em&gt; component to store the pictures in the app itself.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;this.location = FileSystem.documentDirectory ; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here &lt;em&gt;location&lt;/em&gt; will now have a unique location at which it is being stored, however it won’t be accessible by the phone itself, since by default the location is in the Android OS. For changing this to save things to your Gallery, or to change it up to have things saved up on some other default location &lt;a href=&quot;https://facebook.github.io/react-native/docs/cameraroll.html#savetocameraroll&quot;&gt;Camera Roll API&lt;/a&gt; can be used.&lt;/p&gt;

&lt;p&gt;Thus inside the &lt;em&gt;then&lt;/em&gt; construct for the code to capture pictures, we put in mechanisms to store the photo. The photo is accessible via the &lt;em&gt;data&lt;/em&gt; variable. For every photo we get a unique &lt;em&gt;file-path&lt;/em&gt;, then we add the following :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FileSystem.moveAsync({
	from: data.uri,
	to: file_path
}).then( (response) =&amp;gt; {
	let photo_data = {
		key: // some key 
        name: // some name 
	};
	store.push('database_name', photo_data);
});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function simply moves the photo from the obscure location it is at, to a location somewhere either local or external (in that case, we have to take permission to access device content’s as well) in the app. &lt;em&gt;photo_data&lt;/em&gt; here is used as a temporary variable to eventually store the photo in a database (Async Storage) for the app. So at a later point in time, we can view the photo by retrieving it using the key that was used to store it in the first place. A lot more can be done, in terms of enhancing functionalities of the camera, styling the app, some other 3rd party modules can be used that aren’t supported by expo, and thus projects for those can be made using ‘react-native init’. However, using expo components always holds the advantage of not having to install external packages, also there not being any effort whatsover to make changes specific to any particular platform ( neither Android nor IOS ).&lt;/p&gt;

&lt;h3 id=&quot;integrating-maps&quot;&gt;Integrating Maps&lt;/h3&gt;

&lt;p&gt;Before integrating Maps add the third party modules&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	yarn add react-native-elements 
	yarn add react-native-vector-icons
	react-native link react-native-vector-icons 
	yarn add react-native-maps
	react-native link react-native-maps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, for using any Map Service ( eg. Google Maps, OpenLayers, TomTom ) we need to generate API keys to render the Map View. Thus after generating the appropriate keys, we add it to our Android.manifest file as follows &lt;em&gt;demo for google API given&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta-data&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;android:name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;com.google.android.geo.API_KEY&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;android:value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;---- Insert your own key ----&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A common error that occurs on running the application at this very instance is as follows, &lt;strong&gt;android.support.v4.accessibilityservice.AccessibilityServiceInfoCompat Error&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A solution for this is to replace the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	compile/implementation project('react-native-maps')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with the following code segments,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	compile(project(':react-native-maps')) {
		exclude group: 'com.android.support'
		exclude module: 'appcompat-v7'
		exclude module: 'support-v4'
	}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, hereupon we can render the MapView anywhere in the application. It is generally better to render the map with absolute coordinates, an example of a scheme for full screen map shown here :-&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	map: {position:'absolute', top:0, left:0, right:0,bottom:0}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Before rendering the Map, import all required components&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	import MapView, {Marker} from react-native-elements
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;MapView has a &lt;em&gt;Region&lt;/em&gt; variable, which has in 4 parameters viz, latitude, longititude, latitudeDelta , longitudeDelta to specify what area the MapView should be showing. Initial Region for the Map should always be shown, a sample setup is shown here,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;MapView
	ref = &quot;Mapfunction&quot;
	style={styles.map}
	initialRegion={
		{
			latitude: 37.78825,
			longitude: -122.4324,
			latitudeDelta: 0.00922,
			longitudeDelta: 0.00421
		}
	}
	&amp;lt;Marker
		ref = &quot;MapMove&quot;
		coordinate={
			{
				latitude : 37.78825,
				longitude : -122.4324
			}
		}
	/&amp;gt;		
&amp;lt;/MapView&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;ref&lt;/strong&gt; is used to call the methods native to a particular component. Thus, for example for a button focus, blur methods are provided. So using ref, we create a object like entity which we used to call that method. So here, using any method for the MapView would involve the following, &lt;strong&gt;this.refs.Mapfunction.methodToCall&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The code above renders the Map to be set to the region specified, with the center of the map being marked by the Marker component and the expanse of the map being the variables latitudeDelta and longitudeDelta.&lt;/p&gt;

&lt;p&gt;Say, if we get the coordinates of a new location through some other means in our application, then we can change our MapView to refer to those coordinates and our marker to point to that very location in the following way,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;this.refs.Mapfunction.animateToRegion( {
		latitude : Number(this.state.latitude),
		longitude : Number(this.state.longitude),
		latitudeDelta : 0.00922,
		longitudeDelta: 0.00421,
	},
    100 
); 
this.refs.MapMove.animateMarkerToCoordinate( {
		latitude : Number(this.state.latitude),
		longitude : Number(this.state.longitude),      
	},
	100
) ; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The ref variables are in line with what we had set while rendering the components, also we assume the new latitude and longitude values are stored in state component of the application.&lt;/p&gt;

&lt;h4 id=&quot;geocoding&quot;&gt;Geocoding&lt;/h4&gt;
&lt;p&gt;Geocoding is the process of converting human readable addresses to latitudes and longitudes, thus while searching for some location, Geocoding comes into place, since the user would be putting in the name of the addresses, and thus we need to generate the corresponding geocoded values, for any further application, example getting the route.&lt;/p&gt;

&lt;p&gt;Thus, for doing the same add one more package to our project,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yarn add react-native-geocoding
react-native link react-native-geocoding
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thereafter we add another API key to our application, one which is capable of doing geocoding. Thus amongst the Google API’s the &lt;em&gt;Places&lt;/em&gt; API suffices. Some other key could also be used.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import Geododer from 'react-native-geocoding'
Geocoder.init('---- Insert your API key here ----- ')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, a sample conversion of a place’s address to latitude and longitude is given here&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Geocoder.from(this.state.sampleAddress)
.then(json =&amp;gt; {
	var location = json.results[0].geometry.location;
	console.log(location.lat); 
	console.log(location.lng); 
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we get the 1st location result that we get, and retrieve that result’s latitude and longitude. If all the results are needed, we can use the json.results[] area to access all the results. It should be noted that the scope of the location variable here is local, thus to access these values anywhere else, we must store these to some variables, and for 
doing the same, differing datatypes must be handled with extreme care.&lt;/p&gt;

&lt;p&gt;The reverse of this process called &lt;em&gt;reverse-geocoding&lt;/em&gt; can also be used using the same third party module.&lt;/p&gt;

&lt;p&gt;Another thing worth exploring is the SearchBar component provided by ‘react-native-elements’. Generally used in conjunction with React Native Maps, it is a very efficient way to display the results of a search and to choose amongst the options.&lt;/p&gt;

&lt;p&gt;A sample code snippet, which is self explanatory is provided here,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;SearchBar 
	data = {some array component}
	containerStyle = {some stylesheet component}
	onChangeText= {some method }
	onSubmitEditing = {some method}
	placeholder=&quot;Type here ....&quot;
/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here the data component is the array where the search will be performed, and from which the results will be finally refined. Other components are self explanatory. For discovering other functions associated with the SearchBar component, &lt;a href=&quot;https://react-native-training.github.io/react-native-elements/docs/searchbar.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;_Note : Haven’t added details on setting up a React Native Environment, since there are a lot of references available for that. I recommend going through this &lt;a href=&quot;https://github.com/jondot/awesome-react-native&quot;&gt;repository&lt;/a&gt; if you wish to explore React Native in more depth&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Technical" /><summary type="html">During my intership at Optimize IT Systems, my job entailed creating a basic curriculum for React Native, since the company wanted to migrate towards developing mobile applications with it. This post contains snippets from that curriculum along with a mention of the potential pitfalls that one might fall into as a beginner.</summary></entry><entry><title type="html">A guess about where we are headed</title><link href="http://localhost:4000/thoughts/journey-ahead.html" rel="alternate" type="text/html" title="A guess about where we are headed" /><published>2018-12-06T00:00:00+05:30</published><updated>2018-12-06T00:00:00+05:30</updated><id>http://localhost:4000/thoughts/journey-ahead</id><content type="html" xml:base="http://localhost:4000/thoughts/journey-ahead.html">&lt;p&gt;There hasn’t been such an outlandish spur of developments and advancements as in the past decade since perhaps the 17th Century when the industrial revolution happened, all so without any new major discovery and sans any new scientific breakthrough. The contribution of theoritical Physics and Chemistry, have become  extremely miniscule, in the last quarter of a century. All major advancements in the purest of fields have merely been shoddy proofs of abstract concepts including teleportation, tapping into alternate dimensions (cue - string theory) most of which when generally talked about with the general consensus never fail to invoke smirks at the apparent obscurity, far-fetchedness, and the dogmatic nature of the work being done. Compare that to the technological advancements that we have made, and you have a completly contrasting picture. The progress and extension of ideas has been staggering, with more and more growth in every aspect of living possible, leading to a time, where it has become practically impossible to point out even one activity, or field that hasn’t been 
touched by this phenomenon that we are witness to, and this pace is only increasing. Interestingly, there is something called the “Law of accelerating returns” which in its essence implies that the rate of technological change, always has and always will keep increasing exponentially. Studies show that in the coming decade, we are probably going to achieve a 100 times more than what we did in the last decade, and to people who like stability, and love to reminisce about the good old days, this is 
not a very pleasant pill to swallow.&lt;/p&gt;

&lt;p&gt;So the question that remains to be asked is that can times ever be simple again, and I guess the answer to that is as rhetoric as it gets. Luxury is one of the most powerful drugs, and perhaps demands the most difficult rehab ever. In a short term, perhaps people might even adapt to varying circumstances, and give up a few things that they had gotten used to, but in the long term, mother earth has other plans for us, with the damned biological evolution making it virtually impossible for people to go back to what they were. Today, we might be able to withstand extreme heat and cold conditions, but with being continuosly used to temperature control all around us, our limits will continually decrease, likewise it might even get psychologically impossible for us after a point in time to live without the internet. Some lost human abilities include weaker digestive systems, hearing, and sense of smell, an almost non-existent peripheral vision, not to mention massive decline in our survival instincts. And that is what troubles people in the tech field, not the fact that they wouldn’t ever be needed in the future, with them reaching an ultimate exhaustion point of adding layers to a theoritical basis. Every 
device that is being made, every new service that is being provided, every task that is being automated, is in a way contributing to us not needing to do that work on our own ever again, and thus eventually resulting with us losing out on the ability to do it at all, and this serves as a massive dampener to the spirits of a community, whose only aim there ever has 
been, is to make life in this world easier for one and all.&lt;/p&gt;

&lt;p&gt;And the solution isn’t as banal as just stopping this marauding juggernaut all of a sudden, which in itself could lead to catastrophic circumstances. It would be the equivalent of deciding to stay afloat instead of moving forward at the sight of an imminent gushing wave. Standing down, not even giving a try would only push us backwards, a consequence that we had set out to avoid in the first place. So, what then could be the solution to this seemingly insurmountable conundrum; well in many ways, humanity is currently behaving like a teen, getting paranoid about every other decision they are making, feeling 
helpless over the fact that they have no control whatsoever, tirelessly thinking about what the future holds for them for hours on end. But the thing with being a teen is that we eventually grow up, mature and get our heads set straight and maybe, just maybe if humanity starts claiming more responsibility, starts growing up a little, we might just weather the storm, get through this rought night into a much more pleasing and beautiful dawn.&lt;/p&gt;</content><author><name>Yash Sarrof</name></author><category term="Thoughts" /><summary type="html">There hasn’t been such an outlandish spur of developments and advancements as in the past decade since perhaps the 17th Century when the industrial revolution happened, all so without any new major discovery and sans any new scientific breakthrough. The contribution of theoritical Physics and Chemistry, have become extremely miniscule, in the last quarter of a century. All major advancements in the purest of fields have merely been shoddy proofs of abstract concepts including teleportation, tapping into alternate dimensions (cue - string theory) most of which when generally talked about with the general consensus never fail to invoke smirks at the apparent obscurity, far-fetchedness, and the dogmatic nature of the work being done. Compare that to the technological advancements that we have made, and you have a completly contrasting picture. The progress and extension of ideas has been staggering, with more and more growth in every aspect of living possible, leading to a time, where it has become practically impossible to point out even one activity, or field that hasn’t been touched by this phenomenon that we are witness to, and this pace is only increasing. Interestingly, there is something called the “Law of accelerating returns” which in its essence implies that the rate of technological change, always has and always will keep increasing exponentially. Studies show that in the coming decade, we are probably going to achieve a 100 times more than what we did in the last decade, and to people who like stability, and love to reminisce about the good old days, this is not a very pleasant pill to swallow.</summary></entry></feed>