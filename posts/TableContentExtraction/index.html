<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Getting Started with Table Extraction in Document AI" />
<meta property="og:locale" content="en" />
<meta name="description" content="Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field." />
<meta property="og:description" content="Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field." />
<link rel="canonical" href="http://localhost:4000/posts/TableContentExtraction/" />
<meta property="og:url" content="http://localhost:4000/posts/TableContentExtraction/" />
<meta property="og:site_name" content="Yash Sarrof" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-01T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Getting Started with Table Extraction in Document AI" />
<meta name="twitter:site" content="@yashYRS" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-08-30T12:43:04+02:00","datePublished":"2021-09-01T00:00:00+02:00","description":"Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field.","headline":"Getting Started with Table Extraction in Document AI","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/TableContentExtraction/"},"url":"http://localhost:4000/posts/TableContentExtraction/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Getting Started with Table Extraction in Document AI | Yash Sarrof
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Yash Sarrof">
<meta name="application-name" content="Yash Sarrof">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.27.20/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      let self = this;this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          self.clearMode();
        }
        self.notify();
      });

      if (!this.hasMode) {
        return;
      }

      if (this.isDarkMode) {
        this.setDark();
      } else {
        this.setLight();
      }
    }

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isPreferDark() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }get modeStatus() {
      if (this.hasMode) {
        return this.mode;
      } else {
        return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        this.clearMode();
      } else {
        if (this.isPreferDark) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    }
  }

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/images/op2.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <h1 class="site-title">
      <a href="/">Yash Sarrof</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0">M.Sc. Student</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/resume/" class="nav-link">
            <i class="fa-fw fas fa-user-graduate"></i>
            

            <span>RESUME</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/publications/" class="nav-link">
            <i class="fa-fw far fa-book"></i>
            

            <span>PUBLICATIONS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/" class="nav-link">
            <i class="fa-fw fas fa-newspaper"></i>
            

            <span>BLOG</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/yashYRS"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/yashYRS"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['ysarrof','lst.uni-saarland.de'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="https://scholar.google.com/citations?user=flWrpAoAAAAJ&hl=en"
          aria-label="google-scholar"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-google-scholar"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            
              <span>
                <a href="/">Home</a>
              </span>
            

          
        
          
        
          
            
              <span>Getting Started with Table Extraction in Document AI</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h2 data-toc-skip>Getting Started with Table Extraction in Document AI</h2>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1630447200"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Sep  1, 2021
</time>

      </span>

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://twitter.com/yashYRS">Yash Sarrof</a>
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="2749 words"
>
  <em>15 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  <div class="content">
    <p>Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field.</p>

<figure class="image">
  <center>
  <a href="/images/table_extraction/hard1.png" class="popup img-link  shimmer"><img src="/images/table_extraction/hard1.png" alt="Tables not following a pattern in Multi column layouts" loading="lazy"></a>
  <figcaption>Tables not following a pattern in Multi column layouts</figcaption>
  </center>
</figure>
<figure class="image">
  <center>
  <a href="/images/table_extraction/hard2.png" class="popup img-link  shimmer"><img src="/images/table_extraction/hard2.png" alt="Variations in appearance of tables" loading="lazy"></a>
  <figcaption>Variations in appearance of tables</figcaption>
  </center>
</figure>

<h3 id="popular-datasets"><span class="me-2">Popular Datasets</span><a href="#popular-datasets" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Some popular datasets that have been curated over the last few years in order to facilitate testing and training of Document AI tasks and will be used in this post are briefly described here. The disparity in the number of samples across datasets depends on whether the entire dataset was manually curated or generated automatically in a semi-supervised fashion.</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th>Dataset Name</th>
      <th>Number of Samples</th>
      <th>Document categories covered</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://ir.nist.gov/cdip/">IIT-CDIP</a> (Illinois Institute of Technology Complex Document Information Processing Test Collection)</td>
      <td>Made from over 6 Million Scanned documents</td>
      <td>Spans all kinds of documents</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://www.cs.cmu.edu/~aharley/rvl-cdip/">RVL-CDIP</a> (Ryerson Vision Lab Complex Document Information Processing)</td>
      <td>400K grayscale images (320K training, 40K validation, and 40K test images)</td>
      <td>Subset of IIT-CDIP</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/pdf/2103.10213.pdf">SROIE</a> (Scanned Receipt OCR and Information Extraction)</td>
      <td>1000</td>
      <td>Scanned Receipts</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://guillaumejaume.github.io/FUNSD/">FUNSD</a> (Form Understanding in Noisy Scanned Documents)</td>
      <td>199 fully annotated forms, 31485 words, 9707 semantic entities, 5304 relations Samples</td>
      <td>Exclusively contains forms in scanned documents</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge">CORD</a> (COVID-19 Open Research Dataset)</td>
      <td>Curated from 500K scholarly articles</td>
      <td>Articles about COVID-19, SARS-CoV-2, and related coronaviruses</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://github.com/applicaai/kleister-nda">Kleister NDA</a></td>
      <td>540 NDAs, 3299 unique pages</td>
      <td>Scanned and born-digital long formal Non Disclosure Agreements</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://paperswithcode.com/dataset/docvqa">Doc VQA</a></td>
      <td>12767 Document Images</td>
      <td>Industrial documents including typewritten, printed, handwritten and born-digital text</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://github.com/ibm-aur-nlp/PubLayNet">PubLayNet</a></td>
      <td>made from over 1 million PDF articles</td>
      <td>Scientific articles and reports in the medical domain</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://www.primaresearch.org/dataset/">PRImA</a> (Pattern Recognition and Image Analysis) Layout Anaylsis Dataset</td>
      <td>305 ground-truthed images</td>
      <td>Magazines and techinical articles spanning multiple domains</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://github.com/doc-analysis/TableBank">Table Bank</a></td>
      <td>417,234 high quality labeled tables</td>
      <td>Tables extracted from LaTeX and Word documents</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><a href="https://dell-research-harvard.github.io/HJDataset/">HJ</a> Historical Japanese Dataset</td>
      <td>Over 250,000 layout element annotations of seven types</td>
      <td>Complex layouts from all kinds of historical documents</td>
    </tr>
  </tbody>
</table></div>

<h3 id="initial-approaches"><span class="me-2">Initial Approaches</span><a href="#initial-approaches" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ul>
  <li>
    <p>The first efforts to compartmentalize a report into tabular and non tabular areas were made in 2016, where existing object detection architectures (CNNs, R - CNNs and their variants) were trained to detect tables in pdfs.</p>
  </li>
  <li>
    <p>In 2018, an end to end framwework was recommended in order to extract semantic structures of tables and paragraphs from documents. Pretrained word embeddings were in a fully conventional network to get decent results.</p>
  </li>
  <li>
    <p>In 2019, a Graph Convolutional Network was proposed, which combined visual and textual cues from a document for achieving the same result of identifying semantic structures.</p>
  </li>
</ul>

<h3 id="layout-lm"><span class="me-2">Layout LM</span><a href="#layout-lm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Almost every iteration brought with itself promising results, and had scope of application in real world datasets, 
However 2 aspects which the inital approaches hadn’t tried, and was first explored by LayoutLM (<a href="https://dl.acm.org/doi/10.1145/3394486.3403172">Xu et al, 2020</a>) were</p>
<ul>
  <li>Avoiding reliance on labelled data, since the number of such publicly known datasets were limited</li>
  <li>Trying joint pretrained models taking both textual and layout information into account. Up until now, the pretrained models were either CV or NLP models.</li>
</ul>

<p>Proposed by a team at Microsoft Research Asia, LayoutLM constituted a novel approach to simply pretraining for document AI tasks, and recommended fine tuning for subsequent tasks as per need. LayoutLM extends the core idea used in BERT to gain better performance in document AI tasks.</p>

<h4 id="modifications-made-to-the-bert-pretraining-stage"><span class="me-2">Modifications made to the BERT pretraining stage:</span><a href="#modifications-made-to-the-bert-pretraining-stage" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>Along with the word embeddings, embeddings to leverage the visual layout information present in the documents were added.
    <ul>
      <li>2d Position Embedding: The top left corner of the document page is considered to be the origin, and the bounding box for the word in the resulting coordinate system is stored as (x0, y0, x1, y1), with x0, y0 representing the upper left, and (x1, y1) representing the bottom right corner respectively. This helps model sparsity of the current word in the page, vicinity to other words, size of the current word relative to others and other spatial features, that would otherwise not be stored in case of traditional word embeddings.</li>
      <li>Image Embedding:
        <ul>
          <li>Word Image Embedding: These image embeddings are generated by a faster R-CNN, which is fed the document image and the Region of Interest is specified by an OCR system that gives the bounding box results of the word in question.</li>
          <li>[CLS] token Image Embedding: Using the same faster R-CNN, embedding of the entire document image is generated the Region of Interest set as the entire page. This is generated to assist in the downstream fine tuning tasks.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>The pretraining task of Masked Language Modelling(MLM) changed to Masked Visual Language Model(MVLM). Just as 15% of the text tokens were masked in the original task. Here, word embedding is masked but the 2d Positional embeddings are retained, thereby utilizing language contexts in conjunction with spatial information to predict the masked token.</p>
  </li>
  <li>The pretraining task of Next Sentence Prediction (NSP) changed to Multi Label Document Classification (MDC). This change was made to encapsulate the knowledge from different document domains and thus generate better document level representations.</li>
</ul>

<h4 id="layoutlm-fine-tuning"><span class="me-2">LayoutLM Fine tuning:</span><a href="#layoutlm-fine-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The fine tuning stage in LayoutLM is virtually identical that of BERT, differing only in the  3 downstream tasks were carried out viz. Form understanding using the FUNSD dataset, receipt information extraction using the SROIE dataset, and finally document classification on the RVL-CDIP dataset. The exact hyperparameters for each of the tasks differ and can be found in the paper. The overall idea of fine tuning is still the same, however slight improvements to all of these tasks as well as on other document AI tasks are constantly being made.</p>

<h4 id="sample-pipeline"><span class="me-2">Sample Pipeline:</span><a href="#sample-pipeline" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The document image is fed into the OCR system, which in turn generates the bounding boxes of each individual word in the page. Each coordinate x0, x1, y0, y1 along with the actual text are converted into a embedding vector, which in turn is concatenated. These embeddings are used to get pretrained LayoutLM embeddings. Parallely, the bounding boxes of each individual word along with the original document image is also fed into a faster RCNN, which generates the image embeddings. The image embedding and the pretrained LayoutLM embedding are combined and used for all downstream tasks.</p>

<figure class="image">
  <center>
  <a href="/images/table_extraction/LayoutLMArchitecture.png" class="popup img-link  shimmer"><img src="/images/table_extraction/LayoutLMArchitecture.png" alt="Sample Input for Layout LM" loading="lazy"></a>
  <figcaption>Sample Input for Layout LM</figcaption>
  </center>
</figure>

<h3 id="layout-lm-v2"><span class="me-2">Layout LM v2</span><a href="#layout-lm-v2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Research continued in 2 distinct directions in the case of document AI. The first continued with the initial approaches and tried to combine various NLP and CV individually pretrained models combining the outputs in a shallow manner. Although these methods invariably have state of the art performances on several datasets, there are a few key problems with continuing in this fashion. Such methods end up not performing well in case the document type is changed (from receipt understanding to form understanding) or in case the underlying domain is changed (medical documents to financial documents). This results in constant rework to adapt to every domain. The other approach of going about document AI tasks was the one introduced in LayoutLM, where the visual and text components are combined together to generate a unified pretrained model, and as per the task at hand, fine tuning is carried out with minimal effort. LayoutLM v2 (<a href="https://aclanthology.org/2021.acl-long.201/">Xu et al, 2021</a>) quite evidently chose the second route, fruther improving upon the ideas from the previous iteration. Although the number of fine tuning tasks on the which the results of the improved pretrained model were shown were also increased, the more significant changes were made in the pretraining stages.</p>

<ul>
  <li><strong>Pretraining Embeddings</strong>: Unline LayoutLM, where image and text embeddings were added in the finetuning stages, in the new iteration, the image information is encoded in the pretraining itself. To account for the changes, the embeddings are divided into the following categories
    <ul>
      <li>Text Embedding: To encapsulate the textual meaning, the text is initially separated into segments of <code class="language-plaintext highlighter-rouge">L</code> tokens each. In case, some tokens are smaller in lenghts, <code class="language-plaintext highlighter-rouge">[PAD]</code> tokens are used to fill out the gaps. <code class="language-plaintext highlighter-rouge">[CLS]</code> and <code class="language-plaintext highlighter-rouge">[END]</code> tokens are present to denote the start and the end of the text sequence in each segment. The final embedding per token however has a few extra information encoded.
        <ul>
          <li>Token Position embedding (PosEmb1D): Generated based on the position of the token in the segment</li>
          <li>Segment embedding (SegEmb): Generated based on the position of the segment amongst all the ones in the page</li>
          <li>Token embedding (TokEmb): The text embedding as done in LayoutLM</li>
        </ul>
      </li>
    </ul>

\[t_i = TokEmb(w_i) + PosEmb1D(i) + SegEmb(s_i)\]

\[0 ≤ i &lt; L\]

    <ul>
      <li>Visual Embedding: Visual information needs to be embedded to encapsulate information about font styles, text alignments, skew etc.
        <ul>
          <li>Resize document image to <code class="language-plaintext highlighter-rouge">224 X 224</code></li>
          <li>Feed resized image into the encoder architecture. The ResNeXt-FPN architecture is used as a backbone</li>
          <li>Average pool, the feature map output from the encoder to to get a fixed <code class="language-plaintext highlighter-rouge">W X H</code> size</li>
          <li>Flatten the output to get a <code class="language-plaintext highlighter-rouge">W H</code> size</li>
          <li>Each element in the resulting vector is projected linearly, to get the visual token embedding</li>
          <li>To maintain conformity with the textual embeddings, all the visual tokens (each element of the <code class="language-plaintext highlighter-rouge">W H</code> vector) are assigned the same segment <code class="language-plaintext highlighter-rouge">[C]</code>.</li>
          <li>Similar to textual embeddings, each element in the vector, also has a corresponding 1 D positional embedding.</li>
        </ul>
      </li>
    </ul>

\[v_i = Proj(VisTokEmb(I_i)) + PosEmb1D(i) + SegEmb([C])\]

\[0 ≤ i &lt; WH\]

    <ul>
      <li>Layout Embedding: Layout information becomes critical to embed spatial information and is similar to the 2D positional embeddings done for LayoutLM. The encapsulated knowledge here is critical, since in most cases of complex tables, the grammar won’t make sense, unless the text’s position in a table is known apriori. The layout embedding is generated for both visual and text tokens by discretizing and normalzing the bounding boxes of these tokens.</li>
    </ul>

\[l_i = Concat(PosEmb2D_x(x_0, x_1, w), PosEmb2D_y (y_0, y_1, h))\]

\[0 ≤ i &lt; WH + L\]
  </li>
</ul>

<figure class="image">
  <center>
  <a href="/images/table_extraction/Layoutv2Part1.png" class="popup img-link  shimmer"><img src="/images/table_extraction/Layoutv2Part1.png" alt="Generating embeddings from document image in LayoutLMv2 " loading="lazy"></a>
  <figcaption>Generating embeddings from document image in LayoutLMv2 </figcaption>
  </center>
</figure>

<ul>
  <li><strong>Spatial Aware Attention Mechanism</strong>: The textual embeddings (<code class="language-plaintext highlighter-rouge">T</code>), and the visual embeddings (<code class="language-plaintext highlighter-rouge">V</code>) are concatenated into a single vector. The resulting vector is added to the layout embedding vector (<code class="language-plaintext highlighter-rouge">L</code>). The resulting sequence (<code class="language-plaintext highlighter-rouge">X</code>) is fed into the transformer style architecture of BERT and LayoutLM. However, owing to the mulit modal input embeddings, absolute positional embeddings aren’t modelled. The whole purpose of trying to model the local invariance in the document would fail in case standard attention mechanism is used in conjunction with  relative positional embeddings. Therefore, the standard attention mechanism is equipped with 3 different bias terms to denote the learnable 1D and 2D biases along X and Y directions.</li>
</ul>

\[\alpha_{ij} = \alpha_{ij} + b_{j-i}^{(1D)} + b_{x_j - x_i}^{(2D_x)} + b_{y_j - y_i}^{(2D_y)}\]

<figure class="image">
  <center>
  <a href="/images/table_extraction/Layoutv2Part2.png" class="popup img-link  shimmer"><img src="/images/table_extraction/Layoutv2Part2.png" alt="Combining the Embeddings" loading="lazy"></a>
  <figcaption>Combining the Embeddings</figcaption>
  </center>
</figure>

<ul>
  <li><strong>Pretraining Tasks</strong> Although, the Masked Visual Language Model pretraining task was retained, the document classification task was scrapped and in place 2 new training strategies were introduced.
    <ul>
      <li><em>Text Image Alignment</em>: Randomly the portion in the document image, where a couple of text tokens lie are masked. The objective of the classifier being trained on the encoder outputs is to ascertain, whether the text token is visually present in the entire document image or not.</li>
      <li><em>Text Image Matching</em>: To gain a more coarse multi modal understanding the alignment task is slightly modified, and this time, the entire document image being fed could be a different page altogether, and the classifier needs to determine, whether the image and text belong to the same page or not.</li>
    </ul>
  </li>
</ul>

<p>It must be noted that all these tasks are carried out parallely with a combined loss function to prepare the pretrained model.</p>
<figure class="image">
  <center>
  <a href="/images/table_extraction/Layoutv2Part3.png" class="popup img-link  shimmer"><img src="/images/table_extraction/Layoutv2Part3.png" alt="Pretraining tasks" loading="lazy"></a>
  <figcaption>Pretraining tasks</figcaption>
  </center>
</figure>

<h3 id="global-table-extractor"><span class="me-2">Global Table Extractor</span><a href="#global-table-extractor" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Both frameworks introduced till now focussed on improving their pretraining methods so as to be able to perform better on all document AI tasks, not just table detection and extraction. Global Text Extractor (GTE), (<a href="https://arxiv.org/abs/2005.00589">Zheng et al, 2021</a>) takes a different approach by focussing on just extracting tables. The core idea behind GTE is to try and find individual cells from tables and relate the structure of the detected cells to one another and eventually identifying the entire table.</p>

<h4 id="contributions-in-terms-of-datasets"><span class="me-2">Contributions in terms of datasets</span><a href="#contributions-in-terms-of-datasets" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>
    <p>In the process of the development of such a framework, the authors managed to <strong>enhance PubTabNet</strong> by adding the cell structure annotations. The HTML version and the PDF version of the same document is matched. The HTML structure gives the logical structure of table cells, while the PDF gives the exact bounding boxes of each word. The combination of the 2 streams of data, therefore are useful in exactly annotating the boundary of each table cell.</p>
  </li>
  <li>
    <p>Tables differ a lot based on domains, and thus to not improve the performance of the GTE framework, a new dataset called <strong>FinTabNet</strong> was curated. This dataset contains annual reports from S&amp;P 500 companies. Processes similar to the enhanced PubTabNet were used to provide the table and the individual cell bounding boxes.</p>
  </li>
</ul>

<h4 id="architecture"><span class="me-2">Architecture</span><a href="#architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The entire GTE framework is construed by a combination of a few object detectors used in a sequence. The object detection system used here is repplacable, and can be chosen based on convenience. The 2 major components in the framework are GTE Table and GTE Cell that are used for table detection and cell structure recognition respectively.</p>
<figure class="image">
  <center>
  <a href="/images/table_extraction/GTE.png" class="popup img-link  shimmer"><img src="/images/table_extraction/GTE.png" alt="Overall Architecture" loading="lazy"></a>
  <figcaption>Overall Architecture</figcaption>
  </center>
</figure>

<ul>
  <li><strong>GTE Table</strong>: A object detector trained to find only cells, and another to find tables are run in parallel. It should be noted that the cell detector does so, without the knowledge of the overall bounding boxes of the tables. In addition to the standard loss functions of the individual object detectors, a cell constraint penalty loss function is utilized. This loss function penalizes the outputs of the table detector by comparing the tables detected with the cells produced. A few simple rules dictated by the structure of tables are used to calculate the penalty.
    <ul>
      <li>The percentage of the area covered by the cells inside a table is lower than a threshold.</li>
      <li>Area just inside the table (around the boundary of the table), has too few cells. Since it is rare for the 1st row and columns to be empty in tables</li>
      <li>Area outside the table contains cells.</li>
      <li>The bottom of the table does not have many cells. Absence of enough cells in the final few rows indicate that the table boundary could have been drawn earlier. 
The hyperparameters used to specify each of the thresholds in these constraint conditions are provided in detail in the Supplementary section of the paper. The final constraint loss function is thereby used to score the table bounding boxes and again, and final predictions are made based on these new rankings.</li>
    </ul>
  </li>
</ul>
<figure class="image">
  <center>
  <a href="/images/table_extraction/GTETable.png" class="popup img-link  shimmer"><img src="/images/table_extraction/GTETable.png" alt="Structure of GTE Table" loading="lazy"></a>
  <figcaption>Structure of GTE Table</figcaption>
  </center>
</figure>

<ul>
  <li><strong>GTE Cell</strong>: The table locations obtained from the output of the GTE Table component is masked in the original full page image that was fed to GTE Table. The masked image is then fed into a network to determine the kind of subsequent cell detection network it should be fed to. In some tables, cells are demarcated based on drawn lines (both horizontal and vertical), while in some cases, the boundaries of the cells are understood, and not explicitly drawn. Therefore, a preliminary check to determine, whether the drawn lines are useful demarcators or not is done by the first network. The output for the cell detectors is then post processed, so that any text box inside the table that did not overlap with any of the cell bounding boxes does not go unassigned.</li>
</ul>
<figure class="image">
  <center>
  <a href="/images/table_extraction/GTECell.png" class="popup img-link  shimmer"><img src="/images/table_extraction/GTECell.png" alt="Structure of GTE Cell" loading="lazy"></a>
  <figcaption>Structure of GTE Cell</figcaption>
  </center>
</figure>

<p>Note: The performance on individual datasets for all the frameworks mentioned here were state of the art when they were published, and the optimal hyperparameter configurations for each of the tasks are mentioned in detail in the respective papers.</p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/technical/"
            class="post-tag no-text-decoration"
          >technical</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Getting%20Started%20with%20Table%20Extraction%20in%20Document%20AI%20-%20Yash%20Sarrof&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FTableContentExtraction%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Getting%20Started%20with%20Table%20Extraction%20in%20Document%20AI%20-%20Yash%20Sarrof&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FTableContentExtraction%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FTableContentExtraction%2F&text=Getting%20Started%20with%20Table%20Extraction%20in%20Document%20AI%20-%20Yash%20Sarrof" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Latest Posts</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/TableContentExtraction/">Getting Started with Table Extraction in Document AI</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/SadamOptimizer/">SAdam: An Adam variant that converges faster for convex loss functions</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Hopeless/">Agony, expectation, relief & the journey in between</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/technical/">technical</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reflections/">reflections</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/projects/">projects</a>
      
    </div>
  </section>


            </div>

            
              
              



  <section id="toc-wrapper" class="d-none ps-0 pe-4">
    <h2 class="panel-heading ps-3 mb-2">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    










  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/SadamOptimizer/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1616968800"
  data-df="ll"
  
>
  Mar 29, 2021
</time>

              <h4 class="pt-0 my-2">SAdam: An Adam variant that converges faster for convex loss functions</h4>
              <div class="text-muted">
                <p>SAdam (Wang et al, 2019) is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these mod...</p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/LinuxTools/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1602280800"
  data-df="ll"
  
>
  Oct 10, 2020
</time>

              <h4 class="pt-0 my-2">My Personal Productivity Hacks: Open Source Tools for the Adventurous Linux User</h4>
              <div class="text-muted">
                <p>As a Computer Science engineer every miniscule jump in productivity in and around the workspace tends to have a butterfly effect, and ends up being a huge time savior. Over the years, I have disove...</p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/GardnerTheory/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1583794800"
  data-df="ll"
  
>
  Mar 10, 2020
</time>

              <h4 class="pt-0 my-2">Using Gardner's Multiple Intelligence Theory to gauge aptitude of children</h4>
              <div class="text-muted">
                <p>Howard Gardner in 1983 came up with a theory to challenge the traditional view on Intelligence. According to him, cognitive abilities of an individual does not solely constitue intelligence. He cla...</p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/SadamOptimizer/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>SAdam: An Adam variant that converges faster for convex loss functions</p>
    </a>
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Newer">
      <p>-</p>
    </div>
  
</nav>

            
              
              <!-- The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2024</time>

    
      <a href="https://twitter.com/yashYRS">Yash Sarrof</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/technical/">technical</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reflections/">reflections</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/projects/">projects</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->
    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->




  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.11/dayjs.min.js,npm/dayjs@1.11.11/locale/en.min.js,npm/dayjs@1.11.11/plugin/relativeTime.min.js,npm/dayjs@1.11.11/plugin/localizedFormat.min.js,npm/tocbot@4.27.20/dist/tocbot.min.js"></script>






<script src="/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  







    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5">Oops! No results found.</p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

