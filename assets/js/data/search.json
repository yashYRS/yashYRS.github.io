[
  
  {
    "title": "Getting Started with Table Extraction in Document AI",
    "url": "/posts/TableContentExtraction/",
    "categories": "",
    "tags": "technical",
    "date": "2021-09-01 00:00:00 +0200",
    





    
    "snippet": "Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layou...",
    "content": "Table extraction fall under the umbrella of Document Intelligence, a relatively new research topic that deals with analzying and understanding business documents. The documents vary in style, layouts, fonts and generally have a complex template. Since digitisation of documents is a recent phenonmenon, majority of such documents are scanned copies of their printed counterparts. The poor quality of images, skew arising due to scans made in haste compound the difficulty of the problem. Especially in the financial domain, where the significance of every number and alphabet is paramount. Manual supervision along with some software aid is the current norm, however each day, efforts are made to reduce the amount of interventions required by humans. Extracting tables from these documents is one such sub domain, which has attracted a lot of researchers. Tables do not have a specified way of being constructed, and often the artistic proclivities involved in making tables look more presentable, add a layer to the difficulty of the problem. Most of the information in tables would make sense, if the relationships and contexts are known prior, as tables generally contain limited data. I will attempt to give an intuition behind some of the work being carried out in this area, and hopefully spark enough interest in anyone reading this, to delve deeper in the field.      Tables not following a pattern in Multi column layouts        Variations in appearance of tables  Popular DatasetsSome popular datasets that have been curated over the last few years in order to facilitate testing and training of Document AI tasks and will be used in this post are briefly described here. The disparity in the number of samples across datasets depends on whether the entire dataset was manually curated or generated automatically in a semi-supervised fashion.            Dataset Name      Number of Samples      Document categories covered                  IIT-CDIP (Illinois Institute of Technology Complex Document Information Processing Test Collection)      Made from over 6 Million Scanned documents      Spans all kinds of documents                  RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing)      400K grayscale images (320K training, 40K validation, and 40K test images)      Subset of IIT-CDIP                  SROIE (Scanned Receipt OCR and Information Extraction)      1000      Scanned Receipts                  FUNSD (Form Understanding in Noisy Scanned Documents)      199 fully annotated forms, 31485 words, 9707 semantic entities, 5304 relations Samples      Exclusively contains forms in scanned documents                  CORD (COVID-19 Open Research Dataset)      Curated from 500K scholarly articles      Articles about COVID-19, SARS-CoV-2, and related coronaviruses                  Kleister NDA      540 NDAs, 3299 unique pages      Scanned and born-digital long formal Non Disclosure Agreements                  Doc VQA      12767 Document Images      Industrial documents including typewritten, printed, handwritten and born-digital text                  PubLayNet      made from over 1 million PDF articles      Scientific articles and reports in the medical domain                  PRImA (Pattern Recognition and Image Analysis) Layout Anaylsis Dataset      305 ground-truthed images      Magazines and techinical articles spanning multiple domains                  Table Bank      417,234 high quality labeled tables      Tables extracted from LaTeX and Word documents                  HJ Historical Japanese Dataset      Over 250,000 layout element annotations of seven types      Complex layouts from all kinds of historical documents      Initial Approaches      The first efforts to compartmentalize a report into tabular and non tabular areas were made in 2016, where existing object detection architectures (CNNs, R - CNNs and their variants) were trained to detect tables in pdfs.        In 2018, an end to end framwework was recommended in order to extract semantic structures of tables and paragraphs from documents. Pretrained word embeddings were in a fully conventional network to get decent results.        In 2019, a Graph Convolutional Network was proposed, which combined visual and textual cues from a document for achieving the same result of identifying semantic structures.  Layout LMAlmost every iteration brought with itself promising results, and had scope of application in real world datasets, However 2 aspects which the inital approaches hadn’t tried, and was first explored by LayoutLM (Xu et al, 2020) were  Avoiding reliance on labelled data, since the number of such publicly known datasets were limited  Trying joint pretrained models taking both textual and layout information into account. Up until now, the pretrained models were either CV or NLP models.Proposed by a team at Microsoft Research Asia, LayoutLM constituted a novel approach to simply pretraining for document AI tasks, and recommended fine tuning for subsequent tasks as per need. LayoutLM extends the core idea used in BERT to gain better performance in document AI tasks.Modifications made to the BERT pretraining stage:  Along with the word embeddings, embeddings to leverage the visual layout information present in the documents were added.          2d Position Embedding: The top left corner of the document page is considered to be the origin, and the bounding box for the word in the resulting coordinate system is stored as (x0, y0, x1, y1), with x0, y0 representing the upper left, and (x1, y1) representing the bottom right corner respectively. This helps model sparsity of the current word in the page, vicinity to other words, size of the current word relative to others and other spatial features, that would otherwise not be stored in case of traditional word embeddings.      Image Embedding:                  Word Image Embedding: These image embeddings are generated by a faster R-CNN, which is fed the document image and the Region of Interest is specified by an OCR system that gives the bounding box results of the word in question.          [CLS] token Image Embedding: Using the same faster R-CNN, embedding of the entire document image is generated the Region of Interest set as the entire page. This is generated to assist in the downstream fine tuning tasks.                          The pretraining task of Masked Language Modelling(MLM) changed to Masked Visual Language Model(MVLM). Just as 15% of the text tokens were masked in the original task. Here, word embedding is masked but the 2d Positional embeddings are retained, thereby utilizing language contexts in conjunction with spatial information to predict the masked token.    The pretraining task of Next Sentence Prediction (NSP) changed to Multi Label Document Classification (MDC). This change was made to encapsulate the knowledge from different document domains and thus generate better document level representations.LayoutLM Fine tuning:The fine tuning stage in LayoutLM is virtually identical that of BERT, differing only in the  3 downstream tasks were carried out viz. Form understanding using the FUNSD dataset, receipt information extraction using the SROIE dataset, and finally document classification on the RVL-CDIP dataset. The exact hyperparameters for each of the tasks differ and can be found in the paper. The overall idea of fine tuning is still the same, however slight improvements to all of these tasks as well as on other document AI tasks are constantly being made.Sample Pipeline:The document image is fed into the OCR system, which in turn generates the bounding boxes of each individual word in the page. Each coordinate x0, x1, y0, y1 along with the actual text are converted into a embedding vector, which in turn is concatenated. These embeddings are used to get pretrained LayoutLM embeddings. Parallely, the bounding boxes of each individual word along with the original document image is also fed into a faster RCNN, which generates the image embeddings. The image embedding and the pretrained LayoutLM embedding are combined and used for all downstream tasks.      Sample Input for Layout LM  Layout LM v2Research continued in 2 distinct directions in the case of document AI. The first continued with the initial approaches and tried to combine various NLP and CV individually pretrained models combining the outputs in a shallow manner. Although these methods invariably have state of the art performances on several datasets, there are a few key problems with continuing in this fashion. Such methods end up not performing well in case the document type is changed (from receipt understanding to form understanding) or in case the underlying domain is changed (medical documents to financial documents). This results in constant rework to adapt to every domain. The other approach of going about document AI tasks was the one introduced in LayoutLM, where the visual and text components are combined together to generate a unified pretrained model, and as per the task at hand, fine tuning is carried out with minimal effort. LayoutLM v2 (Xu et al, 2021) quite evidently chose the second route, fruther improving upon the ideas from the previous iteration. Although the number of fine tuning tasks on the which the results of the improved pretrained model were shown were also increased, the more significant changes were made in the pretraining stages.  Pretraining Embeddings: Unline LayoutLM, where image and text embeddings were added in the finetuning stages, in the new iteration, the image information is encoded in the pretraining itself. To account for the changes, the embeddings are divided into the following categories          Text Embedding: To encapsulate the textual meaning, the text is initially separated into segments of L tokens each. In case, some tokens are smaller in lenghts, [PAD] tokens are used to fill out the gaps. [CLS] and [END] tokens are present to denote the start and the end of the text sequence in each segment. The final embedding per token however has a few extra information encoded.                  Token Position embedding (PosEmb1D): Generated based on the position of the token in the segment          Segment embedding (SegEmb): Generated based on the position of the segment amongst all the ones in the page          Token embedding (TokEmb): The text embedding as done in LayoutLM                  \\[t_i = TokEmb(w_i) + PosEmb1D(i) + SegEmb(s_i)\\]\\[0 ≤ i &lt; L\\]          Visual Embedding: Visual information needs to be embedded to encapsulate information about font styles, text alignments, skew etc.                  Resize document image to 224 X 224          Feed resized image into the encoder architecture. The ResNeXt-FPN architecture is used as a backbone          Average pool, the feature map output from the encoder to to get a fixed W X H size          Flatten the output to get a W H size          Each element in the resulting vector is projected linearly, to get the visual token embedding          To maintain conformity with the textual embeddings, all the visual tokens (each element of the W H vector) are assigned the same segment [C].          Similar to textual embeddings, each element in the vector, also has a corresponding 1 D positional embedding.                  \\[v_i = Proj(VisTokEmb(I_i)) + PosEmb1D(i) + SegEmb([C])\\]\\[0 ≤ i &lt; WH\\]          Layout Embedding: Layout information becomes critical to embed spatial information and is similar to the 2D positional embeddings done for LayoutLM. The encapsulated knowledge here is critical, since in most cases of complex tables, the grammar won’t make sense, unless the text’s position in a table is known apriori. The layout embedding is generated for both visual and text tokens by discretizing and normalzing the bounding boxes of these tokens.    \\[l_i = Concat(PosEmb2D_x(x_0, x_1, w), PosEmb2D_y (y_0, y_1, h))\\]\\[0 ≤ i &lt; WH + L\\]        Generating embeddings from document image in LayoutLMv2     Spatial Aware Attention Mechanism: The textual embeddings (T), and the visual embeddings (V) are concatenated into a single vector. The resulting vector is added to the layout embedding vector (L). The resulting sequence (X) is fed into the transformer style architecture of BERT and LayoutLM. However, owing to the mulit modal input embeddings, absolute positional embeddings aren’t modelled. The whole purpose of trying to model the local invariance in the document would fail in case standard attention mechanism is used in conjunction with  relative positional embeddings. Therefore, the standard attention mechanism is equipped with 3 different bias terms to denote the learnable 1D and 2D biases along X and Y directions.\\[\\alpha_{ij} = \\alpha_{ij} + b_{j-i}^{(1D)} + b_{x_j - x_i}^{(2D_x)} + b_{y_j - y_i}^{(2D_y)}\\]      Combining the Embeddings    Pretraining Tasks Although, the Masked Visual Language Model pretraining task was retained, the document classification task was scrapped and in place 2 new training strategies were introduced.          Text Image Alignment: Randomly the portion in the document image, where a couple of text tokens lie are masked. The objective of the classifier being trained on the encoder outputs is to ascertain, whether the text token is visually present in the entire document image or not.      Text Image Matching: To gain a more coarse multi modal understanding the alignment task is slightly modified, and this time, the entire document image being fed could be a different page altogether, and the classifier needs to determine, whether the image and text belong to the same page or not.      It must be noted that all these tasks are carried out parallely with a combined loss function to prepare the pretrained model.      Pretraining tasks  Global Table ExtractorBoth frameworks introduced till now focussed on improving their pretraining methods so as to be able to perform better on all document AI tasks, not just table detection and extraction. Global Text Extractor (GTE), (Zheng et al, 2021) takes a different approach by focussing on just extracting tables. The core idea behind GTE is to try and find individual cells from tables and relate the structure of the detected cells to one another and eventually identifying the entire table.Contributions in terms of datasets      In the process of the development of such a framework, the authors managed to enhance PubTabNet by adding the cell structure annotations. The HTML version and the PDF version of the same document is matched. The HTML structure gives the logical structure of table cells, while the PDF gives the exact bounding boxes of each word. The combination of the 2 streams of data, therefore are useful in exactly annotating the boundary of each table cell.        Tables differ a lot based on domains, and thus to not improve the performance of the GTE framework, a new dataset called FinTabNet was curated. This dataset contains annual reports from S&amp;P 500 companies. Processes similar to the enhanced PubTabNet were used to provide the table and the individual cell bounding boxes.  ArchitectureThe entire GTE framework is construed by a combination of a few object detectors used in a sequence. The object detection system used here is repplacable, and can be chosen based on convenience. The 2 major components in the framework are GTE Table and GTE Cell that are used for table detection and cell structure recognition respectively.      Overall Architecture    GTE Table: A object detector trained to find only cells, and another to find tables are run in parallel. It should be noted that the cell detector does so, without the knowledge of the overall bounding boxes of the tables. In addition to the standard loss functions of the individual object detectors, a cell constraint penalty loss function is utilized. This loss function penalizes the outputs of the table detector by comparing the tables detected with the cells produced. A few simple rules dictated by the structure of tables are used to calculate the penalty.          The percentage of the area covered by the cells inside a table is lower than a threshold.      Area just inside the table (around the boundary of the table), has too few cells. Since it is rare for the 1st row and columns to be empty in tables      Area outside the table contains cells.      The bottom of the table does not have many cells. Absence of enough cells in the final few rows indicate that the table boundary could have been drawn earlier. The hyperparameters used to specify each of the thresholds in these constraint conditions are provided in detail in the Supplementary section of the paper. The final constraint loss function is thereby used to score the table bounding boxes and again, and final predictions are made based on these new rankings.            Structure of GTE Table    GTE Cell: The table locations obtained from the output of the GTE Table component is masked in the original full page image that was fed to GTE Table. The masked image is then fed into a network to determine the kind of subsequent cell detection network it should be fed to. In some tables, cells are demarcated based on drawn lines (both horizontal and vertical), while in some cases, the boundaries of the cells are understood, and not explicitly drawn. Therefore, a preliminary check to determine, whether the drawn lines are useful demarcators or not is done by the first network. The output for the cell detectors is then post processed, so that any text box inside the table that did not overlap with any of the cell bounding boxes does not go unassigned.      Structure of GTE Cell  Note: The performance on individual datasets for all the frameworks mentioned here were state of the art when they were published, and the optimal hyperparameter configurations for each of the tasks are mentioned in detail in the respective papers."
  },
  
  {
    "title": "SAdam: An Adam variant that converges faster for convex loss functions",
    "url": "/posts/SadamOptimizer/",
    "categories": "",
    "tags": "technical, projects",
    "date": "2021-03-29 00:00:00 +0200",
    





    
    "snippet": "SAdam (Wang et al, 2019) is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these mod...",
    "content": "SAdam (Wang et al, 2019) is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases. I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper.A mathematical function is called convex if a 2nd order of differentiation is possible, and if the second derivative is never negative. Owing to this, one of the properties of convex functions is that they have a global minimum. This has led to the adoptiion convex loss functions, although for training deep neural networks working with non-convex loss functions become imperative.SAdam follows the general framework of Adam, deploying a faster decaying rate step size controlled by time-variant hyperparameters to exploit strong convexity. Although it must be mentioned that similar ideas have successfully been applied in the past to the frameworks of Adagrad and RMSProp to get the variants called SC-Adagrad and SC-RMSProp (Mukkamala &amp; Hein, 2017). Theoretical analysis of SAdam show a data-dependent O(logT) regret bound for strongly convex functions, which means that it converges faster than AMSgrad and AdamNC in such cases.We performed all the experiments mentioned in the original paper by the authors to verify their claims. We added a few experiments of our own to verify whether only the best results on some of the datasets were cherry picked in the original paper. Therefore we ended up performing the following tasks:      Calculation of Regret for L2 Regularized Logistic Regression on MNIST, CIFAR10, CIFAR100 for our pool of optimizers, to check whether the central claim of better performance on convex problems stands.        Computing test accuracy and training loss for a 4-layer CNN and ResNet18 on MNIST, CIFAR10, CIFAR100 for our pool of optimizers to check whether SAdam outperforms the existing optimization techniques in the context of deeper networktraining as well, which is inherently a non-convex optimization problem.        Training a Multi-Layer LSTM on the PennTreeBank dataset on the Language Modelling task, to test the performance of SAdam in a context different from the usual vision domains it had hitherto been tested on  It must also be noted that, the authors had implemented the optimizer in Tensorflow and in order to make the optimizer more accessible, we rewrote everything in PyTorch. Not just SAdam, in order to compare all the optimizers mentioned in the paper, we implemented SC_RMSProp and SC_Adagrad in Pytorch as well. All the correpsonding code can be found in this repository. The steps to install and run the optimizers are given in detail there. In addition, the details of all of the experiments that were carried out, including the optimal hyperparameters for each are provided in the reproducibility report published on the OpenReview Portal.Implementation of the SAdam optimizerTo implement custom optimizers in Pytorch, the functions of the default torch.optim.Optimizer viz. __init__ and step() need to be overriden.  __init__: This function serves as the point where all the parameters for the optimizers are initialized. Thus the mandatory params parameter needs to initialized with a dictionary of hyperparameters necessary for optimization. It should be noted however weight_decay is not a part of the optimizer, but is a way of adding L2 Regularization loss in Pytorch.def __init__(self, params, beta_1=0.9, lr=0.01, delta=1e-2, xi_1=0.1,             xi_2=0.1, gamma=0.9, weight_decay=1e-2):    # Making the dictionary with all of the hyperparameters    defaults = dict(lr=lr, beta_1=beta_1, delta=delta, xi_1=xi_1,                    xi_2=xi_2, gamma=gamma, weight_decay=weight_decay)    super(SAdam, self).__init__(params, defaults)  step : A sample optimization step is mentioned in this function. closure is required for a few loss functions, where closure is utilized to terminate the loss function updates by the optimizer (not required for SAdam). The code given below performs 1 step on all of the parameters, and is almost a replica of the Adam optimizer, except that β2 is time variant in SAdam, and instead of dividing by the square root for the second moment vector (denoted by v_t in the code given below), we use it as is. Intuitively, the larger decay caused due to this results in faster convergence, while the time variant hyperparameters make sure the convergence is not unstable.def step(self, closure=None):    loss = None    if closure is not None:        loss = closure()    for group in self.param_groups:        for p in group['params']:            if p.grad is None:                continue            grad = p.grad            state = self.state[p]            # Initialize the derivatives if they haven't been updated yet            if len(state) == 0:                state['step'] = 0                # Exponential moving average of gradient values                state['hat_g_t'] = torch.zeros_like(                    p, memory_format=torch.preserve_format)                # Exponential moving average of squared gradient values                state['v_t'] = torch.zeros_like(                    p, memory_format=torch.preserve_format)            # Extract all the hyperparameters for the optimizer            lr, delta = group['lr'], group['delta']            xi_1, xi_2 = group['xi_1'], group['xi_2']            hat_g_t, v_t = state['hat_g_t'], state['v_t']            gamma, beta_1 = group['gamma'], group['beta_1']            # Update the step taken            state['step'] += 1            # L2 Regularization performed, if weight decay is initialized            if group['weight_decay'] != 0:                grad = grad.add(p, alpha=group['weight_decay'])            time_step = state['step']            beta_2 = 1 - gamma/time_step            hat_g_t.mul_(beta_1).add_(grad, alpha=1 - beta_1)            v_t.mul_(beta_2).addcmul_(grad, grad, value=1-beta_2)            denom = time_step*v_t + delta            p.addcmul_(hat_g_t, 1/denom, value=-lr)return loss"
  },
  
  {
    "title": "Agony, expectation, relief & the journey in between",
    "url": "/posts/Hopeless/",
    "categories": "",
    "tags": "reflections",
    "date": "2020-12-18 00:00:00 +0100",
    





    
    "snippet": "Somewhere in the middle of February 2020, I was moving into my first rented house. The place was perfect, it was isolated enough from the main roads to be quite and sombre at night, whilst also bei...",
    "content": "Somewhere in the middle of February 2020, I was moving into my first rented house. The place was perfect, it was isolated enough from the main roads to be quite and sombre at night, whilst also being a 10 minute walk from the Samsung R&amp;D Bangalore Campus where I was interning. The internship stipend was healthy, and so life in the city was pretty exciting. Weekend get togethers, making my own food, collaborating on expensive side projects that previously wouldn’t even have crossed my mind, life had almost never been better. I had already applied for my Masters to the college of my choice KU Leuven in Belgium. However, I was banking on my internship being converted to a placement offer by Samsung, since I really liked the work and the environment around me. Working for a couple of years at this company to build up my capital and then applying again would have been the ideal scenario. By then, the first news of COVID appearing in India had already started coming in. As always, first warnings are rarely paid heed to by people at large, and I was no different. Eventually reality did kick in, and by the time complete lockdown was announced, there was panic amongst all the people I knew in Bangalore. Most of them started booking their flights to go back to their home towns in order to not be stuck. Me and a couple of other friends that I had made at Samsung, decided to stay back in Bangalore during the lockdown, hoping for things to improve quickly and avoiding the risk of travelling. I had not anticipated for the lockdown to keep increasing from the initial proposal of 21 days. There was a partial relaxation after around 45 days though, and our office campus was opened to 10% of the employees. I requested to be allowed in the campus, since availing the services of the office pantry would have significantly helped me avoid preparing all 3 meals at home everyday without a regular supply of essentials. My division head agreed, since most of the full time employees were reluctant to come to the office, as they lived with their families and did not want to take any risks.Around this time I found out that I had gotten a admit in KU Leuven and was relieved to have secured a safe backup, in case my internship did not convert. The process of getting the placement offer hinged on a positive appraisal by team managers (which I had already received) and me passing the “Advanced Test” of Samsung. The caveat was that, Samsung conducts these tests on their premises or on college campuses only when there are sufficient enough people to give the test. The test was originally supposed to happen in the last week of March, but owing to lockdown and the flurry of people that had gone back to their home towns, the test kept getting postponed. My team manager in the last week of my internship in June 2020, told me that he had tried his best to find a loophole of sorts to get me to stay a little longer to be able to take the test as soon as it was feasible, but his efforts hadn’t borne any fruit. The last communication from the HR ended on a positive note, when they said that they were looking at the possibility of holding the test online. They had also given indications albeit unofficially, that the advanced exam would probably be held in the month of September. I was sent links to 2 consecutive practice tests that they were using to gauge the feasibility of actually relying on the online exam completely. My backup of going for Masters right away was also falling apart as there were no signs of the Belgium Consulate opening for visa appointments anytime soon in any part of the country. Staying in Bengaluru whilst job hunting and incurring the expenses for rent without a stable income made little sense. Therefore in the last week of June 2020, I came back to my hometown of Kolkata.A bad decisionI could have started my job hunt right away and taken the best offer at hand. Whenever Samsung would reply back with the link to the Advanced test, I would give the test and on passing, again evaluate the better job. In hindsight I should have chosen to do this. Today, I dont relate to the idealistic world view I held at that time. I was probably smitten by the work environment that I had seen at Samsung, and desperately wanted to work there. In either case, I chose to take up a 3 month internship in the R&amp;D department at a local firm called Fortuna Impex Private Limited within my first week of arrival at Kolkata. The rationale behind this was to just keep myself occupied till the Advanced test, clearing which, in my head was child’s play. The work environment at this firm however turned out to be awful. My colleagues would procrastinate and only work when the CEO would come knocking around asking questions. There was no defined hierarchy amongst the leadership ergo conflicting instructions about the next steps in the project were a routine occurrence. I took advice from my relatives who were entrepreneurs based in Kolkata, who told me that these things were commonplace and I simply needed time to get used to the work culture in Bengal. I decided to continue after listening to their views, and hoped that things would improve.When it rains it pours15th August, Independence day in India is a public holiday, and a lot of essential services are also generally closed. This going to sound comical, but I was skimming through the day’s newspapers, glad to have gotten an off day from the office, when I took a yawn and my mouth got stuck. I literally wasn’t being able to do something as simple as close my mouth. I tried for a while, pressuring it with my hands, but it was stuck. Helpless, I went to my younger brother, who didn’t understand what was happening. With my mouth stuck, I was just making noises and wasn’t being to speak. I somehow gestured to him to pass me a pen and some paper. I wrote that my mouth was stuck, and he started laughing, thinking I was making a joke. After a while, he realized this was a serious problem, and we woke our parents from their afternoon nap. They suggested a heat compress to calm the nerves, but nothing seemed to work. We tried to call up the clinics we knew of nearby, and all of them did not have a dentist scheduled to come that day. The emergency services were also not willing to respond right away, since they were swamped with COVID cases at that time, and wanted a RT-PCR report before taking me in. We eventually discovered a doctor who lived a few minutes away and was willing to help us out. We went there and the doctor closed my mouth. My mouth had remained stuck in an open position for around 90 minutes, and I was in a lot of pain. He said that he had forcefully closed it at the moment, and that I in all possibility had severe TMJ (temporomandibular joint) disorder. He suggested we cover up skull with a bandage tightly so that my mouth does not open much and that I visit a maxillofacial surgeon at the earliest, since this could recurr at any point. I consulted with Mr. Utsa Butta, who I had found was one of the best in this field. After multiple visits, blood tests, RT-PCR tests and sleep studies, I was diagnosed with level 2 of TMJ and was told that, that the entire treatment could span over 3 years, but it was curable. We got started with the treatment plan right away. I had never contemplated quitting an internship before, but the pain, being on a liquid diet and hence low on energy coupled with the toxic work environment were too much for me, and I tendered my resignation in the the 1st week of September, and after completing the knowledge transfer in the next few days, left the internship on the 12th September.The steadily rising COVID cases in Belgium and the constant unabated pain in my Jaw meant that the plan of going for Masters right away had to be dropped. I decided to wait for the treatment to complete and only then apply for Masters again. Around this time, I contacted my ex team manager at Samsung, who hinted that HR had other priorities and were not even considering the Advanced test for the interns. Owing to a mixture of naive decision making, an unusual health problem and an unforeseen global crisis, a promising situation in March had gradually turned into a grim situation in September. I finally started looking for jobs that allowed remote work so as to not hinder my treatment.Trying to navigate out of the muddleOver the course of October, I submitted a plethora of applications to a lot of jobs. I got my first break in the 2nd week of October, when I cleared multiple rounds for the post of a Junior Research Fellow at IIT Kharagpur. The posting was remote till at least a couple of months, and even after that, Kharagpur was 3-4 hour train journey from my place. I made it to the last round, eventually losing out to the only other applicant who had made it there. I was not making a lot of headways in other companies of my choice, and all throughout the feeling of being worthless for not being able to advance my career in any way whatsoever, was difficult to shake off.Finally, I got my first job offer at Bharat Electronics Limited (BEL). During the final interview, I had been informed by the panelists that the joining would be within 2 weeks. I had to submit some bit of paperwork, and was told that within a week of completing the same, I would get an email with the joining date. Having learnt the lesson from last time, I intended not to stop the application process till I got the joining date, and indeed weeks upon weeks passed and there was no communication from them.In the first week of December, I got an email from Crossover, stating that I had made it to the final round. It was one of the first companies that I had applied to, and had cleared all their preliminary rounds in September itself. I was surprised by the late response but scheduled an interview right away. The interview went well, and soon after a job offer came along. They wanted me to start within 4 days on the coming Monday. However, the job offer was contingent on clearing an aptitude test that would be taken on the 1st day of work. The company had changed the format for their aptitude test deeming the test that I had given initially, invalid. I accepted the offer without any qualms, as this job fulfilled all the criterias in my checklist. I got done with an impending minor tongue surgery (lingual frenectomy) which was a part of the treatment for TMJ. I had been putting it off, since it can take around 7 days until after the surgery to start speaking coherently again, and I wanted to get it done, once I knew there were not going to be any clashes with any interviews.Darkest before the dawn, they sayI remember the day clearly, it was the 14th of December, my intended 1st day at the job at Crossover. I had worked for about 3 hours, when I got the mail with the link for the aptitude exam. There was a personal proctor that was assigned to me for the duration of the test, which was supposed to have 50 questions, that I had to answer in 30 minutes. Around 15 minutes into the test, something that never happens in my house happened, the electricity in my house went away. It takes about 2-3 minutes for the generators to get the power back on. The proctor deemed the test invalid for the interruption that had happened. I could not fight back properly on call, since I had impaired speech at that time, but I explained the hapless nature of it all, soon after in an email thread. Subsequently, I was given another chance to give the test after around 6 hours. In the meantime, I continued working as it was still supposedly the 1st day at the job. I also  arranged a Tier 3 Internet connection in a matter of hours, and told my parents to buy a small inverter from the local shop for the room that I was working in. I gave the test without any interruptions. Within minutes of completion, I got an email from the Crossover team with the results. I had scored 44/50 in that test. I broke down in disappointment, as the cut off was 45. After 3 months of hustling, I was back to square one.On 15th December I went back to the Doctor, for him to inspect my recovery post the frenectomy. I was told, that I should be able to start speaking within a day or two. I had stopped applying to new places, ever since the surgery, and I completed my first application since then on the 16th Night. On the morning of 17th December, I finally felt my voice again and was being able to speak clearly albeit in a low voice. I went through the usual drills, looking for callbacks, or emails regarding the applications that I had made. It had been 7 weeks since the BEL job offer, and there was still no word from them. Having found nothing, I had started looking for more opportunities, when I saw a call from an unknown number. I picked it up, and was surprised to know that one of the CEOs of the startup that I had applied to last night had called me. He said, that he had looked at my profile and wanted to proceed for an informal interview right away. I didn’t mind, and after about 30 minutes of an impromptu interview, he said that he wanted to take a final technical interview, and if possible wanted to get it done right away. He sent me a link to a video conference, where there were 3 other panelists. The interview spanned more than 2 hours, and at the end, Mohit who had taken my first interview said that he would get back to me, by the end of the day. Having experienced multiple hollow claims in the past couple of months I had started to disregard such assertions unless given to me in writing. But true to his word, Mohit did call me within 3 hours, and made me a job offer. After negotiating on the terms for a while, I accepted. I was to start the job on literally the next day, which incidentally happened to be my birthday as well.I have long been a repudiator of the theories involving happenchance. However, the complete turnaround from having no hope of securing a job in the near future and an inability to speak intelligibly, that just somehow happened to coincide with my birthday, to this day makes me reconsider my views on fate."
  },
  
  {
    "title": "My Personal Productivity Hacks: Open Source Tools for the Adventurous Linux User",
    "url": "/posts/LinuxTools/",
    "categories": "",
    "tags": "technical",
    "date": "2020-10-10 00:00:00 +0200",
    





    
    "snippet": "As a Computer Science engineer every miniscule jump in productivity in and around the workspace tends to have a butterfly effect, and ends up being a huge time savior. Over the years, I have disove...",
    "content": "As a Computer Science engineer every miniscule jump in productivity in and around the workspace tends to have a butterfly effect, and ends up being a huge time savior. Over the years, I have disovered a few open source programs that have had a similar effect for me, and despite a oblique learning curve for most of them, they have become indispensible aspects of my working environment. It should be noted that most of these tools are specific to Linux, and anyone using other some other Operating System might not find much value from this post. Another common thread amongst everything listed here is that they are all open source, and have an active community. The configuration files for my system can be found here, although I would urge everyone to rice their own systems, since the configurability of each and every tool is what makes Linux so beautiful to work with.NewsboatThe amount of information that needs to be consumed on a daily basis keeps compounding over time, if not kept in check. There are many a technical blogs that one can follow to keep in touch with the latest trends in the industry. However, again filtering out the posts that are relevant to one’s domain can end up being time consuming and tiresome. I used to navigate to each such blog every time a mail notification would come up. I came across the concept of RSS feeds and stumbled upon newsboat, when I looked around for suggestions to reduce the time taken for reading each of these blogs. On Homebrew, newsboat is included by default, however I had to download it in my Ubuntu system. It is essentially a HTML renderer, and enables one to read the RSS feeds on a terminal.      How a post would look like on a RSS-feed reader    There aren’t many dependencies, and can be installed easily from one’s relevant package managers.    $ sudo apt install newsboat        Once install is complete, add the links to the blogs one wants to follow by adding it to the .newsboat/urls file in the .newsboat folder. At the end of the link, tags can be added to each of the links, to filter posts.      Newsboat greets you with the list of blogs added to the config    The feeds can be refreshed manually, however I have added this command to crontab, and the feeds therefore refresh every 30 minutes for me.      # To Manually refresh  newsboat -r  # To add to crontab  crontab -l | { cat; echo \"*/30 0 0 0 0 newsboat -r\"; } | crontab -      Newsboat has vim bindings for navigating through the posts and blogs, although the key bindings can be configured as per one’s preferences. To get a comprehensive deep dive into some of the features, would recommend going through the well maintained official documentation.I3wmI remember being awestruck when I saw my friend Rohit working on his system. The speed at which he was switching between applications, the core idea of not having to use one’s mouse at all while coding unless absolutely necessary was on display. I asked him what OS he was using, and he smirked telling me that it wasn’t a different OS, just a different window manager called i3. I had decided right then and there to try it out, and have not looked back ever since. Traditional window managers, on most desktop environments are floating managers, where each window needs to be resized appropriately. The workspaces also need to be manually set everytime one wants to separate aspects of development. It is not much of a problem, except when using a tiling manager, all of this is handled automatically. There are many a tiling window managers, and i3 is just one of the many alternatives out there. It is my preference, owing to the configurability and the active community online, which comes around to my rescue, everytime I get stuck with something. It should be noted that using i3 out of the box, is not that great a idea. The true power can be only realized, once one tunes it to adjust it for own’s needs.      An example of the way i3 tiles applications  For example, I have separated my workspaces into 6 categories (web, code, terminal, documents, calls, media) and by default the applications will always open in their respective workspaces, irrespective of where I give the command for opening them. Of course, one can easily move the applications around, according to immediate need. However this default action helps me in switching between my working environments with incredible speed. I have key bindings for virtually anything and everything that I do on a daily basis, and thus overall, the experience of using my system is very pleasant. In addition to the extensive flexibility that i3 provides, it is minimalisitc, and hardly uses up resources. If I haven’t sold it enough already, here are some pictures of i3 setups from unixporn, to inspire more people to use it.      Just start using it already  RangerI must have made it obvious by now, that I favour the terminal heavily. So when I came across a tree file manager completely based on the terminal, I had to switch to it. At the very onset, it might seem futile to push for something of this sort, since GUI file managers like nautilus have no problems whatsover. However the advantage of using ranger is that, it can do everything that a GUI manager can do, but in addition to that, has previews, which enables one to look at a snapshot of the contents of a file, before opening it. The vim bindings, configurability, speed brought about by ranger is incomparable. It is one of those tools that one just has to try before appreciating the beauty of it.      A sample of a preview of a video on ranger  An overextended epilogueAlmost everything that I have recommended above, becomes valuable only if one can touch type and ideally also is acquainted with vim bindings. I recommend keybr for learning to touch type, monkeytype for testing your typing speeds reliably, and vimgenius for pratising your vim bindings. And again, this is more of a preference, but browsers don’t really come better than Brave , which uses a Chromium engine called Blink at its core. The main advantage of using it is the complete control it provides over ads, cross-site cookies and trackers across websites. Besides the privacy benefits, it also has a great rewards program given out in form of their natively developed cyptocurrency called BAT (Basic Attention Token priced at $1.18 at the time of writing the post) and provides a local wallet to store them. And yeah, it also consumes way less resources compared to Chrome."
  },
  
  {
    "title": "Using Gardner's Multiple Intelligence Theory to gauge aptitude of children",
    "url": "/posts/GardnerTheory/",
    "categories": "",
    "tags": "technical, projects",
    "date": "2020-03-10 00:00:00 +0100",
    





    
    "snippet": "Howard Gardner in 1983 came up with a theory to challenge the traditional view on Intelligence. According to him, cognitive abilities of an individual does not solely constitue intelligence. He cla...",
    "content": "Howard Gardner in 1983 came up with a theory to challenge the traditional view on Intelligence. According to him, cognitive abilities of an individual does not solely constitue intelligence. He classified Intelligence into 8 categories viz. Linguistic (Word Smart), Logical/Mathematical (Math Smart), Spatial (Picture Smart), Bodily-Kinesthetic (Body Smart), Musical (Music Smart), Interpersonal (People Smart), Intrapersonal (Self Smart), and Naturalist (Nature Smart). His theory suggested that people could be strong in multiple of these categories and their inclination is more based on their experiences rather than genetics.Problem StatementThe Ministry of Culture in 2018 released a problem statement stating that they wanted to gauge they aptitude of children using Gardner’s theory of Intelligence. In India, children are groomed to follow traditional professions of Engineering, Medical Science or Law, often against their own wishes. By identifying the inclination of a child towards a particular Intelligence type, the Ministry wanted to urge parents to groom their children in disciplines where they were more likely to succeed at.A game to identify the aptitude of a childThe tests to identify the aptitude of individuals involves long questionnaires with written and spoken psychological evaluations. Although these methods do pay dividends and have their advantages, they have not worked well with children owing to their low attention spans. We devised a game that consists of many mini-games each of which tests the inclination of the player towards an intelligence type. The format of the game is designed such that the target group to play the game is completely oblivious of the fact that they are being tested. All the correpsonding code can be found in this repositoryThe game has a simple back story, wherein the space ship of the player crashes into a planet. Due to the destruction caused the player is taken into custody. The prison guard however is unhappy about the unfortunate situation and keeps coming up with suggestions for the player to win over the king and return to his/her planet. Each of the suggestions are back stories for mini-games that the player ends up playing.      Video showing the backstory when the game starts  Selection of the mini gamesThe mini-games that the user is required to play to progress are not played in sequence. Every user might get to experience different games in a different order in every session that they play. Children are likely to get bored if the mini-game that they are playing does not interest them or if they are really bad at it. Therefore the score shown on the front end in these mini-games have nothing to do with the final intelligence score that the player will get. The amount of time spent on the game is taken into context along with the performance of the game to get the score in each of the mini games.In addition to that fact, most games require an amalgamation of intelligences for the player to do exceedingly well. Thus, each game has been analyzed by experts in the Gardner’s theory and has been given a weightage across the intelligence types. A sample of how each game is characterized is given below.   &lt;Game ID = \"7\" name = \"spacewars\" levels = \"3\" &gt;      &lt;Parameter picture =\"0.1\"&gt;&lt;/Parameter&gt;      &lt;Parameter music = \"0\"&gt;&lt;/Parameter&gt;      &lt;Parameter word = \"0\"&gt;&lt;/Parameter&gt;      &lt;Parameter nature = \"0\"&gt;&lt;/Parameter&gt;      &lt;Parameter body = \"0.8\"&gt;&lt;/Parameter&gt;      &lt;Parameter people = \"0\"&gt;&lt;/Parameter&gt;      &lt;Parameter self = \"0\"&gt;&lt;/Parameter&gt;      &lt;Parameter logic = \"0.1\"&gt;&lt;/Parameter&gt;   &lt;/Game&gt;Based on the holistic score achieved by the player in these mini-games, suggestions are provided to play games that require intelligence types in which the player is doing better than the rest. The suggestions run out, and the game ends, if 2 things are achieved  Each intelligence type has been tested a mininum number of times.  All the mini-games at all levels have been played in which the player has had the best performance.A mini-game typically does not last more than 2 - 3 minutes in order to retain the interest of the kid playing it. After every mini-game ends, the next game is selected heuristically keeping in mind the aforementioned goals.      Transitions between mini-games  The heuristic score for each game is calculated based on it’s requirement of each intelligence type, and the current overall game situation. The score helps us ascertain whether we should exploit (choose games with intelligence types, where we are doing well) or explore (choose games with intelligence types, which we have not yet seen).  Driver function to get a heuristic score for each of mini-games that has not been completed yet. (Game considered complete when all of the levels are exhausted for that game).    heur_scores = {g: self.get_game_probability(g, self.levels_done[g-1] + 1)                  for g in games_left}        Inside the function, we get the requirements of each category of intelligence for the given level.    _, _, game_state = self.get_game_info(game_id, level)        We assume that the player will get perfect score in the game, if selected. state_with_game tracks the change in the number of games played across intelligence types, while score_with_game tracks the change in score achieved by player across categories.      state_with_game = {}  score_with_game = {}  # assume perfect score acheived for the current candidate game  for key, game_value in game_state.items():      state_with_game[key] = self.curr_state.get(key, 0) + game_value      score_with_game[key] = self.curr_score.get(key, 0) + game_value        We get the category which can be exploited (where performance has been the best) and the one which needs exploration (which has been played least).      for key, curr_value in score_with_game.items():      # Find ratio of score achieved to max score for all categories      if state_with_game[key] == 0:          score_percentages[key] = 0      else:          score_percentages[key] = curr_value/state_with_game[key]      # Find ratio of completion for each category      state_percentages[key] = state_with_game[key]/self.max_states[key]  # Get the category where player has acheived highest scores  exploit_cateogory = max(score_percentages, key=score_percentages.get)  # Get the category, which has been played the least  explore_category = min(state_percentages, key=state_percentages.get)        upper_t is used to measure the requirement to finish all the games in which we are doing well, and lower_t is used to measure the requirment to explore all games. If the game in question, does not have the category that requires exploration, then the heuristic score only gets the discounted exploitation score.      # Exploit: choose a game where we are already doing well  exploit_heur = self.upper_t - state_percentages[exploit_category]  # Explore: choose a game with a category, we have not tried enough  explore_heur = self.lower_t - state_percentages[explore_category]  # If the game we are going to choose, does not have the category  # we want to explore, only add the exploit heuristic  if game_state[explore_category] == 0:      return self.exploit_t*exploit_heur        The final heuristic score is calculated using a weighted sum of the exploitation and exploration scores, with the weights being controlled by the constants exploit_t and explore_t.      # Both exploitation and exploration are valid strategies, in case  # both values are positive, since there is room for both  if exploit_heur &gt; 0 and exploit_heur &gt; 0:      return self.explore_t*explore_heur + self.exploit_t*exploit_heur  elif exploit_heur &lt; 0 and explore_heur &gt; 0:      # We have exploited enough, hence values have crossed even our      # thresholds. So we have to start exploring all categories, in      # order to reach our goal.      return self.explore_t*explore_heur  elif explore_heur &lt; 0 and exploit_heur &gt; 0:      # We have explored all categories, thus now it is time to start      # finding out which of all the categories are we best at.      return self.exploit_t*exploit_heur        The game with the highest heuristic score is chosen.      # Choose the game with the highest heuristic score  game_chosen = max(heur_scores, key=heur_scores.get)      Once the game ends, the user is redirected to his/her home profile page, where the evaluation across intelligence types is shown. As mentioned before, an individual could be strong in multiple aspects, hence the scores achieved across the categories do not amount to 100. The most common professions associated with each intelligence type are listed, with the category in which the user had the best performance, in focus. The evaluation keeps getting updated, after every session is played.Profile Evaluation Page      Profile Page  Logical-mathematicalOften mistaken for the only form of intelligence possible, people who belong to this category recognize patterns quickly and enjoy working on solving reasoning problems. There is no dearth of logical reasoning games - Sudoku, Chess, Checkers etc have existed for long. To identify this intelligence type, we made the popular Connect 4 game and tuned it to have 3 levels of difficulty.      Connect 4  Visual-SpatialThis category intersects with the logical intelligence types, since it also constitues recoginizing patterns. However, the pattern recognition is limited to pictures, maps or charts. Another parameter to judge this intelligence type is the ability of an individual to visualize information. We designed only 1 game that was specifically dedicated to identifying this intelligence type viz. Sliding Puzzle, where the player has to rearrange a jumbled picture into the original frame. However many of the other mini-games require visual intelligence to succeed as well.      Sliding Tiles  InterpersonalThe ability to quickly understand the emotions of other people and consequently react adequately is referred to as Interpersonal intelligence. In the current version of the project, there aren’t adequate games to judge this intelligence type. We have reverted to questionnaires, however to garb the boring questions, the back story provided is that the player is working with the court philosopher to better manage the court proceedings.      Snapshot of the Interpersonal Questionnare  Verbal-LinguisticThe ability to comprehend language and convey their thoughts eloquently in both written and spoken forms. People with linguistic intelligence can learn new languages quicker than others and tend to have a higher capacity to memorize information. We use 2 traditional word games at various difficulty levels to test this intelligence type.  Bulls &amp; Cows: The hidden 4 letter word needs to identifed in minimum number of guesses. After each guess, the player is given 2 numbers viz. the bull score which implies the number of letters that were in the correct position, and the cow score which implies the number of letters which were part of the hidden word but are in incorrect positions.      Bulls &amp; Cows    Hangman: The hidden word needs to identified, by guessing the letter of the word one by one. The player is allowed 5 wrong guesses.      Hangman  IntrapersonalThe ability to be aware of one’s longings, fears, limits and to utilize the data successfully. This kind of insight can assist an individual to understand their life objectives and how to accomplish them. Identifying this intelligence type is often redundant, since people who have good intrapersonal understanding are by definition aware of their strengths and weaknesses.Similar to Interpersonal, we use a standard questionnaire for testing this intelligence type and mask it with a back story.      Questionnare for Self Smart Quiz  MusicalMusical intelligence refers to the skill in the performance, composition, and appreciation of musical patterns. We test this intelligence types by providing a spin to the traditional memory game, where instead of matching pictures on hidden cells. The user has to identify cells emanating similar sounds. The game called Memory music has 3 levels of difficulty.      Memory Music  Bodily-kinestheticPeople who are strong in this area tend to have excellent hand-eye coordination and dexterity. If identified early, children can grow up to have superior control over their body movements, become highly skilled at various sports and dancing forms. We identify this intelligence type, via 2 games      Flappy Bird: The traditional game is played with a twist. Instead of using the keyboard to keep the bird from flying, the player’s hand is tracked, and the hand movement mirrors the way the bird moves across the screen.        Space Wars: Hand-eye coordination is tested, since multiple allies and enemies are confined to a small window space, and the user has to eliminate as many enemey ships possible accurately without hurting their own allies.        Space Wars  NaturalisticIndividuals who are more in tune with the environment, and are interested in knowing more about the different flora and fauna species around the world. We made multiple games to identify this intelligence type  Identify animals from their nature calls  Identify different species of plants and animals.      Animal Kingdom Knowledge    Exploring Game, where the player roams around the map. There are options to plant saplings, water trees, cut the extra grass, move trash to the dustbin. The player however is not given any aim for this game. How, the player spends their time in this game, indicates their empathy towards nature. In a limited time span, trying the different variety of activities is given a higher score, compared to repeating a single activity and increasing the displayed score on the game page.User HandlingSqlite is used to save all the user and game related information. To ease quering of the database Flask-SqlAlchemy was used. The login, registration and user handling at this point is naive, as the focus of the prototype is the identification of the intelligence types.      Login Page  Option to play only Mini-GamesThere is a page on the flask server to facilitate playing the mini-games outside of the story mode. It should be noted that the scores achieved while playing games in this manner are not counted towards the overall profile evaluation of the player.      Play Mini-Games without the story mode  Shortcomings  All the games as of now are spawned in separate windows and not on the browser itself. In an ideal scenario the user should be able to play all the games without leaving the browser window. Contributions on that front are welcome.  Progress in a game is lost, if the player quits in the middle of a session. Therefore adding options to save/load previous sessions are needed.  Better RPG Games are needed to test Inter-personal intelligence. The ability to make allies, being empathetic towards violence around them could be used as markers for good performance.Project BackgroundThe Smart India Hackathon (SIH) is organised annually by the Government of India. Problem statements are released by various private and government enterprises. Participating teams can choose to submit a proposal for any of the problem statements. If the proposal is selected, the teams have to reach a venue and build the proposed solution in a 36 hour code sprint. The submissions are judged by a panel elected by the same enterprise that released the problem statement. As mentioned earlier, the aforementioned problem statement was released by the Ministry of Culture in 2018, and this project was our teams’ official submission towards the same, for which we were adjudged winners.Team Members  Arshia Anand  Damodar Nayak  Daniel Savio  Mohammad Aadil  Sushant Kumar  Yash Sarrof"
  },
  
  {
    "title": "Some of my favourite mini-side projects",
    "url": "/posts/AllMiniSideProjects/",
    "categories": "",
    "tags": "technical, projects",
    "date": "2019-06-18 00:00:00 +0200",
    





    
    "snippet": "Like any other beginner to the field, I have experimented with different technologies, often times collaborating with others to learn more about a particular topic that spurred interest. That over ...",
    "content": "Like any other beginner to the field, I have experimented with different technologies, often times collaborating with others to learn more about a particular topic that spurred interest. That over time, has led to me making weird, unsurprisingly unsustainable projects in short bursts. However each of these projects acted as a gateway for me into a new domain, and therefore are memorable for me.Earthquake Detection SystemThe theme for Microsoft’s Code Fun Do++ for the year 2018, was Earthquakes. Each team was to make something that would help in the overall management of natural disasters.  Tanmay and  Chhayank were my teammates for the event. We had noticed that most other teams were working on either the prediction or management of earthquakes (making prototypes of platforms to crowdfund resources to distribute for alleviating the distress in affected areas). While brainstorming, we got to know that it takes considerable amount of time (as much as 20 minutes) for the disaster management authorities to be notified about any natural disaster that takes place. We instantly started working on something to reduce this time gap. Our final solution was extremely minimalistic and simple. We wanted to use live feeds provided by the government owned surveilance tools (cameras/satelite data) and periodically scan the frames (say every 30 seconds). For every camera, we would identify a static background object like a lamp post, or a zebra crossing or a street signal. These static background objects can be occluded owing to some foreground objects (humans, cars, bikes …) appearing. However, the idea was that we would have multiple such live feeds that were being scanned, and the chance of the occlusions appearing in sync in all of these cameras was virtually nill. In case of an earthquake, the shaking of the ground, the camera would result in all of the cameras reporting a change in the position of these static objects, and in case of this, we would immediately ping the concerned authorities.Pseudo code for detection of discrepancy using lanesThe nmber of common lines between 2 consecutive frames is high but as the earthquake starts, due to the shifting of the lines, the average keeps dropping due to the constant shaking of everything, and a alert is issuedThe assumptions made were that, the color for demarkating lanes are generally white. Although, it wasn’t a hard assumption, since there was a provision to override the color for cities with different coloring schemes.Here frame refers to the current frame being examined, and upper_white &amp; lower_white controls the color range to identify car lanes. The loop given here is run periodically (every 30 seconds). ave_list refers to the list that contains the historical ground truth against which the current extraction is compared.# Flag to show frame statusframe_status = POSITIVE# detecting white lanes of a road ....hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)mask = cv2.inRange(hsv, lower_white, upper_white)# Finding the edges on the roadedges = cv2.Canny(mask, 75, 150)edges = cv2.GaussianBlur(edges, (5, 5), 0)try:    # Detect white lines in the frame     lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=20)    ave_list.append(lines)    # If lines are not detected for the current frame, return the same     if len(lines) &gt; 0:        # Function to match the current extraction with the historical extraction         same = check_lines(ave_list)\t\t# If the number of similar lines is lower than a threshold\t\t# however the number of lines in the frame is high\t\t# the status of the frame is set to show negative        if same &lt; same_thresh and len(lines) &gt; line_thresh:        \tframe_status = NEGATIVE        else:        \t# If the status continues to be positive, the historical data is scrapped        \t# to keep the current extraction as the basis for future comparisons        \tave_list = [lines]    else:    \tframe_status = POSITIVEexcept Exception as e:    frame_status = NEGATIVEAnother experimental feature of the prototype we had created, was to predict earthquakes based on a long standing theory, that animals can often times sense an impending disaster, and start getting fidgety hours before the actual calamity. As this hadn’t been proved yet, we wanted an accompanying analyzer along with our primary earthquake detection, which would capture animal movements in a zoo/ safari. We tracked their usual entry and exit from frames and stored this information into a database. Our goal was to collate this data with the times at which the earthquake was detected, and in case a correlation was found, animal movement could be permanently be used as a measure to predict occurrences of an earthquake.Pseudo code for recording entry and exit of animalsAn assumption made was that, the live feeds that were to be provided for the zoos, to record animal movements would track a single animal per camera. Thus, identifying the animal in every iteration is not required, and the detection of any large moving object can be considered equivalent to the presence of an animal in the frame. Each frame is processed as follows# By default, every frame is considered be to emptyoccupied = False# resize the frame, convert it to grayscale, and blur it frame = imutils.resize(frame, width=500)gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (21, 21), 0)# if the first frame is None, initialize itif first_frame is None:    first_frame = gray    continue# compute the absolute difference between the current and first frameframeDelta = cv2.absdiff(first_frame, gray)thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]# dilate the thresholded image to fill in holesthresh = cv2.dilate(thresh, None, iterations=2)# find contours on thresholded imagecnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]# loop over the contoursfor c in cnts:    if cv2.contourArea(c) &lt; min_area:        continue    # If contour is large, something is occupying the frame\t# Therefore, record entry if previous frame was empty     if prev_status is False:        occupied = True    \tsave_entry_time(datetime.datetime.now(tz=timezone.utc))        break# if none of the contours were big enough, current frame is emptyif occupied is False and prev_status is True:            \tsave_exit_time(datetime.datetime.now(tz=timezone.utc))# previous status of frames updatedprev_status = occupiedSudoku SolverI had a long weekend somewhere around my 4th Semester, and was feeling bored. I had just completed the CS - 231n course, and wanted to start training models to solve for various tasks. I had also learnt the basics of image processing and wanted to work on something that required the same. An automated sudoku solver happened to be the first suggestion that I came across online while searching for potential ideas that I could work on that required both image processing and neural networks. In either case, it turned out to be a fun weekend, as I implemented it over the course of 3 days. The pipeline of the project was as follows:  Identifying each individual square in a sudoku puzzle: The hard assumptions made for the input is that, the sudoku image has to be non-skewed, and the grid lines in the Sudoku have to be clear. If the input satisfies these conditions, puzzles are extracted from the entire sudoku image. In case, the input can’t be processed, there is a provision to input the puzzle manually. The pseudo code for extraction -# Convert to gray scalegray = cv2.cvtColor(sudoku_image, cv2.COLOR_BGR2GRAY)# Blur to be able to detect all the edges in the image clearlyblur_gray = cv2.GaussianBlur(gray, (5, 5), 0)# Use the Canny edge detector to get the edges on the screenedges = cv2.Canny(blur_gray, 30, 90, apertureSize=3)# Find lines greater than 160 pixels in lengthlines = cv2.HoughLines(edges, 1, np.pi/180, 160, 0, 0)# sort the lines based on the value of Perpendicularlines = sorted(lines, key=lambda line: line[0][0])# Filter the lines to only contain horizontal and vertical linesh_list, v_list = filter_lines_based_on_angle(lines, puzzle)# Find the intersection between horizontal and vertical linespoints = get_intersection(h_list, v_list)# If number of intersection points detected is not 100# the user is asked to enter the board manuallyif len(points) == 100:\t# After identifying points, square images are formed from the points \tpuzzle = extract_individual_digits(image, points)else:\tpuzzle = enter_board_manually()  Extracting square images from the points in the image:def_indidual_digits(image, points)    board = []    for row in range(9):        row_board = []        # goes till 9, since last diagonal not needed        for column in range(9):            x1, y1 = [int(i)+3 for i in points[row*10 + column]]            # coordinates of the diagonals of the rectangle            x2, y2 = [int(i)-3 for i in points[row*10 + column + 11]]            # area of 1 box, each box has 1 digit            temp_img = img[y1:y2, x1:x2]            if(temp.size != 0):                # to maintain uniformity with the model's requirements                cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)            \t# Predict the digit found using a trained digit recognizer model                row_board.append(predict_image(model, temp_img))        board.append(row_board)    return board  Identifying the digits in each square image: I had initially trained a shallow neural net (Conv2d layer, Max Pool layer followed by a couple of fully connected layers) on the MNIST dataset for identifying the digits from the individual squares. I had added a few random images with empty squares and added them with the tag -1 to the MNIST dataset for being able to predict on empty squares as well. However later, I changed the pipeline to a Logistic Regression model trained on the original dataset and a simple check before prediction to check if the input image contained a digit. The pseudo code -# model_path is the file, from where the trained model is loadedmodel = LogisticRegression()model.load_state_dict(torch.load(model_path))# Preprocess and flatten the image for testingimage_tensor = torch.flatten(test_transforms(image))# If the image is non empty, then the digit is identified by the modelif torch.sum(image_tensor) &gt; non_empty_thresh:\toutput_arr = model(image_tensor).detach().numpy()    digit_identified = np.argmax(output_arr)else:\t# The square does not contain any number, return -1 to denote the same\tdigit_identified = -1  Solve the sudoku, based on the information gathered, and display the solution: I simply used backtracking to solve the sudoku. There are innumerable posts explaining backtracking (refer), hence am not providing the pseudo code here.Alpha Beta Pruning to make a game engineThe perception that I had about AI when I was in high school was almost entirely built up by the hype around how no chess grandmaster was capable of beating a chess engine (that and JARVIS in Ironman xD ). It was inevitable that the first thing I started reading about, after learning the fundamentals of programming was how to make a chess engine. Today, I appreciate the intricacies of AI and my understanding of the field has evolved since, but when I read about alpha-beta pruning, the elegance of it all to make engines suitable for games with perfect information, removed some of the mystique behind something I had been utterly fascinated by. The game that I chose to make was Tic-Tac-Toe, since I did not want to spend time, on coding up the graphics of chess, but was just interested in making alpha-beta pruning work. It took me a weekend’s time to make this game, but till date, whenever I am bored, I just fire up the game and start playing. There are a plethora of posts explaining alpha-beta pruning (refer), if anyone starting out wants to explore it."
  },
  
  {
    "title": "The Smart India Hackathon Experience",
    "url": "/posts/SIHExperience/",
    "categories": "",
    "tags": "reflections",
    "date": "2019-03-11 00:00:00 +0100",
    





    
    "snippet": "On some night in January 2019, me and my roommates Aadil &amp; Sushant were discussing how the hectic schedule in our college, and all our other commitments for improving our profiles, virtually me...",
    "content": "On some night in January 2019, me and my roommates Aadil &amp; Sushant were discussing how the hectic schedule in our college, and all our other commitments for improving our profiles, virtually meant that we weren’t spending time with each other at all. We all agreed that the only way to actually hang out more was to work on the same project. We started looking out for options, and after carefully weighing in our alternatives, we decided on participating in the Smart India Hackathon. The decision was guided by the kind of projects on offer, and a time frame that would have allowed to us to maintain our other commitments, manage classes and assignments and still give about 20 hours per week to this project. A blatant flaw with this plan, that we realized much later was that this competition was one which a lot of people from all across the country looked forward to, with some even spending more than a year preparing for it. For us, the first round of the competition was about 20 days away, with the final round in about 2 months time. There was another catch, each team had to have a minimum of 6 members, and after a mix of deliberation &amp; persuation over the next week, we managed to form a team, adding Daniel, Arshia and Damodar to our team, all of whom were our batchmates.The 1st roundThe competition portal had released the problem statements somewhere around July 2018, and the format of the competition was that each such problem statement had a solitary winners prize. Each category’s winner would be selected out of the 4 teams, that would be selected after the 1st round. However, in case a problem statement did not have impressive enough submissions, that problem statement would be scrapped. Each team was allowed to submit proposals to 3 unique problem statements. The challenge was to select a problem that had a high probability of being chosen by others as well, besides thinking about a viable solution to the same. The initial discussions between our team did not not bear any fruit owing to a sense of disharmony between our team members. Some of the members were meeting with each other for the first time, and were not happy with the inequity in the work that was being done. At that time, we had to make a decision we had been putting off till now - selecting a team captain. Again, since the team captain would have to take care of a lot of the administrative side of things without getting any extra credit in the end result, nobody volunteered to take the part. Eventually I agreed to take up the role after a blind vote, where we chose the people we wanted to see become the captain. I distinctly remember the first discussion that we had post that vote. We all were standing in a circle outside our food court, faces looking grim in the team. We all pledged to dedicate a substantial amount of our time to this project, agreeing that taking it lightly and just continuing to act on whims would lead to nothing but a waste of time. Over the next week we all warmed up to each other, working till late at night on somedays to brainstorm over ideas. We eventually narrowed down 2 problems, and submitted proposals for them. The 1st problem statement was to build an application to find a nearby parking spot and the 2nd one was to build a game based on Gardner’s theory of Intelligence to gauge the aptitude of a child, and recommend possible career options he/she might be suited for. After submitting our proposals on the final day, we felt that our solution for parking spot finder was the one that had a higher chance of being picked up if at all.A surprise and some second thoughtsAfter having made the submission, the next few days were quiet as our sessional exams had started. Once our exams got over, we went about inquiring about in all the departments of our college about the other teams that had participated from our college. We found out, that 4 more teams had participated just like us, and we also found out that our college campus was one of the places where the final round was going to be held. After another week, we eventually did get the results of the 1st round. We were the only team that actually did make it through from our college in the 2nd round, but to our surprise, we actually were selected for the problem statement that we were less confident in. But to our dismay the center for participation for the 2nd round for our problem statement specifically, was in Jaipur about 2000 Km away from our college campus in Manipal. Since the 2nd round was only a fortnight away, we seriously thought of quitting the event, since booking air tickets at such a late juncture would have cost us more than our combined prize money. The other option was to travel via train, which would involve 5 days of travel time to and from, with a 2 day stay in the Jaipur center for the competition. The issue here was that our college did not ususally provide any kind of support for students travelling to other venues for participation, which implied that the labs and assignmemnts we would miss during that period wouldn’t be rescheduled for us and our grades would get heavily affected. And in all of this, we were not even thinking about the extra time and effort that would be required to get a prototype for our proposal ready by the day of the final competition. We again had a long meeting amongst ourselves, and eventually agreed that such an experience of being in the final four of a national level competition should not be missed upon. We charted out the plans, set up timelines, completed a lot of the administrative work which included creating tshirts for every member in the team, getting permissions from multiple departments in the college, getting printed banners and standees all of which was mandatory. The only bit of support we got in the entire process was from the guy we went to for making the standees, who promised not charging us, if we won the competition. We also had to select a team name. Having seen Mr. Robot recently and in a bid to get over the formalities, we named ourselves PySociety. By this time, the team had really bonded and spirits were generally high, when we were working together. So, the actual goal of getting to hang out more with friends was somewhere being accomplished.The journeyAbout 60 % of the work had been completed, when we were about to board the train from Manipal. A long journey lay in front of us, the first train would take us to Mumbai, where we would have to wait for about 9 hours, before boarding the train for Jaipur. We had kept local copies of documentations, had taken enough power backup to be able to work without internet during the train journey. We arrived at Mumbai, the Chattrapati Sivaji terminal station somewhere around 3 in the night. Aadil in our team hailed from Navi Mumbai, so after the mandatory walk along Marine Drive where the sea front just looks majestic, we headed towards his home. Aadil’s Mom had made biriyani for us and to this day, I haven’t had biriyani that has tasted as good. We left for Jaipur in a jovial mood that persisted throughout the remaining journey. At the time of our arrival at the Jaipur center around 4 PM on the 1st March, 75% of the work was completed. We were satisfied with the progress, but we weren’t able to get any more work done on that day owing to a overdrawn opening ceremony in the evening. As the competition was due to start at 6 in the morning the next day we called in an early night. 5 of us guys were given one apartment by the organizers, and all the girls from the all the teams were staying in another block. The block in which we were staying was under maintenance and was isolated from the rest of the campus, giving the entire place an eerie look. In our apartment though, there were only 2 beds which could take no more than 3 people. We were given 2 extra mattresses which we kept in the living room. Me and Sushant were sleeping there peacefully and around 5 in the morning we heard vehement knocking on the door. Altough none of us had faith in the existence of spectral beings, the sudden outburst in such an isolated environment terrified us, and we literally started screaming in fear. Everybody in the apartment woke up, we opened the door and we saw no one. We stood outside, confused about the entire ordeal, and just then someone in the corridor shouted that it was just a local guard who liked to play such pranks.After that uncanny start, we got ready and made our way to the hall. There were to be 3 rounds of presentations over the course of the next 36 hours. There was little chance to fraternize with other teams since a considerable amount of work was still left. We got down to it and started working. About 14 hours in, we had our 1st round of presentation in front of the judging panel who gave us their feedback. We estimated that the with the progress that we had made, incorporating the feedback along with completing the rest of the project would take us around 10 more hours, and we would have time to spare before the final presentation. The competition rules mandated the presence of at least 2 members at the table assigned to us. We decided to give Sushant and Arshia who were in charge of integrating all the modules the entire night’s rest, since they would primarily be in action the next day. Aadil and Daniel decided to take a nap in the competition hall itself, since they thought getting up on cue would be easier. Me and Damodar tried to do our best, but by 7 in the morning, we were tired and were virtually sleeping with our eyes open when Sushant and Arshia came back and took charge. However we followed Aadil’s lead, and decided to take rest in the competition hall itself. Around 10, we were all semi-fresh when it was time for our 2nd round of presentation.Panic and HysteriaThe judges in the 2nd round did not give us any suggestions, nor did they make any comments about anything they were seeing. We continued with our work, after their inspection. Around 1 o clock, we were pretty satisfied with our work, and all the modules were individually properly unit tested. The integrated application was working well, albeit enough tests had not been done on them. Seeing the progress, Aadil and Daniel decided to eventually go back to the rooms to get a power nap to catch up on sleep. Meanwhile, me and Damodar started working on improving the graphics and overall look of the application. Around 2, I sensed a bit of panic in Sushant’s eyes, he had been testing the entire application end to end for some time now. On being asked, he said that the application was randomly crashing in 1 out of 5 test runs and the logs weren’t clear enough to debug, what was causing the crash. We all left what we were doing to get to the bottom of this. Only Sushant and Arshia were experts in Flask, the web framework of our choice for the application. Although the others could support by continuously testing end to end, and trying to find the pattern of the crashes, it was up to the 2 to fix the bug. From having hours to spare with a complete application, to not having a stable application an hour before submission was due, anxiety was creeping in.The Education minister of our country at that time, Mr. Prakash Javedkar had come to our center and was meeting with all the participants. There were 32 teams at the venue, 4 each for the 8 problem statements that had been assigned this center. The orientation of the the tables was such that, there were 28 tables around in a circle, and there were 4 tables in the middle file. It would have been a huge honour to meet with a man of his calibre. We were seated in the middle file, but owing to the orientation we were not in his line of vision, and the team escorting Mr Javedkar could not have insisted him to turn around to the teams he had skipped, and thus he ended up meeting the 28 teams seated in the circle. We got back to attending to our crisis. At this point, we had identified that the application was only crashing while transitioning between games. There were 2 kinds of transitions, from the end of the game to the story scene, and from the story scene to launch a new game. We were working on narrowing down the issue, with about half an hour to go, when the team captains were asked to step out and come near the podium. It was frustrating to be interrupted at this juncture, but I had no option but to follow the rules. I was sitting there along with 31 other people, when suddenly the video in front of us started, and the prime minister of our country Mr. Narendra Modi was with us on video call. An address from him was least expected. For a moment, I forgot about the issue at hand and intently listened to what he had to say, and his vision for backing this event.I came back after listening to his rousing speech, with about 10 minutes to go before our final submission. Inspired from Mr Modi, I addressed my team appreciating each one of them for having worked so hard all these days, and to look at the positive side of the mess we were in, hoping that it would lift their spirits. All of them were listening keenly, when suddenly Damodar could not help himself and started laughing hysterically. Apparently the rest of the team had identified that the crash only occurred when the scene transitioned from the story mode to launch the 3rd level of the same game type. The scores from the 1st level were being passed on, instead of the 2nd one, and owing to a mismatch in the scoring patterns in the 3 levels, the crash was occuring. They had already made the change and tested the run a few times successfully. To this day, all 5 of them laugh at me for that “motivating speech”.The Winning momentOnce again in great spirits we all headed for the final presentation, this time in a separate hall with extra panelists who had hitherto not been present. It all went smoothly, and we came back to the hall to interact with the other participants. One of the teams said that they had been working on the project for over a year, having had multiple interactions from the Ministry of Culture who were the guiding force behind the problem statement we were a part of. Another team turned out to be former champions of the event, having won the inaugral edition of the hackathon back in 2017. We discussed our proposals with each of them, fraternized with the other teams who had were submitting solutions for other problem statements. It was a lively atmosphere, and we almost did not realize that it was time for the winners to be announced. Every team assembled back at their tables. We got into one final huddle and agreed that if we win, we will get up and go and get our awards with absolute non-chalance à la MS Dhoni (for context he is always calm, no matter the result). Our problem statement was supposed to be the last one to have the winners proclaimed. The guy who was announcing the winners said ‘P-Y’, and we all knew what the next word was gonna be, and before he could finish saying “society” we were ready to get on the stage to collect our award. Everyone was so full of emotion, but we tried our level best to not show a shred of it. We all got out of the hall, took some pictures to mark the occasion. One would expect people to celebrate after such a win, but we all looked at each other and almost telephatically agreed that we all needed sleep before the long train journey back home. Two and a half days later, we arrived back at Manipal around 8:15 in the morning. I rushed directly to the my lab from the station, as it was to start at 8:30 in the morning. I eventually reached about 5 minutes late, but the teacher was kind enough to pardon my shabby appearance (long train journeys tends to do that to you) and late entry. Tanmay, one of my best buddies in class was gesturing, intending to know what had happened at the hackathon. I simply smiled back at him and showed him the victory sign."
  },
  
  {
    "title": "React Native, thou aren't as complex as thou seem",
    "url": "/posts/ReactNative/",
    "categories": "",
    "tags": "technical",
    "date": "2018-12-14 00:00:00 +0100",
    





    
    "snippet": "During my intership at Optimize IT Systems, my job entailed creating a basic curriculum for React Native, since the company wanted to migrate towards developing mobile applications with it. This po...",
    "content": "During my intership at Optimize IT Systems, my job entailed creating a basic curriculum for React Native, since the company wanted to migrate towards developing mobile applications with it. This post contains snippets from that curriculum along with a mention of the potential pitfalls that one might fall into as a beginner.Navigation between Screens in React NativeNavigation in React Native is generally enabled through third party modules. Although native node modules to do the same can obviously developed, but the already existing ones work pretty well with practically no downsides. I recommend ‘react-navigation’, This contains a comparative study with regards to the options available.Installing React Navigation can be done in two ways -yarn add react-navigation npm install react-navigationRunning any one of the two, will set up the project to use navigation properly. Changing a package manager to install different third party modules leads to complications in in terms of the consistency in the versions of the different modules installed. The .lock files also get hampered with and often the only way out is to rebuild the entire project, which can be done by running yarn install again, after removing the node-modules subdirectory entirely ( or npm install if that is being used ). Since the node-modules sub-directory can be so easily rebuild, it is generally not pushed while using version control for a project. Yarn on one hand, makes a lock file right at the very onset, and uses that to always install a certain version of the dependencies, while npm will always go for the latest dependencies, and that is where the conflict can occur, since some methods get deprecated, and things that work in one environmental setup, might not work at all in a slightly different environmentTwo most popular Navigatiors available with react-navigation are,  Stack Navigator , 2. Tab NavigatorStack Navigator works as expected like a stack, with pages on being called, being pushed on the stack, and going back results in popping, thus we end up with the previously opened page.// Sample Stack Navigator : \tconst AStack = StackNavigator(\t  {\t    A: {\t      screen: PageA\t    },\t    AA: {\t      screen: PageAA\t    }\t  },\t  {\t    initialRouteName: 'A',\t  }\t);This will enable an environment where in navigation would be possible, however all of these need to be done manually by adding functions or other methods to actually navigate to other page. A possible analogy is the fact that making the navigator object is like knowing the route to a certain place, however we just knowing it, won’t take us there, we explicitly need to go there as well.Unlike the stack navigator which goes deeper into the application, TabNavigator gives us quick access to pages that need to toggled more frequently. Like in the facebook app, we can toggle easily between the notifications, messages and friend requests pages, whilst clicking on some page/post enables for us to go deeper.// Sample Tab Navigator, most of the code given is self explanatory... export default TabNavigator(\t  {\t    AS: {\t      screen: pageA\t    },\t    BS: {\t      screen: pageB\t    },\t    CS: {\t      screen: pageC \t    }\t  },\t  {\t    navigationOptions: ({ navigation }) =&gt; ({\t      tabBarIcon: ({ focused, tintColor }) =&gt; {\t        const { routeName } = navigation.state; \t        let iconName = icons[routeName];\t        let color = (focused) ? '#fff' : '#929292';\t        return &lt;MaterialIcons name={iconName} size={35} color={color} /&gt;;\t      },\t    }),\t    tabBarPosition: 'bottom',\t    animationEnabled: true,\t    tabBarOptions: {\t      showIcon: true,\t      showLabel: false,\t      style: { backgroundColor: '#333' }\t    }\t  },\t  {\t    initialRouteName: 'BS'\t  });For passing of data/objects between 2 screens, we pass the objects using a key-value pair using the navigate function :&lt;Button \tonPress={() =&gt; { \t\tnavigation.navigate(\t\t\t'pageC',\t\t\t\t// the page to go to \t\t\t{ \t\t\t\tkey1 : Data1,\t\t\t\tkey2 : Data2\t\t\t}\t\t\t\t\t\t// data to pass to the page \t\t);  \t}} /&gt;Now these passed values need to be retrieved as well, which can be done by adding the following in the NavigationOptions provided, at the start of the main class of a page :static navigationOptions = ({ navigation }) =&gt; {\tconst { params } = navigation.state; // extract params, now using it requires params.data_objname     \treturn {        \theaderTitle: 'Exercises',        \theaderStyle: {\tbackgroundColor: '#333' },   \t}}Appropriately modules ( third party modules and classes referred ) have to be imported for Navigation to work properly. Also by playing around with styling and by modifying parameters, highly complex navigation techniques replete with animations can be made. A combination of the stack and tab navigators could be used to provide for the framework in case of a very complex workflow.Working with DatabasesA local database is almost a bare-minimum for the making of even the simplest of apps, and like any other technologies, there’s a lot to choose from in terms of what one might want to use in their application, to accomplish local storage. This offers a great overall outlook for the options available out there to implement local database in React-Native apps.Sqlite :It uses a third party module called : ‘react-native-sqlite-storage’, thus same needs to be added by yarnyarn add react-native-sqlite-storage yarn add react-navigation Now, this isn’t enough, since we need to provide some extra information in the android-specific folder, for these databases to work properly.Step 1 : Add to the dependencies section in android/app/build.gradleimplementation project(':react-native-sqlite-storage')Step 2 : Add to the android/settings.gradleinclude ':react-native-sqlite-storage'project(':react-native-sqlite-storage').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-sqlite-storage/src/android'Step 3 : Add to MainApplication.javaimport org.pgsqlite.SQLitePluginPackage // import statement new SQLitePluginPackage() // append to getPackagesAfter following the above steps, the application is now ready to use sqlite-database. The basic syntax of an sqlite execute function is as followsdb.transaction(function(txn) {  \t\ttxn.executeSql(\t    query,                 //Query same as in Sqlite\t\targsToBePassed[],      //Argument to the query \t\tfunction(tx, res) {}   //ToDo with the result  \t\t);});However before it is used, importing openDatabase is necessary, and opening the database is necessary as well. Here though the database being accessed does not necessarily have to be created before hand, if nothing is present,a new albeit empty database is created. We can even manipulate existing databases and is precisely the reason why we would even consider using SQLite plugins, since although it is used across domains in myriad applications, the plugin providedin React Native is slower compared to other alternative, for eg : Realm.import {openDatabase} from 'react-native-sqlite-storage'var db = openDatabase({name : 'name-of-database.db'})Thus for illustration a sample operation of all the operations are being shown below :Query a tabledb.transaction(tx =&gt; {      \t\ttx.executeSql(        \t\t'SELECT * FROM table_user where user_id = ?', [this.search_user_id],        \t\t(tx, results) =&gt; {        \t\t\t// DO SOMETHING WITH THE RESULTS, DISPLAY OR SET VALUES         \t\t}        \t)\t\t}\t);Insert into a tabledb.transaction(tx =&gt; {      \t\ttx.executeSql(        \t\t'INSERT INTO table_user (user_name, user_id) VALUES (?,?)', [this.state.name,this.state.id],        \t\t(tx, results) =&gt; {        \t\t\t// After execution of the insert statement, this function gets executed         \t\t}        \t)\t\t}\t);Create a tabledb.transaction(tx =&gt; {      \t\ttx.executeSql(       \t\t\t'CREATE TABLE IF NOT EXISTS table_user(user_id INTEGER PRIMARY KEY AUTOINCREMENT, user_name VARCHAR(20)'       \t\t)\t\t}\t);Update values in a tabledb.transaction(tx =&gt; {      \t\ttx.executeSql(        \t\t'UPDATE table_user set user_id = ?. user_name = ?', [this.state.id, this.state.name],        \t\t(tx, results) =&gt; {        \t\t\t// After execution of the update statement, this function gets executed         \t\t}        \t)\t\t}\t);Deletedb.transaction(tx =&gt; {      \t\ttx.executeSql(        \t\t'DELETE FROM table_user where user_id = ?', [this.search_user_id],        \t\t(tx, results) =&gt; {        \t\t\t// After delete, this function gets executed         \t\t}        \t)\t\t}\t);A very common error is to not handle the data types appropriately while trying to do any CRUD operation on the database, especially if the state variables are being used to change/update/insert values in the database. For eg: use parseInt(str,10) if the required type is Integer for a particular column in database. A good reference for an IOS-app demo for SQLite is available in this blog post.Realm :Realm is not a database in a very traditional sense, since it cannot be used universally. It has been designed specifically for mobile devices, although it makes up for this disadvantage with substantially faster speed, and it’s simplicity of design.At the very heart of it, it uses JS objects which are dynamically mapped to a full, custom database engine. Extremely complex data models can be modelled using Realm, and is thus is a very popular local database choice for mobile applications.yarn add realm react-native link realm Similar to sqlite, a few additional steps are required to finish the setup1) In the settings.gradle file :include ':realm'project(':realm').projectDir = new File(rootProject.projectDir, '../node_modules/realm/android')2) \tIn the android/app/build.gradle, if gradle version &lt; 3.0, then replace implementation with compile :implementation project(':realm')3) In MainApplication.java :import io.realm.react.RealmReactPackage ; And inside the getPackage() function after MainReactPackage(),new RealmReactPackage() ;Making a SchemaMaking a schema is equivalent to designing the table structure in SQL. However, the functionality provided by Realm, enables making even extremely complex schemas, very easy to design, since overall design is very similar to the object-oriented design.cont Xschema = {\tname : 'X',\tprimaryKey : 'id',\tproperties : {\t\tname : 'string',\t\tid : 'integer',\t\tbirthday : 'date',\t\tachievements : 'achievement[]',\t\tdeath : 'date?',\t}}Here, name : ‘X’ isn’t the property of the network, it is instead serving the purpose of a alias. Thus, every Xschema object will have the same name-alias, for referring. However, the name inside the properties block is a property of each object. There are options galore for datatypes docs. #e can use our own custom datatypes, i.e. we could use embedded schemas in our design, to model extremely difficult situations extremely easily.Appending a [] at the end of some datatype implies that the entity is a list containing those datatypes variables, similarly  a ? implies that using the parameter is optional. Again, there is a lot more options to make better schemas, and a look into the docs might prove worthwhile.Insert a SchemaThe following code block can be used for inserting an entry in some table, in realm referred to as making a object of a schemarealm.write( ()=&gt;  {\trealm.create('X',{\t\tid : parseInt(this.state.id, 10) ,\t\t...\t\t..\t\t.\t}) ; });Thus, after opening the ‘X’ type schema, we create an object of that instance, the reference for which may or may not be saved, as per needs. It must be noted that the realm object needs to be passed, across the screens for it to be accessible everywhere, or the path to the same needs to be passed as props.Query a Schema      Simplest query , return all the objects of a particular schema.    a = realm.objects(‘X’)        Equivalent to a where clause, using filter.    a.filtered(‘condition1 AND condition2’) ;  where a is the collection of all objects of that particular schema, as said beforeDelete a SchemaFor deleting entities, we will need object references of everything we need to delete, thus we use the query function first to get those references, then we call the delete operation. An example -realm.write(() =&gt; {    a = realm.objects('employee').filtered('id =' + parseInt(this.state.id,10))\tif ( a.length &gt; 0 ) {\t\trealm.delete(a) ; \t}});Update a SchemaUpdate is very similar to delete, we simply get the applications, and then we simply update using the object references, just as assignment to any other object is done. An example :realm.write(() =&gt; {    obj = realm.objects('employee').filtered('id =' + parseInt(this.state.id,10)) ;     for (i = 0 ; i &lt; obj.length ; i++ ) {        obj[i].param1 = some_variable ;         ...        ..        .    }\tShared PreferencesShared Preferences in Mobile Application Development refers to the data that is stored in the app itself, for customizing the overall experience of using the application for a user. These are data that persists, even if the the app is closed or the device is stopped. Also these data values can be accessed across the application, as in every page in the app will have access to this data.One of the simplest ways to implement this feature is to use the AsyncStorage functionality provided by react-native as a basic component. Thus since it is not a third party module, no extra setup is required for using this. A disadvantage of Async Storage is that, although the data stored is persistent, it stores everything in an unencrypted form. However, in most use cases, the data stored via Shared preferences isn’t all that sensitive and so is widely used.A few important things to note about Async Storage is the fact that it stores data in key-value pairs [ like a dictionary in Python ], and it can only store value in string format. Thus for any other datatype, the responsibility of conversion handling is passed on to the app itself.All methods in Async Storage return a Promise object ( an object that either has a value or may produce a value in future ), thus, it follows that almost all the times any method of async storage is used, we end up using it within Async-Await blocks. In loose terms, it is a way of handling promise objects in a better fashion, async if prefixed before a function implies that the function will always return a promise, whilst await makes the application wait for the promise result to get resolved. For detailed information on Promise objects link and for Async-Await concepts linkAfter understanding Promise objects, and Await-Async calls, it becomes pretty straight-forward to use Async-Storage, however to avoid the application from crashing, exception handling is critical, and needs to cover cases in which promise returned is NULL.Some of the operations are shown here belowGet Data  -try {  \tAsyncStorage.getItem(    \t'Key').then(data =&gt; {      \tthis.setState({another : data}) // code after **then** dictates what is to be done with the data retrieved    \t}) ; \t} catch (error) {   \t\tconsole.log('retrieve not happening')  \t}Insert/Update Data -try {  AsyncStorage.setItem('Key', JSON.stringify(this.state.textValue)); // set state variable to be equal to key's value} catch (error) {  console.log('Couldn\\'t save') ;}Delete Data -try {  AsyncStorage.removeItem('Key', (err) =&gt; {       console.log('Deleted...');\t\t// code after comma, dictates action after deleting the value associated with key  });\t\t}catch(exception) {  console.log('Delete not happening') ; }Card ViewCardView, a very popular Material Design resembles a frame, and has a very elegant styling already coded up for itself. It is a very potent tool to use, when information of the same kind needs to be displayed somewhere in the application. is the docs detail the the philosophy behind this design.Add the third party modules using your default package manager,yarn add react-native-elements yarn add react-native-vector-iconsreact-native link react-native-vector-icons Note : Not adding react-native-vector-icons will result in the code not working, since Card Component requires that module for the initial auto-generated code added during installation to work.Thereafter using a CardView is exactly similar to how a normal View would be used,Import it,import {Card} from 'react-native-elements'Wrap some content in it,&lt;Card&gt;\t&lt;Text&gt; some text &lt;/Text&gt;\t...\t..\t.&lt;/Card&gt;And whatever is wrapped gets displayed, we don’t need to worry about the size of the card, it adjusts according to the content it has, although we need to careful of the fact that the screen size might not accomodate all the cards. Thus we wrap the overall Content within ScrollView, so that we can view everything there,Common errors      SrollView Child layout must be applied through …. error type : This happens because in ScrollView, we can’t have styling done through the normal style option, instead we have to use :    contentContainerStyle = {styles.someStyle}        ScrollView, does not crash, but it does not scroll either. In this case, generally adding one option to ScrollView styles component ( here someStyle in the styles Stylesheet ) makes the code work,    flexGrow : 1  Custom CameraSince, the Camera component used here is an in-built component, provided by the expo class, using it doesn’t require any kind of external library installation. However, for using the Camera in Android , the app should have the necessary mandation to use the device’s Camera. For the same, we use the Permission component provided by expo. The Camera component simply gives us the view of the camera itself, it does not involve using intents to use the default Camera App in a given android device. Thus every functionality of the camera, from mechanisms to click pictures, to deleting pictures, to flipping the camera needs to be done explicity through code. Again, the photos captured do not get saved themselves, they need to be save explicitly by connecting the FileSystem component. using any database mechanism. Here we use ‘react-native-simple-store’ which is a 3rd party module (thus needs to be added manually using package managers) , a wrapper for Async-Storage,Thus import statement includesimport {Permissions, Camera , FileSystem} from 'expo' ; import 'react-native-simple-store'For looking at the camera contents, we simply use &lt;Camera&gt; &lt;/Camera&gt; as we would use &lt;Text&gt; or &lt;Button&gt;, we style it appropriately, however the only difference is we use Modal View to embed the Camera view, and the advantage of using a modal view is the fact that we can actually set as to whether the Modal view is visible at some point, or whether it isn’t.It becomes really useful with Cameras, since we simulate opening and closing of the camera by manipulating the visibility of the container modal view.&lt;Modal\tanimationType=\"slide\"\ttransparent={false}\t\t\tvisible={this.state.is_camera_visible}\t\t// control visibility by a state variable...\tonRequestClose={() =&gt; { this.setState({ is_camera_visible: false }); }}&gt;Now as a sub element, to this Modal, we could define views, which will have the Camera component, along with a few buttons, meant to serve different functionalities of the camera. The styling of the buttons, camera screen component can be modified to one’s suiting and serve the customization. A sample is given below&lt;Camera style={/* style */} type={this.state.type} ref={ref =&gt; { this.camera = ref; }}&gt;\t&lt;View style={/* style for Body */}&gt;\t\t&lt;View style={/* style for Button */}&gt;\t\t\t&lt;Button {/* close camera */} /&gt;\t\t\t&lt;Button {/* flip- camera */}/&gt;\t\t\t&lt;Button {/* capture-photo */}/&gt;\t\t&lt;/View&gt;\t&lt;/View&gt;&lt;/Camera&gt;here, one important point that needs to be emphasized upon is the setting of the this.camera variable, we connect the Camera View component inside Modal, to the actual Camera using this, very reference. Thus playing around with these, we can even simultaneously use both the front and back cameras, also the type here signifies, whether the front or the rear camera is being used, therefore a state variable can be used to keep track of the current type, and changing to a different one, would involve, simply changing the state variable’s content.However, before using any of these functionalities, as said before permissions need to be sorted for Android Devices. A way of securing permissions is shown below : \tcomponentWillMount() {    \tPermissions.askAsync(Permissions.CAMERA).then((response) =&gt; {      \t\tthis.setState({ has_camera_permission: response.status === 'granted' });     \t});  \t}componentWillMount code, executes before any page has been rendered, thus including this, would invoke an alert, asking for permissions to use the Camera. Here a state variable, has_camera_permission is used to store whether or not permission has been availed, so the button to launch the camera can use this status as a condition, to open up camera, only if status == granted.The code to take a picture is given below :if(this.camera){ // check whether there's a camera reference\tthis.camera.takePictureAsync().then((data) =&gt; {\t\t// code for processing the picture captured, stored in data... \t});}Now, we have to store the pictures that have been captured, using FileSystem another component for which expo provides support. Importing the component is necessary as indicated before, we use a variable to store the location of the directory in which  all the photos captured will get stored. Here we use react-native-simple-store in conjunction with FileSystem component to store the pictures in the app itself.this.location = FileSystem.documentDirectory ; Here location will now have a unique location at which it is being stored, however it won’t be accessible by the phone itself, since by default the location is in the Android OS. For changing this to save things to your Gallery, or to change it up to have things saved up on some other default location Camera Roll API can be used.Thus inside the then construct for the code to capture pictures, we put in mechanisms to store the photo. The photo is accessible via the data variable. For every photo we get a unique file-path, then we add the following :FileSystem.moveAsync({\tfrom: data.uri,\tto: file_path}).then( (response) =&gt; {\tlet photo_data = {\t\tkey: // some key         name: // some name \t};\tstore.push('database_name', photo_data);});This function simply moves the photo from the obscure location it is at, to a location somewhere either local or external (in that case, we have to take permission to access device content’s as well) in the app. photo_data here is used as a temporary variable to eventually store the photo in a database (Async Storage) for the app. So at a later point in time, we can view the photo by retrieving it using the key that was used to store it in the first place. A lot more can be done, in terms of enhancing functionalities of the camera, styling the app, some other 3rd party modules can be used that aren’t supported by expo, and thus projects for those can be made using ‘react-native init’. However, using expo components always holds the advantage of not having to install external packages, also there not being any effort whatsover to make changes specific to any particular platform ( neither Android nor IOS ).Integrating MapsBefore integrating Maps add the third party modules\tyarn add react-native-elements \tyarn add react-native-vector-icons\treact-native link react-native-vector-icons \tyarn add react-native-maps\treact-native link react-native-mapsNow, for using any Map Service ( eg. Google Maps, OpenLayers, TomTom ) we need to generate API keys to render the Map View. Thus after generating the appropriate keys, we add it to our Android.manifest file as follows demo for google API given      &lt;meta-data        android:name=\"com.google.android.geo.API_KEY\"        android:value=\"---- Insert your own key ----\"/&gt;A common error that occurs on running the application at this very instance is as follows, android.support.v4.accessibilityservice.AccessibilityServiceInfoCompat ErrorA solution for this is to replace the line\tcompile/implementation project('react-native-maps')with the following code segments,\tcompile(project(':react-native-maps')) {\t\texclude group: 'com.android.support'\t\texclude module: 'appcompat-v7'\t\texclude module: 'support-v4'\t}Now, hereupon we can render the MapView anywhere in the application. It is generally better to render the map with absolute coordinates, an example of a scheme for full screen map shown here :-\tmap: {position:'absolute', top:0, left:0, right:0,bottom:0}Before rendering the Map, import all required components\timport MapView, {Marker} from react-native-elementsMapView has a Region variable, which has in 4 parameters viz, latitude, longititude, latitudeDelta , longitudeDelta to specify what area the MapView should be showing. Initial Region for the Map should always be shown, a sample setup is shown here,&lt;MapView\tref = \"Mapfunction\"\tstyle={styles.map}\tinitialRegion={\t\t{\t\t\tlatitude: 37.78825,\t\t\tlongitude: -122.4324,\t\t\tlatitudeDelta: 0.00922,\t\t\tlongitudeDelta: 0.00421\t\t}\t}\t&lt;Marker\t\tref = \"MapMove\"\t\tcoordinate={\t\t\t{\t\t\t\tlatitude : 37.78825,\t\t\t\tlongitude : -122.4324\t\t\t}\t\t}\t/&gt;\t\t&lt;/MapView&gt;ref is used to call the methods native to a particular component. Thus, for example for a button focus, blur methods are provided. So using ref, we create a object like entity which we used to call that method. So here, using any method for the MapView would involve the following, this.refs.Mapfunction.methodToCall.The code above renders the Map to be set to the region specified, with the center of the map being marked by the Marker component and the expanse of the map being the variables latitudeDelta and longitudeDelta.Say, if we get the coordinates of a new location through some other means in our application, then we can change our MapView to refer to those coordinates and our marker to point to that very location in the following way,this.refs.Mapfunction.animateToRegion( {\t\tlatitude : Number(this.state.latitude),\t\tlongitude : Number(this.state.longitude),\t\tlatitudeDelta : 0.00922,\t\tlongitudeDelta: 0.00421,\t},    100 ); this.refs.MapMove.animateMarkerToCoordinate( {\t\tlatitude : Number(this.state.latitude),\t\tlongitude : Number(this.state.longitude),      \t},\t100) ; The ref variables are in line with what we had set while rendering the components, also we assume the new latitude and longitude values are stored in state component of the application.GeocodingGeocoding is the process of converting human readable addresses to latitudes and longitudes, thus while searching for some location, Geocoding comes into place, since the user would be putting in the name of the addresses, and thus we need to generate the corresponding geocoded values, for any further application, example getting the route.Thus, for doing the same add one more package to our project,yarn add react-native-geocodingreact-native link react-native-geocodingThereafter we add another API key to our application, one which is capable of doing geocoding. Thus amongst the Google API’s the Places API suffices. Some other key could also be used.import Geododer from 'react-native-geocoding'Geocoder.init('---- Insert your API key here ----- ')Now, a sample conversion of a place’s address to latitude and longitude is given hereGeocoder.from(this.state.sampleAddress).then(json =&gt; {\tvar location = json.results[0].geometry.location;\tconsole.log(location.lat); \tconsole.log(location.lng); })Here, we get the 1st location result that we get, and retrieve that result’s latitude and longitude. If all the results are needed, we can use the json.results[] area to access all the results. It should be noted that the scope of the location variable here is local, thus to access these values anywhere else, we must store these to some variables, and for doing the same, differing datatypes must be handled with extreme care.The reverse of this process called reverse-geocoding can also be used using the same third party module.Another thing worth exploring is the SearchBar component provided by ‘react-native-elements’. Generally used in conjunction with React Native Maps, it is a very efficient way to display the results of a search and to choose amongst the options.A sample code snippet, which is self explanatory is provided here,&lt;SearchBar \tdata = {some array component}\tcontainerStyle = {some stylesheet component}\tonChangeText= {some method }\tonSubmitEditing = {some method}\tplaceholder=\"Type here ....\"/&gt;Here the data component is the array where the search will be performed, and from which the results will be finally refined. Other components are self explanatory. For discovering other functions associated with the SearchBar component, link_Note : Haven’t added details on setting up a React Native Environment, since there are a lot of references available for that. I recommend going through this repository if you wish to explore React Native in more depth"
  },
  
  {
    "title": "Musings about where we are headed",
    "url": "/posts/journey-ahead/",
    "categories": "",
    "tags": "reflections",
    "date": "2018-12-06 00:00:00 +0100",
    





    
    "snippet": "The past decade has seen an unprecedented surge in developments and advancements, comparable only to the Industrial Revolution of the 17th century—yet, remarkably, this progress has occurred withou...",
    "content": "The past decade has seen an unprecedented surge in developments and advancements, comparable only to the Industrial Revolution of the 17th century—yet, remarkably, this progress has occurred without any groundbreaking discoveries or major scientific breakthroughs. The contributions of theoretical physics and chemistry have diminished significantly over the last 25 years. In these fields, most major advancements have been abstract and esoteric, such as proofs of concepts like teleportation or theories about alternate dimensions (e.g., string theory). These topics often elicit skepticism from the general public, who view them as obscure, far-fetched, and dogmatic. In stark contrast, technological advancements have been nothing short of staggering, permeating every aspect of life at an accelerating pace. The “Law of Accelerating Returns” suggests that the rate of technological change has always increased exponentially and will continue to do so. Studies predict that in the coming decade, we could achieve 100 times more than what was accomplished in the last decade—a notion that may be unsettling for those who long for stability and the “good old days.”This raises the question: Can times ever be simple again? The answer is likely rhetorical. Luxury is one of the most powerful addictions, and its withdrawal the most challenging. In the short term, people might adapt to changing circumstances and give up some comforts they’ve grown accustomed to, but in the long term, evolutionary biology makes it virtually impossible for us to revert to our former ways. While we might be able to withstand extreme temperatures today, our constant reliance on temperature control is gradually eroding our resilience. Similarly, the day may come when it is psychologically impossible to live without the internet. Other lost human abilities include weaker digestive systems, diminished hearing and sense of smell, reduced peripheral vision, and a significant decline in survival instincts. This is what troubles those in the tech field—not the fear of becoming obsolete, but the realization that every new device, service, or automated task contributes to the erosion of our ability to perform those tasks independently. This paradox dampens the spirits of a community whose primary goal has always been to make life easier for everyone.The solution isn’t as simple as halting technological progress abruptly, which could lead to catastrophic consequences. It would be akin to trying to stay afloat instead of moving forward in the face of an oncoming wave. Doing nothing would only set us back, the very outcome we sought to avoid. So, what is the solution to this seemingly insurmountable challenge? In many ways, humanity is behaving like a teenager—paranoid about every decision, feeling helpless about a lack of control, and endlessly worrying about the future. But like teenagers, we eventually grow up, mature, and gain clarity. Perhaps if humanity takes more responsibility and approaches this period of rapid change with maturity, we might just weather the storm and emerge into a brighter, more beautiful dawn."
  },
  
  {
    "title": "Tryst with Git",
    "url": "/posts/version-control/",
    "categories": "",
    "tags": "reflections",
    "date": "2018-12-01 00:00:00 +0100",
    





    
    "snippet": "For novice coders, mastering version control systems like Git can seem daunting and unnecessary. However, version control is crucial for managing project changes, and you’ll only realize its value ...",
    "content": "For novice coders, mastering version control systems like Git can seem daunting and unnecessary. However, version control is crucial for managing project changes, and you’ll only realize its value after using it. Version control is your guardian angel in the development world.Consider a personal experience when a lack of version control proved costly:In December 2016, I participated in Microsoft’s Code Fun Do. Our team aimed to build an app that displayed nearby hospitals and their contact information. We extracted hospital locations and contact numbers using Google’s API. The app was submission-worthy, but we made the grave mistake of not using version control. Inevitable bugs and changes led to lost track of progress, preventing a valid submission.Another instance occurred during my internship at Optimize IT Systems in 2018. I was tasked with creating a demo project showcasing advanced React Native features. While version control wasn’t an issue, rushing to finish work led to forgetting to commit changes. The next day, installing a third-party React module messed up my build files, rendering my app unusable. It took an entire day to revert to my previous state.Don’t postpone learning Git or any other version control system. While it may not seem essential at first, it’s an integral part of every project. Frequent commits, even for minor bug fixes, are essential.P.S.: I’m still unsure about commits fixing typos. :)"
  }
  
]

