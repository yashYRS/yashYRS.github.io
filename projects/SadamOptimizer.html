<!doctype html>
<html>

<head>

  <title>
    
      SAdam: An Adam variant that converges faster for convex loss functions
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <link rel="shortcut icon" href="#" />
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="&gt;_  cd /home/" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title=">_  cd /home/ |  "/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Custom Fonts -->
  <link href='//fonts.googleapis.com/css?family=Raleway:800,400,300,600,700|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>SAdam: An Adam variant that converges faster for convex loss functions | _ cd /home/</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="SAdam: An Adam variant that converges faster for convex loss functions" />
<meta name="author" content="Yash Sarrof" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper. SAdam is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases." />
<meta property="og:description" content="I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper. SAdam is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases." />
<link rel="canonical" href="http://localhost:4000/projects/SadamOptimizer.html" />
<meta property="og:url" content="http://localhost:4000/projects/SadamOptimizer.html" />
<meta property="og:site_name" content="_ cd /home/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-29T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="SAdam: An Adam variant that converges faster for convex loss functions" />
<script type="application/ld+json">
{"dateModified":"2021-03-29T00:00:00+05:30","datePublished":"2021-03-29T00:00:00+05:30","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/SadamOptimizer.html"},"url":"http://localhost:4000/projects/SadamOptimizer.html","author":{"@type":"Person","name":"Yash Sarrof"},"description":"I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper. SAdam is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases.","headline":"SAdam: An Adam variant that converges faster for convex loss functions","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="/">>_  cd /home/</a>
    <small class="masthead-subtitle"> </small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/resume/">Résumé</a>
    
      <a href="/menu/articles.html">CS Articles</a>
    
      <a href="/menu/thoughts.html">Thoughts</a>
    
      <a href="/menu/projects.html">Projects</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/yashYRS" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/yashYRS" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/yash-sarrof-094364142/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="yashsarrof@outlook.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>


    <div class="post-container">
      <h2>
  SAdam: An Adam variant that converges faster for convex loss functions
</h2>



<p>I undertook this project along with <a href="https://github.com/naruarjun">Narayanan ER</a> as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper. SAdam is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases.</p>

<p>Explain what convexity means</p>

<p>SAdam follows the general framework of Adam, deploying a faster decaying rate step size controlled by time-variant hyperparameters to exploit strong convexity. Although it must be mentioned that similar ideas have successfully been applied in the past to the frameworks of Adagrad and RMSProp to get the variants called SC-Adagrad and SC-RMSProp <a class="citation" href="#"></a>. Theoretical analysis of SAdam (available in { % cite % }) show a data-dependent <code class="language-plaintext highlighter-rouge">O(logT)</code> regret bound for strongly convex functions, which means that it converges faster than AMSgrad and AdamNC in such cases.</p>

<p>We performed all the experiments mentioned in the original paper by the authors to verify their claims. We added a few experiments of our own to verify whether only the best results on some of the datasets were cherry picked in the original paper. Therefore we ended up performing the following tasks:</p>

<ul>
  <li>
    <p>Calculation of Regret for L2 Regularized Logistic Regression on MNIST, CIFAR10, CIFAR100 for our pool of optimizers, to check whether the central claim of better performance on convex problems stands.</p>
  </li>
  <li>
    <p>Computing test accuracy and training loss for a 4-layer CNN and ResNet18 on MNIST, CIFAR10, CIFAR100 for our pool of optimizers to check whether SAdam outperforms the existing optimization techniques in the context of deeper network
training as well, which is inherently a non-convex optimization problem.</p>
  </li>
  <li>
    <p>Training a Multi-Layer LSTM on the PennTreeBank dataset on the Language Modelling task, to test the performance of SAdam in a context different from the usual vision domains it had hitherto been tested on</p>
  </li>
</ul>

<p>It must also be noted that, the authors had implemented the optimizer in <strong>Tensorflow</strong> and in order to make the optimizer more accessible, we rewrote everything in <strong>PyTorch</strong>. Not just SAdam, in order to compare all the optimizers mentioned in the paper, we implemented SC_RMSProp and SC_Adagrad in Pytorch as well. All the correpsonding code can be found in this <a href="https://github.com/naruarjun/SADAM-reproducibility">repository</a>. The steps to install and run the optimizers are given in detail there. In addition, the details of all of the experiments that were carried out, including the optimal hyperparameters for each are provided in the <a href="https://openreview.net/forum?id=eNj0zqNUkBU">reproducibility report</a> published on the OpenReview Portal.</p>

<h3 id="explanation-of-the-code-for-implementation-of-sadam-optimizer">Explanation of the code for implementation of SAdam Optimizer</h3>

<p>Explain the code for implementation of SAdam.</p>


<span class="post-date">
  Written on
  
  March
  29th,
  2021
</span>

    </div>

    <footer class="footer">
  
  
  
    <a href="https://www.github.com/yashYRS" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/yashYRS" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/yash-sarrof-094364142/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="yashsarrof@outlook.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

</footer>

  </div>

</body>
</html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>