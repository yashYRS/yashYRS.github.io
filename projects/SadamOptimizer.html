<!doctype html>
<html>

<head>

  <title>
    
      SAdam: An Adam variant that converges faster for convex loss functions
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <link rel="shortcut icon" href="#" />
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="&gt;_  cd /home/" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title=">_  cd /home/ |  "/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Custom Fonts -->
  <link href='//fonts.googleapis.com/css?family=Raleway:800,400,300,600,700|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>SAdam: An Adam variant that converges faster for convex loss functions | _ cd /home/</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="SAdam: An Adam variant that converges faster for convex loss functions" />
<meta name="author" content="Yash Sarrof" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="SAdam (Wang et al., 2019) is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases. I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper." />
<meta property="og:description" content="SAdam (Wang et al., 2019) is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases. I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper." />
<link rel="canonical" href="http://localhost:4000/projects/SadamOptimizer.html" />
<meta property="og:url" content="http://localhost:4000/projects/SadamOptimizer.html" />
<meta property="og:site_name" content="_ cd /home/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-29T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="SAdam: An Adam variant that converges faster for convex loss functions" />
<script type="application/ld+json">
{"dateModified":"2021-03-29T00:00:00+05:30","datePublished":"2021-03-29T00:00:00+05:30","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/SadamOptimizer.html"},"url":"http://localhost:4000/projects/SadamOptimizer.html","author":{"@type":"Person","name":"Yash Sarrof"},"description":"SAdam (Wang et al., 2019) is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases. I undertook this project along with Narayanan ER as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper.","headline":"SAdam: An Adam variant that converges faster for convex loss functions","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="/">>_  cd /home/</a>
    <small class="masthead-subtitle"> </small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/resume/">Résumé</a>
    
      <a href="/menu/articles.html">Technical</a>
    
      <a href="/menu/thoughts.html">Thoughts</a>
    
      <a href="/menu/projects.html">Projects</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/yashYRS" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/yashYRS" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/yash-sarrof-094364142/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:yashsarrof@outlook.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>


    <div class="post-container">
      <h2>
  SAdam: An Adam variant that converges faster for convex loss functions
</h2>



<p>SAdam <a class="citation" href="#wang2019sadam">(Wang et al., 2019)</a> is an online convex optimizer that enhances the Adam algorithm by utilizing strong convexity of functions wherever possible. Although the motivation behind making these modifications are to improve performance in only convex cases, they prove to be effective even in non-convex cases. I undertook this project along with <a href="https://github.com/naruarjun">Narayanan ER</a> as part of the ML Reproducibility Challenge 2020. Any variation with respect to Adam with a solid mathematical backing deserves a serious look and hence SAdam got our attention, and we decided to reproduce the results shown in the paper.</p>

<p>A mathematical function is called convex if a 2nd order of differentiation is possible, and if the second derivative is never negative. Owing to this, one of the properties of convex functions is that they have a global minimum. This has led to the adoptiion convex loss functions, although for training deep neural networks working with non-convex loss functions become imperative.</p>

<p>SAdam follows the general framework of Adam, deploying a faster decaying rate step size controlled by time-variant hyperparameters to exploit strong convexity. Although it must be mentioned that similar ideas have successfully been applied in the past to the frameworks of Adagrad and RMSProp to get the variants called SC-Adagrad and SC-RMSProp <a class="citation" href="#mukkamala2017variants">(Mukkamala &amp; Hein, 2017)</a>. Theoretical analysis of SAdam show a data-dependent <code class="language-plaintext highlighter-rouge">O(logT)</code> regret bound for strongly convex functions, which means that it converges faster than AMSgrad and AdamNC in such cases.</p>

<p>We performed all the experiments mentioned in the original paper by the authors to verify their claims. We added a few experiments of our own to verify whether only the best results on some of the datasets were cherry picked in the original paper. Therefore we ended up performing the following tasks:</p>

<ul>
  <li>
    <p>Calculation of Regret for L2 Regularized Logistic Regression on MNIST, CIFAR10, CIFAR100 for our pool of optimizers, to check whether the central claim of better performance on convex problems stands.</p>
  </li>
  <li>
    <p>Computing test accuracy and training loss for a 4-layer CNN and ResNet18 on MNIST, CIFAR10, CIFAR100 for our pool of optimizers to check whether SAdam outperforms the existing optimization techniques in the context of deeper network
training as well, which is inherently a non-convex optimization problem.</p>
  </li>
  <li>
    <p>Training a Multi-Layer LSTM on the PennTreeBank dataset on the Language Modelling task, to test the performance of SAdam in a context different from the usual vision domains it had hitherto been tested on</p>
  </li>
</ul>

<p>It must also be noted that, the authors had implemented the optimizer in <strong>Tensorflow</strong> and in order to make the optimizer more accessible, we rewrote everything in <strong>PyTorch</strong>. Not just SAdam, in order to compare all the optimizers mentioned in the paper, we implemented SC_RMSProp and SC_Adagrad in Pytorch as well. All the correpsonding code can be found in this <a href="https://github.com/naruarjun/SADAM-reproducibility">repository</a>. The steps to install and run the optimizers are given in detail there. In addition, the details of all of the experiments that were carried out, including the optimal hyperparameters for each are provided in the <a href="https://openreview.net/forum?id=eNj0zqNUkBU">reproducibility report</a> published on the OpenReview Portal.</p>

<h3 id="implementation-of-the-sadam-optimizer">Implementation of the SAdam optimizer</h3>

<p>To implement custom optimizers in Pytorch, the functions of the default <code class="language-plaintext highlighter-rouge">torch.optim.Optimizer</code> viz. <code class="language-plaintext highlighter-rouge">__init__</code> and <code class="language-plaintext highlighter-rouge">step()</code> need to be overriden.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__init__</code>: This function serves as the point where all the parameters for the optimizers are initialized. Thus the mandatory <code class="language-plaintext highlighter-rouge">params</code> parameter needs to initialized with a dictionary of hyperparameters necessary for optimization. It should be noted however <code class="language-plaintext highlighter-rouge">weight_decay</code> is not a part of the optimizer, but is a way of adding <code class="language-plaintext highlighter-rouge">L2</code> Regularization loss in Pytorch.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
def __init__(self, params, beta_1=0.9, lr=0.01, delta=1e-2, xi_1=0.1,
             xi_2=0.1, gamma=0.9, weight_decay=1e-2):

    # Making the dictionary with all of the hyperparameters
    defaults = dict(lr=lr, beta_1=beta_1, delta=delta, xi_1=xi_1,
                    xi_2=xi_2, gamma=gamma, weight_decay=weight_decay)
    super(SAdam, self).__init__(params, defaults)

</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">step</code> : A sample optimization step is mentioned in this function. <code class="language-plaintext highlighter-rouge">closure</code> is required for a few loss functions, where closure is utilized to terminate the loss function updates by the optimizer (not required for SAdam). The code given below performs 1 step on all of the parameters, and is almost a replica of the Adam optimizer, except that β<em>2</em> is time variant in SAdam, and instead of dividing by the square root for the second moment vector (denoted by <code class="language-plaintext highlighter-rouge">v_t</code> in the code given below), we use it as is. Intuitively, the larger decay caused due to this results in faster convergence, while the time variant hyperparameters make sure the convergence is not unstable.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def step(self, closure=None):
    loss = None
    if closure is not None:
        loss = closure()

    for group in self.param_groups:
        for p in group['params']:
            if p.grad is None:
                continue

            grad = p.grad
            state = self.state[p]

            # Initialize the derivatives if they haven't been updated yet
            if len(state) == 0:
                state['step'] = 0
                # Exponential moving average of gradient values
                state['hat_g_t'] = torch.zeros_like(
                    p, memory_format=torch.preserve_format)
                # Exponential moving average of squared gradient values
                state['v_t'] = torch.zeros_like(
                    p, memory_format=torch.preserve_format)

            # Extract all the hyperparameters for the optimizer
            lr, delta = group['lr'], group['delta']
            xi_1, xi_2 = group['xi_1'], group['xi_2']
            hat_g_t, v_t = state['hat_g_t'], state['v_t']
            gamma, beta_1 = group['gamma'], group['beta_1']

            # Update the step taken
            state['step'] += 1

            # L2 Regularization performed, if weight decay is initialized
            if group['weight_decay'] != 0:
                grad = grad.add(p, alpha=group['weight_decay'])

            time_step = state['step']
            beta_2 = 1 - gamma/time_step

            hat_g_t.mul_(beta_1).add_(grad, alpha=1 - beta_1)
            v_t.mul_(beta_2).addcmul_(grad, grad, value=1-beta_2)
            denom = time_step*v_t + delta
            p.addcmul_(hat_g_t, 1/denom, value=-lr)

return loss
</code></pre></div></div>

<h3 id="references">References</h3>
<ol class="bibliography"><li><span id="wang2019sadam">Wang, G., Lu, S., Tu, W., &amp; Zhang, L. (2019). Sadam: A variant of adam for strongly convex functions. In <i>arXiv preprint arXiv:1905.02957</i>.</span></li>
<li><span id="mukkamala2017variants">Mukkamala, M. C., &amp; Hein, M. (2017). Variants of rmsprop and adagrad with logarithmic regret bounds. In <i>International Conference on Machine Learning</i> (pp. 2545–2553). PMLR.</span></li></ol>


<span class="post-date">
  Written on
  
  March
  29th,
  2021
</span>

    </div>

    <footer class="footer">
  
  
  
    <a href="https://www.github.com/yashYRS" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/yashYRS" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/yash-sarrof-094364142/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:yashsarrof@outlook.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

</footer>

  </div>

</body>
</html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>